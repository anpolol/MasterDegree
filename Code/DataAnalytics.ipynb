{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "import scipy\n",
    "from scipy.io import loadmat as lm\n",
    "\n",
    "from torch_geometric.utils.convert import  from_scipy_sparse_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ПРОБЛЕМЫ ВОПРОСЫ**:\n",
    "- BlogCatalog, CoCit, Wiki имебют матрицу groupб утвреждается что это лейблы но как быть если у BlogCatalog, Wiki на каждую вершину по более чем одному лейблу -  поставила всем как - фичи\n",
    "- Epinion, Arxiv не имеют ни лейблов ни фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_networkx(data, node_attrs=None, edge_attrs=None):\n",
    "    r\"\"\"Converts a data object graph to a networkx graph.\n",
    "\n",
    "    Args:\n",
    "        data (torch_geometric.data.Data): The data object.\n",
    "        node_attrs (iterable of str, optional): The node attributes to be\n",
    "            copied. (default: :obj:`None`)\n",
    "        edge_attrs (iterable of str, optional): The edge attributes to be\n",
    "            copied. (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "\n",
    "    G = nx.Graph()\n",
    "    if node_attrs is not None:\n",
    "        \n",
    "        for i,attr in enumerate(node_attrs):\n",
    "            G.add_node(i, feature = str(attr.tolist()))\n",
    "    else:\n",
    "        G.add_nodes_from(list(range((data.num_nodes))))\n",
    "    \n",
    "    for i, (u, v) in enumerate(data.edge_index.t().tolist()):\n",
    "        G.add_edge(u, v)\n",
    "    \n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Blogcatalog = lm('datasets/blogcatalog.mat')\n",
    "blogcatalog_edgeindex,_=from_scipy_sparse_matrix(Blogcatalog['network'])\n",
    "blogcatalog_x = torch.tensor(scipy.sparse.csr_matrix.toarray(Blogcatalog['group']))\n",
    "data_BlogCatalog = Data(edge_index = blogcatalog_edgeindex, x = blogcatalog_x)\n",
    "G_BlogCatalog = to_networkx(data_BlogCatalog, blogcatalog_x)\n",
    "#nx.from_scipy_\\sparse_matrix(data_Blogcatalog['network'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CoCit = lm('datasets/CoCit.mat')\n",
    "CoCit_edgeindex,_=from_scipy_sparse_matrix(CoCit['network'])\n",
    "CoCit_x = torch.tensor(scipy.sparse.csr_matrix.toarray(CoCit['group']))\n",
    "data_CoCit = Data(edge_index = CoCit_edgeindex, x = CoCit_x)\n",
    "G_CoCit=to_networkx(data_CoCit,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wiki = lm('datasets/Wiki.mat')\n",
    "Wiki_edgeindex,_=from_scipy_sparse_matrix(Wiki['network'])\n",
    "Wiki_x = torch.tensor(scipy.sparse.csr_matrix.toarray(Wiki['group']))\n",
    "data_Wiki = Data(edge_index = Wiki_edgeindex, x = Wiki_x)\n",
    "G_Wiki=to_networkx(data_Wiki,Wiki_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#провекра на симметричность а значит на ненаправленность\n",
    "np.all(scipy.sparse.csr_matrix.toarray(Blogcatalog['network']).transpose() == scipy.sparse.csr_matrix.toarray(Blogcatalog['network']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#провекра на симметричность а значит на ненаправленность\n",
    "np.all(scipy.sparse.csr_matrix.toarray(CoCit['network']).transpose() == scipy.sparse.csr_matrix.toarray(CoCit['network']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#провекра на симметричность а значит на ненаправленность\n",
    "np.all(scipy.sparse.csr_matrix.toarray(Wiki['network']).transpose() == scipy.sparse.csr_matrix.toarray(Wiki['network']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The number of nodes in your data object can only be inferred by its edge indices, and hence may result in unexpected batch-wise behavior, e.g., in case there exists isolated nodes. Please consider explicitly setting the number of nodes for this data object by assigning it to data.num_nodes.\n"
     ]
    }
   ],
   "source": [
    "with open('datasets/soc-Epinions1.txt','r') as f:\n",
    "    data_Epinion = f.readlines()\n",
    "\n",
    "new_list=list(map(lambda x: x.split('\\t'),data_Epinion[4:]))\n",
    "#print(new_list)\n",
    "dataedge_ep = list(map(lambda x: [int(x[0]),int(x[1].split('\\n')[0])],new_list))\n",
    "set1=set(collections.Counter((np.array(dataedge_ep)).transpose()[1]).keys()) | set(collections.Counter((np.array(dataedge_ep)).transpose()[0]).keys())\n",
    "map_ep={}\n",
    "for j,i in enumerate(set1):\n",
    "    map_ep[i] = j\n",
    "dataedge_ep_new = list(map(lambda x: [map_ep[x[0]],map_ep[x[1]]], dataedge_ep))\n",
    "data_edge_index_Epinion = torch.tensor(dataedge_ep_new).t()\n",
    "data_Eppinion_torch = Data(edge_index=data_edge_index_Epinion)\n",
    "G_Epinion = to_networkx(data_Eppinion_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The number of nodes in your data object can only be inferred by its edge indices, and hence may result in unexpected batch-wise behavior, e.g., in case there exists isolated nodes. Please consider explicitly setting the number of nodes for this data object by assigning it to data.num_nodes.\n"
     ]
    }
   ],
   "source": [
    "with open('datasets/CA-GrQc.txt','r') as f:\n",
    "    data_Arxiv = f.readlines()\n",
    "\n",
    "new_list=list(map(lambda x: x.split('\\t'),data_Arxiv[4:]))\n",
    "#print(new_list)\n",
    "dataedge = list(map(lambda x: [int(x[0]),int(x[1].split('\\n')[0])],new_list))\n",
    "set2=set(collections.Counter((np.array(dataedge)).transpose()[1]).keys()) | set(collections.Counter((np.array(dataedge)).transpose()[0]).keys())\n",
    "map_ar={}\n",
    "for j,i in enumerate(set2):\n",
    "    map_ar[i] = j\n",
    "    \n",
    "dataedge_ar_new = list(map(lambda x: [map_ar[x[0]],map_ar[x[1]]], dataedge))\n",
    "\n",
    "data_edge_index_Arxiv = torch.tensor(dataedge_ar_new).t()\n",
    "data_Arxiv_torch = Data(edge_index=data_edge_index_Arxiv)\n",
    "G_Arxiv = to_networkx(data_Arxiv_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "data_Cora =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())[0]\n",
    "#data_cora =dataset[0]\n",
    "data_Citeseer =Planetoid(root='/tmp/Citeseer', name='Citeseer',transform=T.NormalizeFeatures())[0]\n",
    "data_Pubmed =Planetoid(root='/tmp/Pubmed', name='Pubmed',transform=T.NormalizeFeatures())[0]\n",
    "\n",
    "from torch_geometric.datasets import Amazon\n",
    "\n",
    "data_Photo =Amazon(root='/tmp/Photo', name='Photo',transform=T.NormalizeFeatures())[0]\n",
    "data_Computers =Amazon(root='/tmp/Computers', name='Computers',transform=T.NormalizeFeatures())[0]\n",
    "\n",
    "from torch_geometric.datasets import Coauthor\n",
    "\n",
    "data_CS =Coauthor(root='/tmp/CS', name='CS',transform=T.NormalizeFeatures())[0]\n",
    "data_Physics =Coauthor(root='/tmp/Physics', name='Physics',transform=T.NormalizeFeatures())[0]\n",
    "\n",
    "from torch_geometric.datasets import Reddit\n",
    "\n",
    "\n",
    "#social networks: Reddit, Flickr\n",
    "from torch_geometric.datasets import Flickr\n",
    "data_Flickr =Flickr(root='/tmp/Flickr',transform=T.NormalizeFeatures())[0]\n",
    "# data_Reddit = Reddit(root='/tmp/Reddit',transform=T.NormalizeFeatures()) очень тяжелый \n",
    "\n",
    "#Wikipedia \"Chameleon\", \"Squirrel\"\n",
    "\n",
    "#Datasets of varying the homofily\n",
    "from torch_geometric.datasets import MixHopSyntheticDataset\n",
    "data_Homofily_0 = MixHopSyntheticDataset(root='/tmp/MixHopSyntheticDataset',homophily = 0.0, transform=T.NormalizeFeatures())[0]\n",
    "data_Homofily_01 = MixHopSyntheticDataset(root='/tmp/MixHopSyntheticDataset',homophily = 0.1, transform=T.NormalizeFeatures())[0]\n",
    "data_Homofily_02 = MixHopSyntheticDataset(root='/tmp/MixHopSyntheticDataset',homophily = 0.2, transform=T.NormalizeFeatures())[0]\n",
    "data_Homofily_03 = MixHopSyntheticDataset(root='/tmp/MixHopSyntheticDataset',homophily = 0.3, transform=T.NormalizeFeatures())[0]\n",
    "data_Homofily_04 = MixHopSyntheticDataset(root='/tmp/MixHopSyntheticDataset',homophily = 0.4, transform=T.NormalizeFeatures())[0]\n",
    "data_Homofily_05 = MixHopSyntheticDataset(root='/tmp/MixHopSyntheticDataset',homophily = 0.5, transform=T.NormalizeFeatures())[0]\n",
    "data_Homofily_06 = MixHopSyntheticDataset(root='/tmp/MixHopSyntheticDataset',homophily = 0.6, transform=T.NormalizeFeatures())[0]\n",
    "data_Homofily_07 = MixHopSyntheticDataset(root='/tmp/MixHopSyntheticDataset',homophily = 0.7, transform=T.NormalizeFeatures())[0]\n",
    "data_Homofily_08 = MixHopSyntheticDataset(root='/tmp/MixHopSyntheticDataset',homophily = 0.8, transform=T.NormalizeFeatures())[0]\n",
    "data_Homofily_09 = MixHopSyntheticDataset(root='/tmp/MixHopSyntheticDataset',homophily = 0.9, transform=T.NormalizeFeatures())[0]\n",
    "\n",
    "from torch_geometric.datasets import PPI\n",
    "\n",
    "data_PPI = PPI(root='/tmp/PPI', split=\"train\",transform=T.NormalizeFeatures())[0] #тут только train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [data_Cora, data_Citeseer, data_Pubmed, data_Photo, data_Computers, data_CS, data_Physics,data_Flickr,data_Homofily_0, data_Homofily_01,data_Homofily_02,data_Homofily_03,data_Homofily_04,data_Homofily_05,data_Homofily_06,data_Homofily_07,data_Homofily_08,data_Homofily_09,data_CoCit,data_BlogCatalog,data_Wiki]\n",
    "datasets_names = ['Cora', 'Citeseer', 'Pubmed', 'Photo', 'Computers', 'CS', 'Physics','Flickr','Homophily0','Homophily01','Homophily02','Homophily03','Homophily04','Homophily05','Homophily06','Homophily07','Homophily08','Homophily09','CoCit','BlogCatalog','Wikipedia','Arxiv','Epinion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = []\n",
    "for dataset in datasets:\n",
    "    graphs.append(to_networkx(dataset,dataset.x))\n",
    "graphs.append(G_Arxiv)\n",
    "graphs.append(G_Epinion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\networkx\\algorithms\\assortativity\\correlation.py:263: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  r = (t - s) / (1 - s)\n"
     ]
    }
   ],
   "source": [
    "number_of_nodes = []\n",
    "clustering_coef=[]\n",
    "density=[]\n",
    "degree_assort=[]\n",
    "number_of_features=[]\n",
    "number_of_labels = []\n",
    "attribute_assort = []\n",
    "isdirected=[]\n",
    " \n",
    "for i,gr in enumerate(graphs):\n",
    "        print(i)\n",
    "        try:\n",
    "            n_f=datasets[i].num_features\n",
    "            number_of_features.append(n_f)\n",
    "        except:\n",
    "            if (datasets_names[i] == 'CoCit') or (datasets_names[i]=='BlogCatalog') or (datasets_names[i]=='Wikipedia'):\n",
    "                number_of_features.append(datasets[i].shape(1))\n",
    "            else:\n",
    "                number_of_features.append(0)\n",
    "        \n",
    "        try:\n",
    "            i_d=(datasets[i].is_directed())\n",
    "            isdirected.append(i_d)\n",
    "        except:\n",
    "            if (datasets_names[i] == 'CoCit') or (datasets_names[i]=='BlogCatalog') or (datasets_names[i]=='Wikipedia'):\n",
    "                isdirected.append(False)\n",
    "            elif (datasets_names[i] == 'Arxiv') or (datasets_names[i]=='Epinion'):\n",
    "                isdirected.append(True)\n",
    "        \n",
    "        try:\n",
    "            n_l = len(collections.Counter(datasets[i].y.tolist()).keys())\n",
    "            number_of_labels.append(n_l)\n",
    "        except:\n",
    "            if (datasets_names[i] == 'CoCit') or (datasets_names[i]=='BlogCatalog') or (datasets_names[i]=='Wikipedia'):\n",
    "                number_of_labels.append(datasets[i].x.shape[1])\n",
    "            elif (datasets_names[i] == 'Arxiv') or (datasets_names[i]=='Epinion'):\n",
    "                number_of_labels.append(None)\n",
    "        \n",
    "        try:\n",
    "            a_s = nx.attribute_assortativity_coefficient(gr,'feature')\n",
    "            attribute_assort.append(a_s)\n",
    "        except:\n",
    "            attribute_assort.append(None)\n",
    "            \n",
    "        number_of_nodes.append(gr.number_of_nodes())\n",
    "            #плотность\n",
    "        density.append(nx.density(gr))\n",
    "             #ассортативность\n",
    "        degree_assort.append(nx.degree_assortativity_coefficient(gr))\n",
    "        clustering_coef.append(np.mean(list(nx.clustering(gr).values())))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name of dataset</th>\n",
       "      <th>Number of nodes (NN)</th>\n",
       "      <th>Number of features (NF)</th>\n",
       "      <th>Number of lables (NL)</th>\n",
       "      <th>Directed?</th>\n",
       "      <th>Clustering coefficient (CC)</th>\n",
       "      <th>Density(D)</th>\n",
       "      <th>Degree assortativity coefficient(DAC)</th>\n",
       "      <th>Attribute assortativity coefficient(AAC)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>4777</td>\n",
       "      <td>40</td>\n",
       "      <td>40.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.538570</td>\n",
       "      <td>0.008110</td>\n",
       "      <td>-0.237261</td>\n",
       "      <td>-0.015479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cora</td>\n",
       "      <td>2708</td>\n",
       "      <td>1433</td>\n",
       "      <td>7.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.240673</td>\n",
       "      <td>0.001440</td>\n",
       "      <td>-0.065871</td>\n",
       "      <td>-0.000850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Citeseer</td>\n",
       "      <td>3327</td>\n",
       "      <td>3703</td>\n",
       "      <td>6.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.141471</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.048378</td>\n",
       "      <td>-0.000547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Photo</td>\n",
       "      <td>7650</td>\n",
       "      <td>745</td>\n",
       "      <td>8.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.403979</td>\n",
       "      <td>0.004070</td>\n",
       "      <td>-0.044943</td>\n",
       "      <td>-0.000427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Homophily0</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.009002</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>-0.054907</td>\n",
       "      <td>-0.000410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Homophily01</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.011408</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>-0.053471</td>\n",
       "      <td>-0.000410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Homophily03</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.012979</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>-0.049402</td>\n",
       "      <td>-0.000405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Homophily02</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.011378</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>-0.051903</td>\n",
       "      <td>-0.000403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Homophily04</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.014314</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>-0.046596</td>\n",
       "      <td>-0.000402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Homophily05</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.018294</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>-0.049194</td>\n",
       "      <td>-0.000399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Homophily06</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.023043</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>-0.050115</td>\n",
       "      <td>-0.000396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Homophily07</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.030515</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>-0.043096</td>\n",
       "      <td>-0.000392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Homophily08</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.040033</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>-0.042703</td>\n",
       "      <td>-0.000387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Homophily09</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.057129</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>-0.039710</td>\n",
       "      <td>-0.000378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Computers</td>\n",
       "      <td>13752</td>\n",
       "      <td>767</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.344126</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>-0.056488</td>\n",
       "      <td>-0.000332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pubmed</td>\n",
       "      <td>19717</td>\n",
       "      <td>500</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.060175</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>-0.043640</td>\n",
       "      <td>-0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CS</td>\n",
       "      <td>18333</td>\n",
       "      <td>6805</td>\n",
       "      <td>15.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.342523</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.112609</td>\n",
       "      <td>0.000450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Physics</td>\n",
       "      <td>34493</td>\n",
       "      <td>8415</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.377623</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.201031</td>\n",
       "      <td>0.001631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BlogCatalog</td>\n",
       "      <td>10312</td>\n",
       "      <td>39</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.463196</td>\n",
       "      <td>0.006282</td>\n",
       "      <td>-0.254102</td>\n",
       "      <td>0.031028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CoCit</td>\n",
       "      <td>44034</td>\n",
       "      <td>15</td>\n",
       "      <td>15.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.141935</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>-0.052464</td>\n",
       "      <td>0.298539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Flickr</td>\n",
       "      <td>89250</td>\n",
       "      <td>500</td>\n",
       "      <td>7.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.032957</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>-0.044103</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Arxiv</td>\n",
       "      <td>5242</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.529636</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.659164</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Epinion</td>\n",
       "      <td>75879</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.137756</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>-0.040646</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Name of dataset  Number of nodes (NN)  Number of features (NF)  \\\n",
       "20       Wikipedia                  4777                       40   \n",
       "0             Cora                  2708                     1433   \n",
       "1         Citeseer                  3327                     3703   \n",
       "3            Photo                  7650                      745   \n",
       "8       Homophily0                  5000                        2   \n",
       "9      Homophily01                  5000                        2   \n",
       "11     Homophily03                  5000                        2   \n",
       "10     Homophily02                  5000                        2   \n",
       "12     Homophily04                  5000                        2   \n",
       "13     Homophily05                  5000                        2   \n",
       "14     Homophily06                  5000                        2   \n",
       "15     Homophily07                  5000                        2   \n",
       "16     Homophily08                  5000                        2   \n",
       "17     Homophily09                  5000                        2   \n",
       "4        Computers                 13752                      767   \n",
       "2           Pubmed                 19717                      500   \n",
       "5               CS                 18333                     6805   \n",
       "6          Physics                 34493                     8415   \n",
       "19     BlogCatalog                 10312                       39   \n",
       "18           CoCit                 44034                       15   \n",
       "7           Flickr                 89250                      500   \n",
       "21           Arxiv                  5242                        0   \n",
       "22         Epinion                 75879                        0   \n",
       "\n",
       "    Number of lables (NL)  Directed?   Clustering coefficient (CC)  \\\n",
       "20                   40.0       False                     0.538570   \n",
       "0                     7.0       False                     0.240673   \n",
       "1                     6.0       False                     0.141471   \n",
       "3                     8.0       False                     0.403979   \n",
       "8                    10.0       False                     0.009002   \n",
       "9                    10.0       False                     0.011408   \n",
       "11                   10.0       False                     0.012979   \n",
       "10                   10.0       False                     0.011378   \n",
       "12                   10.0       False                     0.014314   \n",
       "13                   10.0       False                     0.018294   \n",
       "14                   10.0       False                     0.023043   \n",
       "15                   10.0       False                     0.030515   \n",
       "16                   10.0       False                     0.040033   \n",
       "17                   10.0       False                     0.057129   \n",
       "4                    10.0       False                     0.344126   \n",
       "2                     3.0       False                     0.060175   \n",
       "5                    15.0       False                     0.342523   \n",
       "6                     5.0       False                     0.377623   \n",
       "19                   39.0       False                     0.463196   \n",
       "18                   15.0       False                     0.141935   \n",
       "7                     7.0       False                     0.032957   \n",
       "21                    NaN        True                     0.529636   \n",
       "22                    NaN        True                     0.137756   \n",
       "\n",
       "    Density(D)  Degree assortativity coefficient(DAC)  \\\n",
       "20    0.008110                              -0.237261   \n",
       "0     0.001440                              -0.065871   \n",
       "1     0.000823                               0.048378   \n",
       "3     0.004070                              -0.044943   \n",
       "8     0.002384                              -0.054907   \n",
       "9     0.002384                              -0.053471   \n",
       "11    0.002384                              -0.049402   \n",
       "10    0.002384                              -0.051903   \n",
       "12    0.002384                              -0.046596   \n",
       "13    0.002384                              -0.049194   \n",
       "14    0.002384                              -0.050115   \n",
       "15    0.002384                              -0.043096   \n",
       "16    0.002384                              -0.042703   \n",
       "17    0.002384                              -0.039710   \n",
       "4     0.002600                              -0.056488   \n",
       "2     0.000228                              -0.043640   \n",
       "5     0.000487                               0.112609   \n",
       "6     0.000417                               0.201031   \n",
       "19    0.006282                              -0.254102   \n",
       "18    0.000202                              -0.052464   \n",
       "7     0.000113                              -0.044103   \n",
       "21    0.001055                               0.659164   \n",
       "22    0.000141                              -0.040646   \n",
       "\n",
       "    Attribute assortativity coefficient(AAC)  \n",
       "20                                 -0.015479  \n",
       "0                                  -0.000850  \n",
       "1                                  -0.000547  \n",
       "3                                  -0.000427  \n",
       "8                                  -0.000410  \n",
       "9                                  -0.000410  \n",
       "11                                 -0.000405  \n",
       "10                                 -0.000403  \n",
       "12                                 -0.000402  \n",
       "13                                 -0.000399  \n",
       "14                                 -0.000396  \n",
       "15                                 -0.000392  \n",
       "16                                 -0.000387  \n",
       "17                                 -0.000378  \n",
       "4                                  -0.000332  \n",
       "2                                  -0.000189  \n",
       "5                                   0.000450  \n",
       "6                                   0.001631  \n",
       "19                                  0.031028  \n",
       "18                                  0.298539  \n",
       "7                                        NaN  \n",
       "21                                       NaN  \n",
       "22                                       NaN  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'Name of dataset' : datasets_names,\n",
    "                                'Number of nodes (NN)' :number_of_nodes,'Number of features (NF)' :number_of_features,'Number of lables (NL)':number_of_labels , 'Directed? ' :isdirected,\n",
    "                                'Clustering coefficient (CC)': clustering_coef,'Density(D)':density, 'Degree assortativity coefficient(DAC)':degree_assort, 'Attribute assortativity coefficient(AAC)': attribute_assort })\n",
    "df.sort_values(by=['Attribute assortativity coefficient(AAC)']) #добавить атрибуты, направленность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('analytics.pickle','wb') as f:\n",
    "    pickle.dump(df,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d_check = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['Pearson degree assortativity coefficient (PDAC)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['Degree assortativity coefficient(DAC)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    print(datasets[i].x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
