{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-gN3S4NhAjs",
    "outputId": "fef06879-cb42-4f09-db2d-88671e8c4315"
   },
   "outputs": [],
   "source": [
    "#!pip install -q torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 torchtext==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -q --no-index torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
    "#!pip install -q --no-index torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
    "#!pip install -q --no-index torch-cluster -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
    "#!pip install -q --no-index torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
    "#!pip install torch-geometric\n",
    "#!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import optuna\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from torch_geometric.data import NeighborSampler\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import numpy as np\n",
    "from modules.model import Net\n",
    "from sklearn.metrics import f1_score\n",
    "from modules.sampling import Sampler, SamplerContextMatrix, SamplerRandomWalk,SamplerFactorization,SamplerAPP\n",
    "from datetime import datetime\n",
    "import random\n",
    "from torch_geometric.data import GraphSAINTNodeSampler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 1433])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Amazon\n",
    "flag = False\n",
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "#dataset=Amazon(root='/tmp/Photo', name='Photo',transform=T.NormalizeFeatures())\n",
    "\n",
    "if flag: \n",
    "    data = dataset[0]\n",
    "\n",
    "    indices_sh=list(range(len(data.x)))\n",
    "    indices=list(range(len(data.x)))\n",
    "    random.shuffle(indices_sh)\n",
    "    x_new_map={}\n",
    "    for i,x in enumerate(indices_sh):\n",
    "        x_new_map[x] = i\n",
    "    data_x_new = data.x[indices_sh]\n",
    "    data_y_new = data.x[indices_sh]\n",
    "    data_edge_index = torch.tensor(list(map(lambda x: [x_new_map[int(x[0])],x_new_map[int(x[1])]] ,data.edge_index.t()))).t()\n",
    "\n",
    "    data.x=data_x_new\n",
    "    data.edge_index=data_edge_index\n",
    "    data.y=data_y_new\n",
    "    with open('data_new_cora.pickle','wb') as f:\n",
    "        pickle.dump(data,f)\n",
    "else:\n",
    "    with open('data_new_cora.pickle','rb') as f:\n",
    "        data = pickle.load(f)\n",
    "print((data.x).shape)\n",
    "indices=list(range(len(data.x)))\n",
    "train_indices = torch.tensor(indices[:int(0.7*len(indices)+1)])\n",
    "val_indices = torch.tensor(indices[int(0.7*len(indices)+1):int(0.8*len(indices)+1)])\n",
    "test_indices = torch.tensor(indices[int(0.8*len(indices)+1):])\n",
    "train_mask = torch.tensor([False]*len(indices))\n",
    "test_mask = torch.tensor([False]*len(indices))\n",
    "val_mask = torch.tensor([False]*len(indices))\n",
    "train_mask[train_indices] =True\n",
    "test_mask[test_indices]=True\n",
    "val_mask[val_indices]=True\n",
    "        \n",
    "#data_Computers =Amazon(root='/tmp/Computers', name='Computers',transform=T.NormalizeFeatures())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "class Main():\n",
    "    def __init__(self,conv, device, loss_function, mode ,dataset,data,train_indices,val_indices,test_indices,train_mask,val_mask,test_mask):\n",
    "        #self.dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "        self.dataset=dataset\n",
    "        data = self.dataset[0]\n",
    "        \n",
    "        self.Conv = conv\n",
    "        self.device = device\n",
    "        self.dataset = dataset\n",
    "        self.x = data.x\n",
    "        self.y = data.y.squeeze()\n",
    "        self.data=data.to(device)\n",
    "        self.loss = loss_function\n",
    "        self.mode = mode\n",
    "        self.train_indices =train_indices# torch.tensor(indices[:int(0.7*len(indices)+1)])\n",
    "        self.val_indices =val_indices# torch.tensor(indices[int(0.7*len(indices)+1):int(0.8*len(indices)+1)])\n",
    "        self.test_indices = test_indices#torch.tensor(indices[int(0.8*len(indices)+1):])\n",
    "        self.train_mask = train_mask#torch.tensor([False]*len(indices))\n",
    "        self.test_mask = test_mask#torch.tensor([False]*len(indices))\n",
    "        self.val_mask =val_mask# torch.tensor([False]*len(indices))\n",
    "        self.flag = self.loss[\"flag_tosave\"]\n",
    "        #print('huhu',self.data.x.shape)\n",
    "        super(Main, self).__init__()\n",
    "    def sampling(self,Sampler,epoch,nodes):\n",
    "        if (epoch == 0): \n",
    "            if self.flag:  \n",
    "                if \"alpha\" in self.loss: \n",
    "                    name_of_file = \"samples_\"+self.loss[\"Name\"]+\"_alpha_\"+str(self.loss[\"alpha\"])+\".pickle\"\n",
    "                else:\n",
    "                    name_of_file = \"samples_\"+self.loss[\"Name\"]+\".pickle\"\n",
    "                \n",
    "                if os.path.exists(name_of_file):\n",
    "                    with open(name_of_file,'rb') as f:\n",
    "                        self.samples = pickle.load(f)\n",
    "                else:\n",
    "                    self.samples = Sampler.sample(nodes) \n",
    "                    with open(name_of_file,'wb') as f:\n",
    "                        pickle.dump(self.samples,f)\n",
    "            else:\n",
    "                \n",
    "                self.samples = Sampler.sample(nodes)\n",
    " \n",
    "    def train(self, model,data,optimizer,Sampler,train_loader,dropout,epoch):\n",
    "        model.train()   \n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        if model.mode == 'unsupervised':\n",
    "            if model.conv=='GCN':\n",
    "                arr=torch.nonzero(self.train_mask == True)\n",
    "                indices_of_train_data = ([item for sublist in arr for item in sublist])\n",
    "                out = model.inference(data.to(device),dp=dropout)\n",
    "                samples = self.sampling(Sampler,epoch, indices_of_train_data)\n",
    "                loss = model.loss(out[self.train_mask], self.samples)\n",
    "                total_loss+=loss\n",
    "            else:\n",
    "                for batch_size, n_id, adjs in train_loader:\n",
    "                    #if len(train_loader.sizes) == 1:\n",
    "                     #   adjs = [adjs]\n",
    "                    adjs = [adj.to(device) for adj in adjs]\n",
    "                    out = model.forward(data.x[n_id.to(device)].to(device), adjs)\n",
    "                    self.sampling(Sampler,epoch,n_id[:batch_size])                 \n",
    "                    loss = model.loss(out, self.samples)#pos_batch.to(device), neg_batch.to(device))\n",
    "                    total_loss+=loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()      \n",
    "            return total_loss /len(train_loader)\n",
    "        elif model.mode== 'supervised':\n",
    "            if model.conv=='GCN':\n",
    "                out = model.inference(data.to(device),dp=dropout)\n",
    "                loss = model.loss_sup(out[self.train_mask],y[self.train_mask])\n",
    "                total_loss+=loss\n",
    "            else:\n",
    "                for batch_size, n_id, adjs in train_loader:\n",
    "                    adjs = [adj for adj in adjs]\n",
    "                    out = model.forward(data.x[n_id].to(device), adjs)\n",
    "                    loss = model.loss_sup(out,y[n_id[:batch_size]])\n",
    "                    total_loss+=loss\n",
    "            total_loss.backward(retain_graph=True)\n",
    "            optimizer.step()      \n",
    "            return total_loss /len(train_loader)       \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test(self, model,data,classifier,**kwargs):#,n_estimators,learning_rate_carboost, max_depth): \n",
    "        model.eval()\n",
    "        out = model.inference(data.to(device))\n",
    "        y_true = self.y.detach().numpy()\n",
    "        if model.mode == 'supervised':\n",
    "            y_true = self.y.unsqueeze(-1)\n",
    "            y_pred = out.cpu().argmax(dim=-1, keepdim=True)\n",
    "            accs = []\n",
    "            for mask in [self.train_mask, self.val_mask, self.test_mask]:    \n",
    "                accs+=[int(y_pred[mask].eq(y_true[mask]).sum()) / int(mask.sum())]\n",
    "            return accs\n",
    "        elif model.mode == 'unsupervised': \n",
    "            if classifier == 'logistic regression':\n",
    "                clf = LogisticRegression(max_iter = 3000,warm_start=True).fit(out.cpu().detach()[self.train_mask].numpy(), self.y.detach()[self.train_mask].numpy())\n",
    "            else:\n",
    "                n_estimators= kwargs[\"n_estimators\"]\n",
    "                learning_rate_catboost = kwargs[\"learning_rate_catboost\"]\n",
    "                max_depth = kwargs[\"max_depth\"]\n",
    "                clf = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate = learning_rate_catboost, max_depth=max_depth, random_state=0)\n",
    "                clf.fit(out.cpu().detach()[self.train_mask].numpy(), self.y.detach()[self.train_mask].numpy())\n",
    "            accs_micro = []\n",
    "            accs_macro = []\n",
    "            for mask in [self.train_mask,self.test_mask,self.val_mask]:\n",
    "                accs_micro += [f1_score(self.y.detach()[mask].cpu().numpy(),clf.predict(out.cpu().detach()[mask].numpy()), average='micro')]\n",
    "                accs_macro += [f1_score(self.y.detach()[mask].cpu().numpy(),clf.predict(out.cpu().detach()[mask].numpy()), average='macro')]\n",
    "                \n",
    "            return accs_micro,accs_macro\n",
    "\n",
    "    def run(self,hidden_layer=64,out_layer=128,dropout=0.0,size=1,learning_rate=0.001):\n",
    "        \n",
    "\n",
    "        classifier = \"logistic regression\"\n",
    "        train_loader = NeighborSampler(self.data.edge_index, node_idx=self.train_mask, batch_size = int(sum(self.train_mask)), sizes=[-1]*size)\n",
    "        Sampler = self.loss[\"Sampler\"]\n",
    "        LossSampler = Sampler(self.data,device=device,mask=self.train_mask,loss_info=self.loss)\n",
    "        model = Net(dataset = self.dataset,mode=self.mode,conv=self.Conv,loss_function=self.loss,device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = (size),dropout = dropout)\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)\n",
    "                #scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.01, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        scheduler=lr_scheduler.StepLR(optimizer, step_size=25,gamma=0.1)\n",
    "        losses=[]\n",
    "        train_accs=[]\n",
    "        test_accs=[]\n",
    "        val_accs=[]\n",
    "        name_of_plot='conv: '+model.conv+', mode: '+model.mode+', loss from '+self.loss[\"Name\"]\n",
    "\n",
    "        print(name_of_plot)\n",
    "\n",
    "        for epoch in range(100):\n",
    "                    print('epoch',epoch)\n",
    "                    loss = self.train(model,self.data,optimizer,LossSampler,train_loader,dropout,epoch)\n",
    "                    losses.append(loss)\n",
    "                    [train_acc_mi, test_acc_mi,val_acc_mi],[train_acc_ma, test_acc_ma,val_acc_ma] = self.test(model,self.data,'logistic regression')\n",
    "                    train_accs_mi.append(train_acc_mi)\n",
    "                    test_accs_mi.append(test_acc_mi)\n",
    "                    train_accs_mi.append(train_acc_ma)\n",
    "                    test_accs_mi.append(test_acc_ma)\n",
    "                    log = 'Loss: {:.4f}, Epoch: {:03d}, Train: {:.4f}, Test: {:.4f}'\n",
    "                    #scheduler.step()\n",
    "                    print(log.format(loss, epoch, train_acc, test_acc))\n",
    "        print('Test acc on the last epoch ', test_acc)\n",
    "        plt.plot(losses)\n",
    "        plt.title(name_of_plot+' loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "        plt.plot(test_accs_mi)\n",
    "        plt.title(name_of_plot+' test f1 micro')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "                  \n",
    "        plt.plot(test_accs_ma)\n",
    "        plt.title(name_of_plot+' test f1 macro')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainOptuna(Main):\n",
    "    def objective(self,trial):\n",
    "        # Integer parameter\n",
    "        hidden_layer = trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "        out_layer = trial.suggest_categorical(\"out_layer\", [32,64,128])\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "        size = trial.suggest_categorical(\"size of network, number of convs\", [1,2,3])\n",
    "        Conv = self.Conv\n",
    "        learning_rate= trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "\n",
    "        # варьируем параметры\n",
    "        loss_to_train={}\n",
    "        for name in self.loss:\n",
    "            \n",
    "            if type(self.loss[name]) == list :\n",
    "                if len(self.loss[name]) == 3:\n",
    "                    var = trial.suggest_int(name,self.loss[name][0],self.loss[name][1],step=self.loss[name][2])\n",
    "                    loss_to_train[name] = var\n",
    "                elif len(self.loss[name]) == 2:\n",
    "                    var_2 = trial.suggest_float(name,self.loss[name][0],self.loss[name][1])\n",
    "                    loss_to_train[name] = var_2\n",
    "                else:\n",
    "                    var_3 = trial.suggest_categorical(name, self.loss[name])\n",
    "                    loss_to_train[name] = var_3\n",
    "            else:\n",
    "                loss_to_train[name] = self.loss[name]\n",
    "        Sampler =loss_to_train[\"Sampler\"]\n",
    "        model = Net(dataset = self.dataset,mode='unsupervised',conv=Conv,loss_function=loss_to_train,device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = size,dropout = dropout)\n",
    "        train_loader = NeighborSampler(self.data.edge_index, batch_size = int(sum(self.train_mask)),node_idx=self.train_mask, sizes=[-1]*size)\n",
    "       # train_loader = NeighborSampler(self.data.edge_index, batch_size = int(sum(self.train_mask)), sizes=[-1]*size)\n",
    "        LossSampler = Sampler(self.data,device=self.device,mask=self.train_mask,loss_info=loss_to_train)\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "\n",
    "        classifier = \"logistic regression\" #trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "\n",
    "        if classifier == \"catboost\":\n",
    "            n_estimators = trial.suggest_int(\"n of estimators\", 10,40,5)\n",
    "            learning_rate_catboost = trial.suggest_float(\"lr_catboost\",5e-4,1e-2)\n",
    "            max_depth = trial.suggest_int(\"max_depth\",1,10,2)\n",
    "        else:\n",
    "            n_estimators = -1\n",
    "            learning_rate_catboost =-1\n",
    "            max_depth = -1\n",
    "        #training of the model\n",
    "        for epoch in range(50):\n",
    "            #print('hi',self.data.x.shape)\n",
    "            loss = self.train(model,self.data,optimizer,LossSampler,train_loader,dropout,epoch)\n",
    "            #self.test(model,self.data,classifier,n_estimators=n_estimators,learning_rate_catboost=learning_rate_catboost,max_depth=max_depth)\n",
    "        [train_acc_mi, test_acc_mi,val_acc_mi], [train_acc_ma, test_acc_ma,val_acc_ma] = self.test(model,self.data,classifier,n_estimators=n_estimators,learning_rate_catboost=learning_rate_catboost,max_depth=max_depth)\n",
    "        trial.report( np.sqrt(val_acc_mi*val_acc_ma) ,epoch)\n",
    "        return np.sqrt(val_acc_mi*val_acc_ma)\n",
    "\n",
    "    \n",
    "    def run(self,number_of_trials):\n",
    "\n",
    "        study = optuna.create_study(direction=\"maximize\",study_name=self.loss[\"Name\"]+\" loss,\"+str(self.Conv)+\" conv\")\n",
    "        study.optimize(self.objective,n_trials = number_of_trials)\n",
    "\n",
    "        print('Best trial:')\n",
    "        trial = study.best_trial\n",
    "        print(\" Value: \", trial.value)\n",
    "        print(\" Params: \")\n",
    "        for key, value in trial.params.items():\n",
    "            print(\" {}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.sampling import Sampler, SamplerContextMatrix, SamplerRandomWalk,SamplerFactorization,SamplerAPP\n",
    "#хорошо\n",
    "DeepWalk = {\"Name\": \"DeepWalk\",\"walk_length\":[5,20,5],\"walks_per_node\":[5,20,5],\"num_negative_samples\":[1,21,5],\"context_size\" : [5,20,5],\"p\":1,\"q\":1,\"loss var\": \"Random Walks\",\"flag_tosave\":True,\"Sampler\" : SamplerRandomWalk } #Проблемы с памятью после того, как увеличила количество тренировочных данных\n",
    "Node2Vec = {\"Name\": \"Node2Vec\",\"walk_length\":[5,20,5],\"walks_per_node\":[5,20,5],\"num_negative_samples\":[1,21,5],\"context_size\" : [5,20,5],\"p\": [0.0,0.9] ,\"q\":[0.0,0.9], \"loss var\": \"Random Walks\",\"flag_tosave\":False,\"Sampler\": SamplerRandomWalk}#то же самое \n",
    "LINE = {\"Name\": \"LINE\",\"C\": \"Adj\",\"num_negative_samples\":[1,21,5],\"loss var\": \"Context Matrix\",\"flag_tosave\":True,\"Sampler\" :SamplerContextMatrix} \n",
    "\n",
    "#плохо\n",
    "HOPE_RPR = {\"Name\": \"HOPE_RPR\",\"C\":\"RPR\",\"loss var\": \"Factorization\",\"flag_tosave\":False,\"alpha\": [0,1],\"Sampler\" :SamplerFactorization,\"lmbda\": [0.0,1.0]} #проверить\n",
    "HOPE_Katz = {\"Name\": \"HOPE_Katz\",\"C\":\"Katz\",\"loss var\": \"Factorization\",\"flag_tosave\":False,\"betta\": [0,1],\"Sampler\" :SamplerFactorization,\"lmbda\": [0.0,1.0]} #проверить\n",
    "HOPE_AA = {\"Name\": \"HOPE_AdamicAdar\",\"C\":\"AA\",\"loss var\": \"Factorization\",\"flag_tosave\":False,\"Sampler\" :SamplerFactorization,\"lmbda\": [0.0,1.0]} \n",
    "LapEigen = {\"Name\": \"LaplacianEigenMaps\", \"C\":\"Adj\",\"loss var\": \"Laplacian EigenMaps\",\"flag_tosave\":True,\"Sampler\" :SamplerFactorization,\"lmbda\": [0.0,1.0]}\n",
    "VERSE_PPR =  {\"Name\": \"VERSE_PPR\",\"C\": \"PPR\",\"num_negative_samples\":[1,21,5],\"loss var\": \"Context Matrix\",\"flag_tosave\":False,\"alpha\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\"Sampler\" :SamplerContextMatrix}\n",
    "\n",
    "#под вопросом\n",
    "VERSE_SR =  {\"Name\": \"VERSE_SimRank\",\"C\": \"SR\",\"num_negative_samples\":[1,21,5],\"loss var\": \"Context Matrix\",\"flag_tosave\":True,\"Sampler\":SamplerContextMatrix} \n",
    "\n",
    "Struc2Vec ={} #Implement\n",
    "#в приципе можно найти нормальны результат через оптюну, но в основном плохо\n",
    "GraphFactorization = {\"Name\": \"Graph Factorization\",\"C\":\"Adj\",\"loss var\": \"Factorization\",\"flag_tosave\":False,\"Sampler\" :SamplerFactorization,\"lmbda\": [0.0,1.0]} \n",
    "HOPE_CN = {\"Name\": \"HOPE_CommonNeighbors\",\"C\":\"CN\",\"loss var\": \"Factorization\",\"flag_tosave\":False,\"Sampler\" :SamplerFactorization,\"lmbda\": [0.0,1.0]} \n",
    "APP ={\"Name\": \"APP\",\"C\": \"PPR\",\"num_negative_samples\":[1,21,5],\"loss var\": \"Context Matrix\",\"flag_tosave\":False,\"alpha\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\"Sampler\" :SamplerAPP}\n",
    "VERSE_Adj =  {\"Name\": \"VERSE_Adj\",\"C\": \"Adj\",\"num_negative_samples\":[1,21,5],\"loss var\": \"Context Matrix\",\"flag_tosave\":False,\"Sampler\" :SamplerContextMatrix} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-05-04 12:17:57,298]\u001b[0m A new study created in memory with name: Graph Factorization loss,GCN conv\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:18:14,091]\u001b[0m Trial 0 finished with value: 0.3477988343982244 and parameters: {'hidden_layer': 64, 'out_layer': 128, 'dropout': 0.5, 'size of network, number of convs': 3, 'lr': 0.0015407563941826838, 'lmbda': 0.22066896764253896}. Best is trial 0 with value: 0.3477988343982244.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:18:25,319]\u001b[0m Trial 1 finished with value: 0.5343784161949882 and parameters: {'hidden_layer': 64, 'out_layer': 64, 'dropout': 0.30000000000000004, 'size of network, number of convs': 1, 'lr': 0.0018314337741819195, 'lmbda': 0.18701281400393122}. Best is trial 1 with value: 0.5343784161949882.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:18:36,147]\u001b[0m Trial 2 finished with value: 0.21922563512288326 and parameters: {'hidden_layer': 32, 'out_layer': 32, 'dropout': 0.1, 'size of network, number of convs': 3, 'lr': 0.008538474831853242, 'lmbda': 0.25983583524993414}. Best is trial 1 with value: 0.5343784161949882.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:18:47,185]\u001b[0m Trial 3 finished with value: 0.44404326390264903 and parameters: {'hidden_layer': 64, 'out_layer': 32, 'dropout': 0.0, 'size of network, number of convs': 2, 'lr': 0.0009113895429794857, 'lmbda': 0.28102513458068246}. Best is trial 1 with value: 0.5343784161949882.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:18:59,314]\u001b[0m Trial 4 finished with value: 0.3232716406206487 and parameters: {'hidden_layer': 64, 'out_layer': 64, 'dropout': 0.1, 'size of network, number of convs': 1, 'lr': 0.009989476737214766, 'lmbda': 0.616797095375986}. Best is trial 1 with value: 0.5343784161949882.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:19:10,796]\u001b[0m Trial 5 finished with value: 0.421781694717402 and parameters: {'hidden_layer': 128, 'out_layer': 64, 'dropout': 0.2, 'size of network, number of convs': 1, 'lr': 0.008445228432992807, 'lmbda': 0.6441447607673191}. Best is trial 1 with value: 0.5343784161949882.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:19:22,642]\u001b[0m Trial 6 finished with value: 0.5297975115215066 and parameters: {'hidden_layer': 128, 'out_layer': 64, 'dropout': 0.0, 'size of network, number of convs': 1, 'lr': 0.004260882107549517, 'lmbda': 0.19331146261838394}. Best is trial 1 with value: 0.5343784161949882.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:19:34,989]\u001b[0m Trial 7 finished with value: 0.2523497605766904 and parameters: {'hidden_layer': 64, 'out_layer': 64, 'dropout': 0.1, 'size of network, number of convs': 2, 'lr': 0.009796619461766423, 'lmbda': 0.5702790257091437}. Best is trial 1 with value: 0.5343784161949882.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:19:47,710]\u001b[0m Trial 8 finished with value: 0.222830244046063 and parameters: {'hidden_layer': 64, 'out_layer': 32, 'dropout': 0.5, 'size of network, number of convs': 3, 'lr': 0.0014194522616011672, 'lmbda': 0.7149646774832986}. Best is trial 1 with value: 0.5343784161949882.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:19:58,125]\u001b[0m Trial 9 finished with value: 0.512249236749586 and parameters: {'hidden_layer': 32, 'out_layer': 32, 'dropout': 0.5, 'size of network, number of convs': 1, 'lr': 0.009972515342594598, 'lmbda': 0.1938914385788636}. Best is trial 1 with value: 0.5343784161949882.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:20:14,492]\u001b[0m Trial 10 finished with value: 0.5351718299047878 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.30000000000000004, 'size of network, number of convs': 1, 'lr': 0.003926685062400167, 'lmbda': 0.0195804476249912}. Best is trial 10 with value: 0.5351718299047878.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:20:30,576]\u001b[0m Trial 11 finished with value: 0.5518893007155253 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.30000000000000004, 'size of network, number of convs': 1, 'lr': 0.0037921605929168165, 'lmbda': 0.002261739004363994}. Best is trial 11 with value: 0.5518893007155253.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:20:46,767]\u001b[0m Trial 12 finished with value: 0.5443599468128416 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.30000000000000004, 'size of network, number of convs': 1, 'lr': 0.004107642278228672, 'lmbda': 0.0056958436143895695}. Best is trial 11 with value: 0.5518893007155253.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:21:02,384]\u001b[0m Trial 13 finished with value: 0.4505486828778657 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.4, 'size of network, number of convs': 1, 'lr': 0.005883212727629469, 'lmbda': 0.8946679402613811}. Best is trial 11 with value: 0.5518893007155253.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:21:18,944]\u001b[0m Trial 14 finished with value: 0.4945081545803082 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.4, 'size of network, number of convs': 1, 'lr': 0.005913340760066352, 'lmbda': 0.0007580900895787224}. Best is trial 11 with value: 0.5518893007155253.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:21:44,433]\u001b[0m Trial 15 finished with value: 0.1587120845132899 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.2, 'size of network, number of convs': 2, 'lr': 0.0031281357565861056, 'lmbda': 0.3991323625771317}. Best is trial 11 with value: 0.5518893007155253.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:22:01,733]\u001b[0m Trial 16 finished with value: 0.5267146811771553 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.4, 'size of network, number of convs': 1, 'lr': 0.004993935279758395, 'lmbda': 0.04422055328961779}. Best is trial 11 with value: 0.5518893007155253.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:22:17,439]\u001b[0m Trial 17 finished with value: 0.5597043588764273 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.30000000000000004, 'size of network, number of convs': 1, 'lr': 0.0025203942838261106, 'lmbda': 0.08479881846374371}. Best is trial 17 with value: 0.5597043588764273.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:22:32,335]\u001b[0m Trial 18 finished with value: 0.5615347847477731 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.2, 'size of network, number of convs': 1, 'lr': 0.002754227208485633, 'lmbda': 0.4421743879221859}. Best is trial 18 with value: 0.5615347847477731.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:22:46,992]\u001b[0m Trial 19 finished with value: 0.5509052122182945 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.2, 'size of network, number of convs': 1, 'lr': 0.002740891209455661, 'lmbda': 0.405842735281713}. Best is trial 18 with value: 0.5615347847477731.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:23:08,715]\u001b[0m Trial 20 finished with value: 0.21013527065814266 and parameters: {'hidden_layer': 128, 'out_layer': 128, 'dropout': 0.2, 'size of network, number of convs': 3, 'lr': 0.0025328305151220156, 'lmbda': 0.7988492102868148}. Best is trial 18 with value: 0.5615347847477731.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:23:24,681]\u001b[0m Trial 21 finished with value: 0.5494097445252063 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.30000000000000004, 'size of network, number of convs': 1, 'lr': 0.00333553924550739, 'lmbda': 0.10851911684003115}. Best is trial 18 with value: 0.5615347847477731.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:23:40,306]\u001b[0m Trial 22 finished with value: 0.5323528847685536 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.4, 'size of network, number of convs': 1, 'lr': 0.005056489898764949, 'lmbda': 0.41778352420857034}. Best is trial 18 with value: 0.5615347847477731.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:23:55,872]\u001b[0m Trial 23 finished with value: 0.5401926410627055 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.30000000000000004, 'size of network, number of convs': 1, 'lr': 0.002330582540703189, 'lmbda': 0.08183427112131832}. Best is trial 18 with value: 0.5615347847477731.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:24:12,082]\u001b[0m Trial 24 finished with value: 0.5294106435335441 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.2, 'size of network, number of convs': 1, 'lr': 0.003491214684894374, 'lmbda': 0.993780340151831}. Best is trial 18 with value: 0.5615347847477731.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:24:36,918]\u001b[0m Trial 25 finished with value: 0.5904498256104176 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.30000000000000004, 'size of network, number of convs': 2, 'lr': 0.0005510274485117277, 'lmbda': 0.3327649328145855}. Best is trial 25 with value: 0.5904498256104176.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-05-04 12:25:03,720]\u001b[0m Trial 26 finished with value: 0.5861463709686868 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.1, 'size of network, number of convs': 2, 'lr': 0.0006132917195819021, 'lmbda': 0.4927642631865051}. Best is trial 25 with value: 0.5904498256104176.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:25:20,550]\u001b[0m Trial 27 finished with value: 0.31666115354637603 and parameters: {'hidden_layer': 32, 'out_layer': 128, 'dropout': 0.1, 'size of network, number of convs': 2, 'lr': 0.0006816446117308415, 'lmbda': 0.48956922630259525}. Best is trial 25 with value: 0.5904498256104176.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:25:46,656]\u001b[0m Trial 28 finished with value: 0.5897781840647496 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.1, 'size of network, number of convs': 2, 'lr': 0.0006319580385911794, 'lmbda': 0.32577736710163896}. Best is trial 25 with value: 0.5904498256104176.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:26:19,565]\u001b[0m Trial 29 finished with value: 0.5914920862773847 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.0, 'size of network, number of convs': 2, 'lr': 0.0006891753135671845, 'lmbda': 0.3484294694610578}. Best is trial 29 with value: 0.5914920862773847.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:26:44,005]\u001b[0m Trial 30 finished with value: 0.5633014322483124 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.0, 'size of network, number of convs': 2, 'lr': 0.0013995263533934563, 'lmbda': 0.30011445133082637}. Best is trial 29 with value: 0.5914920862773847.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:27:07,400]\u001b[0m Trial 31 finished with value: 0.57117447804433 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.0, 'size of network, number of convs': 2, 'lr': 0.0005168935677253492, 'lmbda': 0.34324762235081885}. Best is trial 29 with value: 0.5914920862773847.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:27:32,145]\u001b[0m Trial 32 finished with value: 0.6027427139540538 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.1, 'size of network, number of convs': 2, 'lr': 0.0005133109593953691, 'lmbda': 0.5018013545641902}. Best is trial 32 with value: 0.6027427139540538.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:27:56,997]\u001b[0m Trial 33 finished with value: 0.5252764127511883 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.0, 'size of network, number of convs': 2, 'lr': 0.0017760953117591837, 'lmbda': 0.34800176294285856}. Best is trial 32 with value: 0.6027427139540538.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:28:12,097]\u001b[0m Trial 34 finished with value: 0.4405410336428073 and parameters: {'hidden_layer': 32, 'out_layer': 128, 'dropout': 0.1, 'size of network, number of convs': 2, 'lr': 0.0012180847742755932, 'lmbda': 0.2400967699275237}. Best is trial 32 with value: 0.6027427139540538.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:28:31,178]\u001b[0m Trial 35 finished with value: 0.5756345876589541 and parameters: {'hidden_layer': 256, 'out_layer': 32, 'dropout': 0.1, 'size of network, number of convs': 2, 'lr': 0.0019615290429614296, 'lmbda': 0.33230290767957654}. Best is trial 32 with value: 0.6027427139540538.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:28:49,610]\u001b[0m Trial 36 finished with value: 0.5765211495741811 and parameters: {'hidden_layer': 128, 'out_layer': 128, 'dropout': 0.0, 'size of network, number of convs': 2, 'lr': 0.0009784488243865853, 'lmbda': 0.5691634356153248}. Best is trial 32 with value: 0.6027427139540538.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:29:10,190]\u001b[0m Trial 37 finished with value: 0.6021282782794717 and parameters: {'hidden_layer': 256, 'out_layer': 64, 'dropout': 0.1, 'size of network, number of convs': 2, 'lr': 0.0005167954556925265, 'lmbda': 0.5449338737824445}. Best is trial 32 with value: 0.6027427139540538.\u001b[0m\n",
      "\u001b[32m[I 2021-05-04 12:29:21,988]\u001b[0m Trial 38 finished with value: 0.5676943962096694 and parameters: {'hidden_layer': 64, 'out_layer': 64, 'dropout': 0.1, 'size of network, number of convs': 2, 'lr': 0.0019240372727712656, 'lmbda': 0.5642110958253448}. Best is trial 32 with value: 0.6027427139540538.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "device =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "loss = GraphFactorization\n",
    "MO = MainOptuna('GCN', device, loss , 'unsupervised',dataset,data,train_indices,val_indices,test_indices,train_mask,val_mask,test_mask)\n",
    "MO.run(number_of_trials=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "My__RW_Neighbour.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
