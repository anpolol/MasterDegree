{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-gN3S4NhAjs",
    "outputId": "fef06879-cb42-4f09-db2d-88671e8c4315"
   },
   "outputs": [],
   "source": [
    "#!pip install -q torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 torchtext==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 21 10:12:32 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 456.71       Driver Version: 456.71       CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 207... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   54C    P0    63W / 215W |    442MiB /  8192MiB |      1%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       988    C+G   ...ekyb3d8bbwe\\HxOutlook.exe    N/A      |\n",
      "|    0   N/A  N/A      1384    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      1424    C+G   ...kyb3d8bbwe\\HxAccounts.exe    N/A      |\n",
      "|    0   N/A  N/A      1744    C+G   ...w5n1h2txyewy\\SearchUI.exe    N/A      |\n",
      "|    0   N/A  N/A      2232    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A      2464    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      3096    C+G   ...es.TextInput.InputApp.exe    N/A      |\n",
      "|    0   N/A  N/A      4828    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A      6196    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A      7796    C+G   ...kyb3d8bbwe\\Calculator.exe    N/A      |\n",
      "|    0   N/A  N/A     10796    C+G   ...3d8bbwe\\MicrosoftEdge.exe    N/A      |\n",
      "|    0   N/A  N/A     11868    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     13488    C+G   ...b3d8bbwe\\WinStore.App.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -q --no-index torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
    "#!pip install -q --no-index torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
    "#!pip install -q --no-index torch-cluster -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
    "#!pip install -q --no-index torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
    "#!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import optuna\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from torch_geometric.data import NeighborSampler\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import numpy as np\n",
    "from modules.model import Net\n",
    "from sklearn.metrics import precision_score\n",
    "from datetime import datetime\n",
    "from modules.sampling import Sampler\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "class Main():\n",
    "    def __init__(self,conv, device, loss_function, mode = 'unsupervised',**kwargs):\n",
    "        self.dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "        data = self.dataset[0]\n",
    "        self.Conv = conv\n",
    "        self.device = device# if torch.cuda.is_available() else 'cpu')\n",
    "        self.x = data.x#.to(device)\n",
    "        self.y = data.y.squeeze()#.to(device)\n",
    "        self.data=data.to(device)\n",
    "        self.loss = loss_function\n",
    "        self.mode = mode\n",
    "        self.train_mask = torch.tensor([True]*int(0.8*len(data.x)+1) + [False]*int(0.2*len(data.x)))\n",
    "        #val_mask = torch.tensor([False]*int(0.6*len(data.x)+1) + [True]*int(0.2*len(data.x)+1)+[False]*int(0.2*len(data.x)))\n",
    "        self.test_mask = torch.tensor([False]*int(0.8*len(data.x)+1) + [True]*int(0.2*len(data.x)))\n",
    "        self.flag = self.loss[\"flag\"]\n",
    "        \n",
    "      #  if self.flag:\n",
    "       #     name_of_file = \"samples_\"+self.loss[\"Name\"]+\".pickle\"\n",
    "        #    if os.path.exists(name_of_file):\n",
    "         #       with open(name_of_file,'rb') as f:\n",
    "          #          self.samples = pickle.load(f)\n",
    "           # else:\n",
    "            #    arr=torch.nonzero(self.train_mask == True)\n",
    "             #   indices_of_train_data = ([item for sublist in arr for item in sublist])\n",
    "              #  self.samples = self.LossSampler.sample(indices_of_train_data,alpha = self.alpha)\n",
    "\n",
    "        super(Main, self).__init__()\n",
    "    \n",
    "    def train(self, model,data,optimizer,Sampler,train_loader,dropout,epoch,**kwargs):\n",
    "        model.train()   \n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        if model.mode == 'unsupervised':\n",
    "            if model.conv=='GCN':\n",
    "                arr=torch.nonzero(self.train_mask == True)\n",
    "                indices_of_train_data = ([item for sublist in arr for item in sublist])\n",
    "                d_out = datetime.now()\n",
    "                out = model.inference(data.to(device),dp=dropout)\n",
    "                #print('свертка посчиталась за: ',datetime.now() - d_out)\n",
    "                d_samples=datetime.now()\n",
    "                if (self.flag and epoch == 0) or not self.flag:\n",
    "                    self.samples = Sampler.sample(indices_of_train_data) \n",
    "\n",
    "                #print('сэмплирование соседей для ф.п./матрица similarity: ', datetime.now()-d_samples)\n",
    "\n",
    "                d_loss = datetime.now()\n",
    "                loss = model.loss(out[self.train_mask], self.samples)\n",
    "                #print('подсчет функции потерь: ', datetime.now()-d_loss)\n",
    "                total_loss+=loss\n",
    "            else:\n",
    "                for batch_size, n_id, adjs in train_loader:\n",
    "                    if len(train_loader.sizes) == 1:\n",
    "                        adjs = [adjs]\n",
    "                    # adjs holds a list of (edge_index, e_id, size) tuples.\n",
    "                    adjs = [adj.to(device) for adj in adjs]\n",
    "                    d_out = datetime.now()\n",
    "                    out = model.forward(data.x[n_id.to(device)].to(device), adjs)\n",
    "                   # print('свертка (на батче) посчиталась за: ',datetime.now() - d_out)\n",
    "                    d_samples=datetime.now()\n",
    "                    if (self.flag and epoch == 0) or not self.flag:\n",
    "                        self.samples = Sampler.sample(n_id[:batch_size]) \n",
    "\n",
    "                                          \n",
    "                    #print('сэмплирование соседей для ф.п./матрица similarity: ', datetime.now()-d_samples)\n",
    "                    d_loss = datetime.now()\n",
    "                    loss = model.loss(out, self.samples)#pos_batch.to(device), neg_batch.to(device))\n",
    "                    #print('подсчет функции потерь: ', datetime.now()-d_loss)\n",
    "                    total_loss+=loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()      \n",
    "            return total_loss /len(train_loader)\n",
    "        elif model.mode== 'supervised':\n",
    "            if model.conv=='GCN':\n",
    "                out = model.inference(data.to(device),dp=dropout)\n",
    "                loss = model.loss_sup(out[self.train_mask],y[self.train_mask])\n",
    "                total_loss+=loss\n",
    "            else:\n",
    "                for batch_size, n_id, adjs in train_loader:\n",
    "                    adjs = [adj for adj in adjs]\n",
    "                    out = model.forward(data.x[n_id].to(device), adjs)\n",
    "                    loss = model.loss_sup(out,y[n_id[:batch_size]])\n",
    "                    total_loss+=loss\n",
    "            total_loss.backward(retain_graph=True)\n",
    "            optimizer.step()      \n",
    "            return total_loss /len(train_loader)       \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test(self, model,data,classifier,**kwargs):#,n_estimators,learning_rate_carboost, max_depth): \n",
    "        model.eval()\n",
    "        out = model.inference(data.to(device))\n",
    "        y_true = self.y.detach().numpy()\n",
    "        if model.mode == 'supervised':\n",
    "            y_true = self.y.unsqueeze(-1)\n",
    "            y_pred = out.cpu().argmax(dim=-1, keepdim=True)\n",
    "            accs = []\n",
    "            for mask in [self.train_mask, self.val_mask, self.test_mask]:    \n",
    "                accs+=[int(y_pred[mask].eq(y_true[mask]).sum()) / int(mask.sum())]\n",
    "            return accs\n",
    "        elif model.mode == 'unsupervised': \n",
    "            if classifier == 'logistic regression':\n",
    "                clf = LogisticRegressionCV(cv = 5, max_iter = 3000).fit(out.cpu().detach()[self.train_mask].numpy(), self.y.detach()[self.train_mask].numpy())\n",
    "            else:\n",
    "                n_estimators= kwargs[\"n_estimators\"]\n",
    "                learning_rate_catboost = kwargs[\"learning_rate_catboost\"]\n",
    "                max_depth = kwargs[\"max_depth\"]\n",
    "                clf = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate = learning_rate_catboost, max_depth=max_depth, random_state=0)\n",
    "                clf.fit(out.cpu().detach()[self.train_mask].numpy(), self.y.detach()[self.train_mask].numpy())\n",
    "            accs = []\n",
    "            for mask in [self.train_mask, self.test_mask]:    \n",
    "                accs += [(clf.score(out.cpu().detach()[mask].numpy(), self.y.detach()[mask].cpu().numpy()))]\n",
    "            return accs\n",
    "            \n",
    "\n",
    "    def run(self,**kwargs):\n",
    "        \n",
    "        hidden_layer = 64\n",
    "        out_layer = 128\n",
    "        dropout = 0.4\n",
    "        size = 1\n",
    "        learning_rate = 0.001\n",
    "\n",
    "        classifier = \"logistic regression\"\n",
    "        train_loader = NeighborSampler(self.data.edge_index, node_idx=self.train_mask, batch_size = int(sum(self.train_mask)), sizes=[-1]*size)\n",
    "        LossSampler = Sampler(self.data,device=device,mask=self.train_mask,loss_info=self.loss)\n",
    "        model = Net(dataset = self.dataset,mode=self.mode,conv=self.Conv,loss_function=self.loss,device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = (size),dropout = dropout)\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)\n",
    "                #scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.01, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        scheduler=lr_scheduler.StepLR(optimizer, step_size=25,gamma=0.1)\n",
    "        losses=[]\n",
    "        train_accs=[]\n",
    "        test_accs=[]\n",
    "        val_accs=[]\n",
    "        name_of_plot='conv: '+model.conv+', mode: '+model.mode+', loss from '+self.loss[\"Name\"]\n",
    "\n",
    "        print(name_of_plot)\n",
    "\n",
    "        for epoch in range(50):\n",
    "                    print('epoch',epoch)\n",
    "                    loss = self.train(model,self.data,optimizer,LossSampler,train_loader,0,0.7, epoch)\n",
    "                    losses.append(loss)\n",
    "                    d_test = datetime.now()\n",
    "                    train_acc, test_acc = self.test(model,self.data,'logistic regression')\n",
    "                   # print('тестирование заняло: ', datetime.now()-d_test)\n",
    "                    train_accs.append(train_acc)\n",
    "                    test_accs.append(test_acc)\n",
    "                   # val_accs.append(val_acc)\n",
    "                    log = 'Loss: {:.4f}, Epoch: {:03d}, Train: {:.4f}, Test: {:.4f}'\n",
    "                    #scheduler.step()\n",
    "                    print(log.format(loss, epoch, train_acc, test_acc))\n",
    "        print('Test acc on the last epoch ', test_acc)\n",
    "        plt.plot(losses)\n",
    "        plt.title(name_of_plot+' loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "        plt.plot(test_accs)\n",
    "        plt.title(name_of_plot+' loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "                    #return [precision_score(y_true[train_mask.cpu()], best_preds_train, average='macro'), precision_score(y_true[val_mask.cpu()], best_preds_val, average='macro'),precision_score(y_true[test_mask.cpu()], best_preds, average='macro')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainOptuna(Main):\n",
    "    def objective(self,trial):\n",
    "        # Integer parameter\n",
    "        hidden_layer = trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "        out_layer = trial.suggest_categorical(\"out_layer\", [32,64,128])\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "        size = trial.suggest_categorical(\"size of network, number of convs\", [1,2,3])\n",
    "        Conv = self.Conv# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "        if self.loss[\"Name\"] == \"SAGE\" or self.loss[\"Name\"] == \"DeepWalk\" or self.loss[\"Name\"] == \"Node2Vec\":\n",
    "            walk_length = trial.suggest_int(\"walk length\",5,20,step = 5)\n",
    "            walks_per_node = trial.suggest_int(\"walks per node\", 5,10,step = 5)\n",
    "            context_size = trial.suggest_int(\"context size\",5,20,step = 5)\n",
    "        if self.loss[\"Name\"] == \"Node2Vec\":\n",
    "            q = trial.suggest_float(\"q\", 0.0,0.9,step = 0.1)\n",
    "            p = trial.suggest_float(\"p\", 0.0,0.9,step = 0.1)\n",
    "        if self.loss[\"C\"] == \"RPR\" or self.loss[\"C\"] == \"PPR\":\n",
    "            alpha = trial.suggest_float(\"alpha\", 0.5,1)\n",
    "\n",
    "        num_negative_samples = trial.suggest_int(\"num_negative_samples\",1,16,step = 5)\n",
    "        self.loss =  {\"Name\": \"VERSE_Adj\",\"C\": \"Adj\",\"num negative samples\":num_negative_samples,\"loss var\": \"Context Matrix\",\"flag\":True} \n",
    "\n",
    "        model = Net(dataset = self.dataset,mode='unsupervised',conv=Conv,loss_function=self.loss,device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = size,dropout = dropout)\n",
    "\n",
    "        train_loader = NeighborSampler(self.data.edge_index, batch_size = len(self.train_mask),node_idx=self.train_mask, sizes=[-1]*size)\n",
    "        LossSampler = Sampler(self.data,device=self.device,mask=self.train_mask,loss_info=self.loss)\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "    #optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "        learning_rate= trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "\n",
    "        classifier = \"logistic regression\" #trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "\n",
    "        if classifier == \"catboost\":\n",
    "            n_estimators = trial.suggest_int(\"n of estimators\", 10,40,5)\n",
    "            learning_rate_catboost = trial.suggest_float(\"lr_catboost\",5e-4,1e-2)\n",
    "            max_depth = trial.suggest_int(\"max_depth\",1,10,2)\n",
    "        else:\n",
    "            n_estimators = -1\n",
    "            learning_rate_catboost =-1\n",
    "            max_depth = -1\n",
    "        #training of the model\n",
    "        for epoch in range(50):\n",
    "            loss = self.train(model,self.data,optimizer,LossSampler,train_loader,dropout,epoch)\n",
    "\n",
    "        train_acc, test_acc = self.test(model,self.data,classifier,n_estimators=n_estimators,learning_rate_catboost=learning_rate_catboost,max_depth=max_depth)\n",
    "\n",
    "\n",
    "        trial.report(test_acc,epoch)\n",
    "\n",
    "        return test_acc\n",
    "\n",
    "    \n",
    "    def run(self):\n",
    "\n",
    "\n",
    "        self.Conv = \"GCN\"\n",
    "        study = optuna.create_study(direction=\"maximize\",study_name=self.loss[\"Name\"]+\" loss, GCN conv\")\n",
    "        study.optimize(self.objective,n_trials = 10)\n",
    "\n",
    "        print('Best trial:')\n",
    "        trial = study.best_trial\n",
    "        print(\" Value: \", trial.value)\n",
    "        print(\" Params: \")\n",
    "        for key, value in trial.params.items():\n",
    "            print(\" {}: {}\".format(key,value))\n",
    "        \n",
    "        self.Conv = \"SAGE\"\n",
    "        study = optuna.create_study(direction=\"maximize\",study_name=self.loss[\"Name\"]+\" loss, SAGE conv\")\n",
    "        study.optimize(self.objective,n_trials = 10)\n",
    "\n",
    "        print('Best trial:')\n",
    "        trial = study.best_trial\n",
    "        print(\" Value: \", trial.value)\n",
    "        print(\" Params: \")\n",
    "        for key, value in trial.params.items():\n",
    "            print(\" {}: {}\".format(key,value))\n",
    "        \n",
    "        self.Conv = \"GAT\"\n",
    "        study = optuna.create_study(direction=\"maximize\",study_name=self.loss[\"Name\"]+\" loss, GAT conv\")\n",
    "        study.optimize(self.objective,n_trials = 10)\n",
    "\n",
    "        print('Best trial:')\n",
    "        trial = study.best_trial\n",
    "        print(\" Value: \", trial.value)\n",
    "        print(\" Params: \")\n",
    "        for key, value in trial.params.items():\n",
    "            print(\" {}: {}\".format(key,value))    \n",
    "            \n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAGE = {\"Name\":\"SAGE\" , \"walk length\":5,\"walks per node\":50,\"num negative samples\":20,\"context size\" : 10,\"p\":1,\"q\":1, \"loss var\": \"Random Walks\",\"flag\":False}\n",
    "DeepWalk = {\"Name\": \"DeepWalk\",\"walk length\":10,\"walks per node\":10,\"num negative samples\":10,\"context size\" : 10,\"p\":1,\"q\":1,\"loss var\": \"Random Walks\",\"flag\":False } #Проблемы с памятью после того, как увеличила количество тренировочных данных\n",
    "#DeepWalk = {\"Name\": \"DeepWalk\",\"walk length\":40,\"walks per node\":80,\"num negative samples\":20,\"context size\" : 10,\"p\":1,\"q\":1,\"loss var\": \"Random Walks\" } #Проблемы с памятью после того, как увеличила количество тренировочных данных\n",
    "Node2Vec = {\"Name\": \"Node2Vec\",\"walk length\":20,\"walks per node\":10,\"num negative samples\":1,\"context size\" : 10,\"p\":1.414 ,\"q\":1.414, \"loss var\": \"Random Walks\",\"flag\":False}#то же самое \n",
    "#Node2Vec = {\"Name\": \"Node2Vec\",\"walk length\":100,\"walks per node\":18,\"num negative samples\":20,\"context size\" : 16,\"p\":1.414 ,\"q\":1.414, \"loss var\": \"Random Walks\"}#то же самое \n",
    "\n",
    "LINE = {\"Name\": \"LINE\",\"C\": \"Adj\",\"num negative samples\":10,\"loss var\": \"Context Matrix\",\"flag\":True} \n",
    "HOPE_RPR = {\"Name\": \"HOPE_RPR\",\"C\":\"RPR\",\"loss var\": \"Factorization\",\"flag\":True} #проверить\n",
    "HOPE_Katz = {\"Name\": \"HOPE_Katz\",\"C\":\"Katz\",\"loss var\": \"Factorization\",\"flag\":True} #проверить\n",
    "HOPE_CN = {\"Name\": \"HOPE_CommonNeighbors\",\"C\":\"CN\",\"loss var\": \"Factorization\",\"flag\":True} \n",
    "HOPE_AA = {\"Name\": \"HOPE_AdamicAdar\",\"C\":\"AA\",\"loss var\": \"Factorization\",\"flag\":True} \n",
    "\n",
    "LapEigen = {\"Name\": \"LaplacianEigenMaps\", \"C\":\"Adj\",\"loss var\": \"Laplacian EigenMaps\",\"flag\":True}\n",
    "\n",
    "GraphFactorization = {\"Name\": \"Graph Factorization\",\"C\":\"Adj\",\"loss var\": \"Factorization\",\"flag\":True} \n",
    "VERSE_PPR =  {\"Name\": \"VERSE_PPR\",\"C\": \"PPR\",\"num negative samples\":10,\"loss var\": \"Context Matrix\",\"flag\":True}\n",
    "\n",
    "VERSE_SR =  {\"Name\": \"VERSE_SimRank\",\"C\": \"SR\",\"num negative samples\":10,\"loss var\": \"Context Matrix\",\"flag\":True} \n",
    "VERSE_Adj =  {\"Name\": \"VERSE_Adj\",\"C\": \"Adj\",\"num negative samples\":10,\"loss var\": \"Context Matrix\",\"flag\":True} \n",
    "\n",
    "APP ={}#Implement\n",
    "Struc2Vec ={} #Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-21 10:12:36,079]\u001b[0m A new study created in memory with name: VERSE_Adj loss, GAT conv\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 10:14:10,908]\u001b[0m Trial 0 finished with value: 0.8022181146025879 and parameters: {'hidden_layer': 32, 'out_layer': 32, 'dropout': 0.5, 'size of network, number of convs': 1, 'num_negative_samples': 11, 'lr': 0.0037438473939578614}. Best is trial 0 with value: 0.8022181146025879.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 10:15:47,695]\u001b[0m Trial 1 finished with value: 0.8133086876155268 and parameters: {'hidden_layer': 64, 'out_layer': 64, 'dropout': 0.4, 'size of network, number of convs': 1, 'num_negative_samples': 6, 'lr': 0.009329630500344502}. Best is trial 1 with value: 0.8133086876155268.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 10:17:07,875]\u001b[0m Trial 2 finished with value: 0.3179297597042514 and parameters: {'hidden_layer': 128, 'out_layer': 64, 'dropout': 0.1, 'size of network, number of convs': 2, 'num_negative_samples': 16, 'lr': 0.006806163697828752}. Best is trial 1 with value: 0.8133086876155268.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 10:18:40,541]\u001b[0m Trial 3 finished with value: 0.8040665434380776 and parameters: {'hidden_layer': 64, 'out_layer': 32, 'dropout': 0.0, 'size of network, number of convs': 1, 'num_negative_samples': 11, 'lr': 0.008515618304068072}. Best is trial 1 with value: 0.8133086876155268.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 10:20:09,293]\u001b[0m Trial 4 finished with value: 0.3179297597042514 and parameters: {'hidden_layer': 32, 'out_layer': 64, 'dropout': 0.0, 'size of network, number of convs': 2, 'num_negative_samples': 16, 'lr': 0.005273945505316889}. Best is trial 1 with value: 0.8133086876155268.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 10:21:43,164]\u001b[0m Trial 5 finished with value: 0.7781885397412199 and parameters: {'hidden_layer': 64, 'out_layer': 32, 'dropout': 0.2, 'size of network, number of convs': 1, 'num_negative_samples': 6, 'lr': 0.00390538668916377}. Best is trial 1 with value: 0.8133086876155268.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 10:23:18,372]\u001b[0m Trial 6 finished with value: 0.3179297597042514 and parameters: {'hidden_layer': 32, 'out_layer': 128, 'dropout': 0.2, 'size of network, number of convs': 2, 'num_negative_samples': 11, 'lr': 0.009451342993040538}. Best is trial 1 with value: 0.8133086876155268.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 10:24:55,262]\u001b[0m Trial 7 finished with value: 0.8243992606284658 and parameters: {'hidden_layer': 64, 'out_layer': 64, 'dropout': 0.4, 'size of network, number of convs': 1, 'num_negative_samples': 16, 'lr': 0.008293519025088082}. Best is trial 7 with value: 0.8243992606284658.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 10:26:15,800]\u001b[0m Trial 8 finished with value: 0.3179297597042514 and parameters: {'hidden_layer': 32, 'out_layer': 128, 'dropout': 0.2, 'size of network, number of convs': 3, 'num_negative_samples': 6, 'lr': 0.0098456778338252}. Best is trial 7 with value: 0.8243992606284658.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 10:27:36,862]\u001b[0m Trial 9 finished with value: 0.3179297597042514 and parameters: {'hidden_layer': 256, 'out_layer': 32, 'dropout': 0.4, 'size of network, number of convs': 3, 'num_negative_samples': 11, 'lr': 0.0026012360693100484}. Best is trial 7 with value: 0.8243992606284658.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 10:27:36,877]\u001b[0m A new study created in memory with name: VERSE_Adj loss, SAGE conv\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      " Value:  0.8243992606284658\n",
      " Params: \n",
      " hidden_layer: 64\n",
      " out_layer: 64\n",
      " dropout: 0.4\n",
      " size of network, number of convs: 1\n",
      " num_negative_samples: 16\n",
      " lr: 0.008293519025088082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-21 10:29:08,410]\u001b[0m Trial 0 finished with value: 0.7966728280961183 and parameters: {'hidden_layer': 128, 'out_layer': 64, 'dropout': 0.5, 'size of network, number of convs': 1, 'num_negative_samples': 1, 'lr': 0.007662113449191953}. Best is trial 0 with value: 0.7966728280961183.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 10:30:37,666]\u001b[0m Trial 1 finished with value: 0.6469500924214417 and parameters: {'hidden_layer': 128, 'out_layer': 32, 'dropout': 0.1, 'size of network, number of convs': 1, 'num_negative_samples': 6, 'lr': 0.0009343452769613199}. Best is trial 0 with value: 0.7966728280961183.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 10:32:18,307]\u001b[0m Trial 2 finished with value: 0.5508317929759704 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.4, 'size of network, number of convs': 2, 'num_negative_samples': 16, 'lr': 0.007985304254885993}. Best is trial 0 with value: 0.7966728280961183.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 10:33:52,279]\u001b[0m Trial 3 finished with value: 0.7689463955637708 and parameters: {'hidden_layer': 256, 'out_layer': 32, 'dropout': 0.5, 'size of network, number of convs': 1, 'num_negative_samples': 6, 'lr': 0.005275134595160281}. Best is trial 0 with value: 0.7966728280961183.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 10:35:24,807]\u001b[0m Trial 4 finished with value: 0.5378927911275416 and parameters: {'hidden_layer': 64, 'out_layer': 32, 'dropout': 0.2, 'size of network, number of convs': 2, 'num_negative_samples': 1, 'lr': 0.009336212699976185}. Best is trial 0 with value: 0.7966728280961183.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 10:36:55,933]\u001b[0m Trial 5 finished with value: 0.5194085027726433 and parameters: {'hidden_layer': 32, 'out_layer': 64, 'dropout': 0.30000000000000004, 'size of network, number of convs': 2, 'num_negative_samples': 16, 'lr': 0.005581556439688022}. Best is trial 0 with value: 0.7966728280961183.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 10:38:28,165]\u001b[0m Trial 6 finished with value: 0.767097966728281 and parameters: {'hidden_layer': 128, 'out_layer': 64, 'dropout': 0.1, 'size of network, number of convs': 1, 'num_negative_samples': 16, 'lr': 0.0009962803982087043}. Best is trial 0 with value: 0.7966728280961183.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 10:39:56,739]\u001b[0m Trial 7 finished with value: 0.5804066543438078 and parameters: {'hidden_layer': 128, 'out_layer': 128, 'dropout': 0.4, 'size of network, number of convs': 3, 'num_negative_samples': 6, 'lr': 0.0016482683490881152}. Best is trial 0 with value: 0.7966728280961183.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 10:41:33,034]\u001b[0m Trial 8 finished with value: 0.8022181146025879 and parameters: {'hidden_layer': 256, 'out_layer': 64, 'dropout': 0.5, 'size of network, number of convs': 1, 'num_negative_samples': 11, 'lr': 0.009823445521725012}. Best is trial 8 with value: 0.8022181146025879.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 10:43:09,329]\u001b[0m Trial 9 finished with value: 0.5249537892791127 and parameters: {'hidden_layer': 32, 'out_layer': 32, 'dropout': 0.4, 'size of network, number of convs': 3, 'num_negative_samples': 1, 'lr': 0.008572063531107045}. Best is trial 8 with value: 0.8022181146025879.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 10:43:09,329]\u001b[0m A new study created in memory with name: VERSE_Adj loss, GCN conv\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      " Value:  0.8022181146025879\n",
      " Params: \n",
      " hidden_layer: 256\n",
      " out_layer: 64\n",
      " dropout: 0.5\n",
      " size of network, number of convs: 1\n",
      " num_negative_samples: 11\n",
      " lr: 0.009823445521725012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-21 10:44:41,452]\u001b[0m Trial 0 finished with value: 0.5804066543438078 and parameters: {'hidden_layer': 32, 'out_layer': 32, 'dropout': 0.5, 'size of network, number of convs': 3, 'num_negative_samples': 6, 'lr': 0.0025622630013137936}. Best is trial 0 with value: 0.5804066543438078.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 10:46:05,734]\u001b[0m Trial 1 finished with value: 0.3179297597042514 and parameters: {'hidden_layer': 32, 'out_layer': 64, 'dropout': 0.2, 'size of network, number of convs': 2, 'num_negative_samples': 1, 'lr': 0.00876622933238325}. Best is trial 0 with value: 0.5804066543438078.\u001b[0m\n",
      "\u001b[32m[I 2021-04-21 10:47:26,422]\u001b[0m Trial 2 finished with value: 0.3179297597042514 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.0, 'size of network, number of convs': 3, 'num_negative_samples': 1, 'lr': 0.006966361252059097}. Best is trial 0 with value: 0.5804066543438078.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "MO = MainOptuna('SAGE', device, VERSE_Adj, mode = 'unsupervised')\n",
    "MO.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "My__RW_Neighbour.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
