{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-gN3S4NhAjs",
    "outputId": "fef06879-cb42-4f09-db2d-88671e8c4315"
   },
   "outputs": [],
   "source": [
    "#!pip install -q torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 torchtext==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 23 12:51:13 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 456.71       Driver Version: 456.71       CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 207... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   52C    P0    64W / 215W |    605MiB /  8192MiB |      1%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1936    C+G   ...kyb3d8bbwe\\Calculator.exe    N/A      |\n",
      "|    0   N/A  N/A      2612    C+G   ...3d8bbwe\\MicrosoftEdge.exe    N/A      |\n",
      "|    0   N/A  N/A      2700    C+G   ...kyb3d8bbwe\\HxAccounts.exe    N/A      |\n",
      "|    0   N/A  N/A      3028    C+G   ...es.TextInput.InputApp.exe    N/A      |\n",
      "|    0   N/A  N/A      4440    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A      4764    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      5916    C+G   ...b3d8bbwe\\WinStore.App.exe    N/A      |\n",
      "|    0   N/A  N/A      8040    C+G   ...ekyb3d8bbwe\\HxOutlook.exe    N/A      |\n",
      "|    0   N/A  N/A     10220    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10656    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A     10724    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     10800    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     10948    C+G   ...w5n1h2txyewy\\SearchUI.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -q --no-index torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
    "#!pip install -q --no-index torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
    "#!pip install -q --no-index torch-cluster -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
    "#!pip install -q --no-index torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
    "#!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import optuna\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from torch_geometric.data import NeighborSampler\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import numpy as np\n",
    "from modules.model import Net\n",
    "from sklearn.metrics import f1_score\n",
    "from modules.sampling import Sampler, SamplerContextMatrix, SamplerRandomWalk\n",
    "from datetime import datetime\n",
    "from torch_geometric.data import GraphSAINTRandomWalkSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-2b3f501c6c39>, line 121)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-17-2b3f501c6c39>\"\u001b[1;36m, line \u001b[1;32m121\u001b[0m\n\u001b[1;33m    accs + = [f1_score(out.cpu().detach()[mask].numpy(), self.y.detach()[mask].cpu().numpy(), average='macro')]#, precision_score(y_true[val_mask.cpu()], best_preds_val, average='macro'),precision_score(y_true[test_mask.cpu()], best_preds, average='macro')]\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "class Main():\n",
    "    def __init__(self,conv, device, loss_function, mode = 'unsupervised',**kwargs):\n",
    "        self.dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "        data = self.dataset[0]\n",
    "        self.Conv = conv\n",
    "        self.device = device# if torch.cuda.is_available() else 'cpu')\n",
    "        self.x = data.x#.to(device)\n",
    "        self.y = data.y.squeeze()#.to(device)\n",
    "        self.data=data.to(device)\n",
    "        self.loss = loss_function\n",
    "        self.mode = mode\n",
    "        self.train_mask = torch.tensor([True]*int(0.8*len(data.x)+1) + [False]*int(0.2*len(data.x)))\n",
    "        #val_mask = torch.tensor([False]*int(0.6*len(data.x)+1) + [True]*int(0.2*len(data.x)+1)+[False]*int(0.2*len(data.x)))\n",
    "        self.test_mask = torch.tensor([False]*int(0.8*len(data.x)+1) + [True]*int(0.2*len(data.x)))\n",
    "        self.flag = self.loss[\"flag\"]\n",
    "        \n",
    "        super(Main, self).__init__()\n",
    "    \n",
    "    def train(self, model,data,optimizer,Sampler,train_loader,dropout,epoch,**kwargs):\n",
    "        model.train()   \n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        if model.mode == 'unsupervised':\n",
    "            if model.conv=='GCN':\n",
    "                arr=torch.nonzero(self.train_mask == True)\n",
    "                indices_of_train_data = ([item for sublist in arr for item in sublist])\n",
    "                d_out = datetime.now()\n",
    "                out = model.inference(data.to(device),dp=dropout)\n",
    "                #print('свертка посчиталась за: ',datetime.now() - d_out)\n",
    "                d_samples=datetime.now()\n",
    "                if (epoch == 0): \n",
    "                    \n",
    "                #    name_of_file = \"samples_\"+self.loss[\"Name\"]+\"_alpha_\"+str(self.loss[\"alpha\"])+\".pickle\"\n",
    "                    \n",
    "                 #   if os.path.exists(name_of_file):\n",
    "                  #      with open(name_of_file,'rb') as f:\n",
    "                   #         self.samples = pickle.load(f)\n",
    "                    #else:\n",
    "                        arr=torch.nonzero(self.train_mask == True)\n",
    "                        indices_of_train_data = ([item for sublist in arr for item in sublist])\n",
    "                        self.samples = Sampler.sample(indices_of_train_data)\n",
    "                     #   with open(name_of_file,'wb') as f:\n",
    "                      #      self.samples = pickle.dump(self.samples,f)\n",
    "               \n",
    "             #   print('сэмплирование соседей для ф.п./матрица similarity: ', datetime.now()-d_samples)\n",
    "                d_loss = datetime.now()\n",
    "                loss = model.loss(out[self.train_mask], self.samples)\n",
    "              #  print('подсчет функции потерь: ', datetime.now()-d_loss)\n",
    "                total_loss+=loss\n",
    "            else:\n",
    "                for batch_size, n_id, adjs in train_loader:\n",
    "                    if len(train_loader.sizes) == 1:\n",
    "                        adjs = [adjs]\n",
    "                    adjs = [adj.to(device) for adj in adjs]\n",
    "                    d_out = datetime.now()\n",
    "                    out = model.forward(data.x[n_id.to(device)].to(device), adjs)\n",
    "                    d_samples=datetime.now()\n",
    "                    if (epoch == 0): \n",
    "                        #name_of_file = \"samples_\"+self.loss[\"Name\"]+\"_alpha_\"+str(self.loss[\"alpha\"])+\".pickle\"\n",
    "                        \n",
    "                        #if os.path.exists(name_of_file):\n",
    "                         #   with open(name_of_file,'rb') as f:\n",
    "                          #      self.samples = pickle.load(f)\n",
    "                        #else:\n",
    "                            arr=torch.nonzero(self.train_mask == True)\n",
    "                            indices_of_train_data = ([item for sublist in arr for item in sublist])\n",
    "                            self.samples = Sampler.sample(n_id[:batch_size])\n",
    "                         #   with open(name_of_file,'wb') as f:\n",
    "                          #      self.samples = pickle.dump(self.samples,f)\n",
    "                   \n",
    "                        \n",
    "                    #print('сэмплирование соседей для ф.п./матрица similarity: ', datetime.now()-d_samples)\n",
    "                    d_loss = datetime.now()\n",
    "                    loss = model.loss(out, self.samples)#pos_batch.to(device), neg_batch.to(device))\n",
    "                    #print('подсчет функции потерь: ', datetime.now()-d_loss)\n",
    "                    total_loss+=loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()      \n",
    "            return total_loss /len(train_loader)\n",
    "        elif model.mode== 'supervised':\n",
    "            if model.conv=='GCN':\n",
    "                out = model.inference(data.to(device),dp=dropout)\n",
    "                loss = model.loss_sup(out[self.train_mask],y[self.train_mask])\n",
    "                total_loss+=loss\n",
    "            else:\n",
    "                for batch_size, n_id, adjs in train_loader:\n",
    "                    adjs = [adj for adj in adjs]\n",
    "                    out = model.forward(data.x[n_id].to(device), adjs)\n",
    "                    loss = model.loss_sup(out,y[n_id[:batch_size]])\n",
    "                    total_loss+=loss\n",
    "            total_loss.backward(retain_graph=True)\n",
    "            optimizer.step()      \n",
    "            return total_loss /len(train_loader)       \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test(self, model,data,classifier,**kwargs):#,n_estimators,learning_rate_carboost, max_depth): \n",
    "        model.eval()\n",
    "        out = model.inference(data.to(device))\n",
    "        y_true = self.y.detach().numpy()\n",
    "        if model.mode == 'supervised':\n",
    "            y_true = self.y.unsqueeze(-1)\n",
    "            y_pred = out.cpu().argmax(dim=-1, keepdim=True)\n",
    "            accs = []\n",
    "            for mask in [self.train_mask, self.val_mask, self.test_mask]:    \n",
    "                accs+=[int(y_pred[mask].eq(y_true[mask]).sum()) / int(mask.sum())]\n",
    "            return accs\n",
    "        elif model.mode == 'unsupervised': \n",
    "            if classifier == 'logistic regression':\n",
    "                clf = LogisticRegressionCV(cv = 5, max_iter = 3000).fit(out.cpu().detach()[self.train_mask].numpy(), self.y.detach()[self.train_mask].numpy())\n",
    "            else:\n",
    "                n_estimators= kwargs[\"n_estimators\"]\n",
    "                learning_rate_catboost = kwargs[\"learning_rate_catboost\"]\n",
    "                max_depth = kwargs[\"max_depth\"]\n",
    "                clf = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate = learning_rate_catboost, max_depth=max_depth, random_state=0)\n",
    "                clf.fit(out.cpu().detach()[self.train_mask].numpy(), self.y.detach()[self.train_mask].numpy())\n",
    "            accs = []\n",
    "            for mask in [self.train_mask,self.test_mask]:\n",
    "                accs + = [f1_score(out.cpu().detach()[mask].numpy(), self.y.detach()[mask].cpu().numpy(), average='macro')]#, precision_score(y_true[val_mask.cpu()], best_preds_val, average='macro'),precision_score(y_true[test_mask.cpu()], best_preds, average='macro')]\n",
    "            \n",
    "            return accs\n",
    "        @torch.no_grad()\n",
    "    def test(self, model,data,classifier,**kwargs):#,n_estimators,learning_rate_carboost, max_depth): \n",
    "        model.eval()\n",
    "        out = model.inference(data.to(device))\n",
    "        y_true = self.y.detach().numpy()\n",
    "        if model.mode == 'supervised':\n",
    "            y_true = self.y.unsqueeze(-1)\n",
    "            y_pred = out.cpu().argmax(dim=-1, keepdim=True)\n",
    "            accs = []\n",
    "            for mask in [self.train_mask, self.val_mask, self.test_mask]:    \n",
    "                accs+=[int(y_pred[mask].eq(y_true[mask]).sum()) / int(mask.sum())]\n",
    "            return accs\n",
    "        elif model.mode == 'unsupervised': \n",
    "            if classifier == 'logistic regression':\n",
    "                clf = LogisticRegressionCV(cv = 5, max_iter = 3000).fit(out.cpu().detach()[self.train_mask].numpy(), self.y.detach()[self.train_mask].numpy())\n",
    "            else:\n",
    "                n_estimators= kwargs[\"n_estimators\"]\n",
    "                learning_rate_catboost = kwargs[\"learning_rate_catboost\"]\n",
    "                max_depth = kwargs[\"max_depth\"]\n",
    "                clf = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate = learning_rate_catboost, max_depth=max_depth, random_state=0)\n",
    "                clf.fit(out.cpu().detach()[self.train_mask].numpy(), self.y.detach()[self.train_mask].numpy())\n",
    "            accs = []\n",
    "            for mask in [self.train_mask,self.test_mask]:\n",
    "                accs + = [f1_score(out.cpu().detach()[mask].numpy(), self.y.detach()[mask].cpu().numpy(), average='macro')]#, precision_score(y_true[val_mask.cpu()], best_preds_val, average='macro'),precision_score(y_true[test_mask.cpu()], best_preds, average='macro')]\n",
    "            \n",
    "            return accs\n",
    "\n",
    "    def run(self,**kwargs):\n",
    "        \n",
    "        hidden_layer = 64\n",
    "        out_layer = 128\n",
    "        dropout = 0.4\n",
    "        size = 2\n",
    "        learning_rate = 0.001\n",
    "\n",
    "        classifier = \"logistic regression\"\n",
    "        train_loader = NeighborSampler(self.data.edge_index, node_idx=self.train_mask, batch_size = int(sum(self.train_mask)), sizes=[-1]*size)\n",
    "        LossSampler = Sampler(self.data,device=device,mask=self.train_mask,loss_info=self.loss)\n",
    "        model = Net(dataset = self.dataset,mode=self.mode,conv=self.Conv,loss_function=self.loss,device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = (size),dropout = dropout)\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)\n",
    "                #scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.01, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        scheduler=lr_scheduler.StepLR(optimizer, step_size=25,gamma=0.1)\n",
    "        losses=[]\n",
    "        train_accs=[]\n",
    "        test_accs=[]\n",
    "        val_accs=[]\n",
    "        name_of_plot='conv: '+model.conv+', mode: '+model.mode+', loss from '+self.loss[\"Name\"]\n",
    "\n",
    "        print(name_of_plot)\n",
    "\n",
    "        for epoch in range(50):\n",
    "                    print('epoch',epoch)\n",
    "                    loss = self.train(model,self.data,optimizer,LossSampler,train_loader,dropout,epoch)\n",
    "                    losses.append(loss)\n",
    "                    d_test = datetime.now()\n",
    "                    train_acc, test_acc = self.test(model,self.data,'logistic regression')\n",
    "                   # print('тестирование заняло: ', datetime.now()-d_test)\n",
    "                    train_accs.append(train_acc)\n",
    "                    test_accs.append(test_acc)\n",
    "                   # val_accs.append(val_acc)\n",
    "                    log = 'Loss: {:.4f}, Epoch: {:03d}, Train: {:.4f}, Test: {:.4f}'\n",
    "                    #scheduler.step()\n",
    "                    print(log.format(loss, epoch, train_acc, test_acc))\n",
    "        print('Test acc on the last epoch ', test_acc)\n",
    "        plt.plot(losses)\n",
    "        plt.title(name_of_plot+' loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "        plt.plot(test_accs)\n",
    "        plt.title(name_of_plot+' loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "                    #return [precision_score(y_true[train_mask.cpu()], best_preds_train, average='macro'), precision_score(y_true[val_mask.cpu()], best_preds_val, average='macro'),precision_score(y_true[test_mask.cpu()], best_preds, average='macro')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 8, 8])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr =np.array([4,2,6,8,6,8,9,1,2,3,5])\n",
    "arr_sorted = -np.sort(-arr)\n",
    "top_three = arr_sorted[:3]\n",
    "top_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainOptuna(Main):\n",
    "    def objective(self,trial):\n",
    "        # Integer parameter\n",
    "        hidden_layer = trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "        out_layer = trial.suggest_categorical(\"out_layer\", [32,64,128])\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "        size = trial.suggest_categorical(\"size of network, number of convs\", [1,2,3])\n",
    "        Conv = self.Conv# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "\n",
    "        # варьируем параметры\n",
    "        loss_to_train={}\n",
    "        for name in self.loss:\n",
    "            \n",
    "            if type(self.loss[name]) == list :\n",
    "                if len(self.loss[name]) == 3:\n",
    "                    var = trial.suggest_int(name,self.loss[name][0],self.loss[name][1],step=self.loss[name][2])\n",
    "                    loss_to_train[name] = var\n",
    "                elif len(self.loss[name]) == 2:\n",
    "                    var_2 = trial.suggest_float(name,self.loss[name][0],self.loss[name][1])\n",
    "                    loss_to_train[name] = var_2\n",
    "                else:\n",
    "                    var_3 = trial.suggest_categorical(name, self.loss[name])\n",
    "                    loss_to_train[name] = var_3\n",
    "            else:\n",
    "                loss_to_train[name] = self.loss[name]\n",
    "        Sampler =loss_to_train[\"Sampler\"]\n",
    "        model = Net(dataset = self.dataset,mode='unsupervised',conv=Conv,loss_function=loss_to_train,device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = size,dropout = dropout)\n",
    "\n",
    "        train_loader = NeighborSampler(self.data.edge_index, batch_size =int(sum(self.train_mask)),node_idx=self.train_mask, sizes=[-1]*size)\n",
    "        \n",
    "        #train_loader = GraphSAINTRandomWalkSampler(data, batch_size=2176, walk_length=2,num_steps=5, sample_coverage=100,save_dir=dataset.processed_dir,num_workers=4)\n",
    "\n",
    "        LossSampler = Sampler(self.data,device=self.device,mask=self.train_mask,loss_info=loss_to_train)\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        learning_rate= trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "\n",
    "        classifier = \"logistic regression\" #trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "\n",
    "        if classifier == \"catboost\":\n",
    "            n_estimators = trial.suggest_int(\"n of estimators\", 10,40,5)\n",
    "            learning_rate_catboost = trial.suggest_float(\"lr_catboost\",5e-4,1e-2)\n",
    "            max_depth = trial.suggest_int(\"max_depth\",1,10,2)\n",
    "        else:\n",
    "            n_estimators = -1\n",
    "            learning_rate_catboost =-1\n",
    "            max_depth = -1\n",
    "        #training of the model\n",
    "        for epoch in range(50):\n",
    "            loss = self.train(model,self.data,optimizer,LossSampler,train_loader,dropout,epoch)\n",
    "            #print(loss)\n",
    "\n",
    "        train_acc, test_acc = self.test(model,self.data,classifier,n_estimators=n_estimators,learning_rate_catboost=learning_rate_catboost,max_depth=max_depth)\n",
    "\n",
    "\n",
    "        trial.report(test_acc,epoch)\n",
    "\n",
    "        return test_acc\n",
    "\n",
    "    \n",
    "    def run(self,number_of_trials):\n",
    "\n",
    "        study = optuna.create_study(direction=\"maximize\",study_name=self.loss[\"Name\"]+\" loss,\"+str(self.Conv)+\" conv\")\n",
    "        study.optimize(self.objective,n_trials = number_of_trials)\n",
    "\n",
    "        print('Best trial:')\n",
    "        trial = study.best_trial\n",
    "        print(\" Value: \", trial.value)\n",
    "        print(\" Params: \")\n",
    "        for key, value in trial.params.items():\n",
    "            print(\" {}: {}\".format(key,value))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    " #SAGE = {\"Name\":\"SAGE\" , \"walk_length\":[5,20,5],\"walks_per_node\":[5,20,5],\"num_negative_samples\":20,\"context size\" : [5,20,5],\"p\":1,\"q\":1, \"loss var\": \"Random Walks\",\"flag\":False,\"Sampler\" =SamplerRandomWalk }\n",
    "from modules.sampling import Sampler, SamplerContextMatrix, SamplerRandomWalk,SamplerFactorization,SamplerAPP\n",
    "DeepWalk = {\"Name\": \"DeepWalk\",\"walk_length\":[5,20,5],\"walks_per_node\":[5,20,5],\"num_negative_samples\":[1,21,5],\"context_size\" : [5,20,5],\"p\":1,\"q\":1,\"loss var\": \"Random Walks\",\"flag\":True,\"Sampler\" : SamplerRandomWalk } #Проблемы с памятью после того, как увеличила количество тренировочных данных\n",
    "Node2Vec = {\"Name\": \"Node2Vec\",\"walk_length\":[5,20,5],\"walks_per_node\":[5,20,5],\"num_negative_samples\":[1,21,5],\"context_size\" : [5,20,5],\"p\": [0.0,0.9] ,\"q\":[0.0,0.9], \"loss var\": \"Random Walks\",\"flag\":True,\"Sampler\": SamplerRandomWalk}#то же самое \n",
    "\n",
    "LINE = {\"Name\": \"LINE\",\"C\": \"Adj\",\"num_negative_samples\":[1,21,5],\"loss var\": \"Context Matrix\",\"flag\":True,\"Sampler\" :SamplerContextMatrix} \n",
    "HOPE_RPR = {\"Name\": \"HOPE_RPR\",\"C\":\"RPR\",\"loss var\": \"Factorization\",\"flag\":True,\"alpha\": [0,1],\"Sampler\" :SamplerFactorization} #проверить\n",
    "HOPE_Katz = {\"Name\": \"HOPE_Katz\",\"C\":\"Katz\",\"loss var\": \"Factorization\",\"flag\":True,\"betta\": [0,1],\"Sampler\" :SamplerFactorization,} #проверить\n",
    "HOPE_CN = {\"Name\": \"HOPE_CommonNeighbors\",\"C\":\"CN\",\"loss var\": \"Factorization\",\"flag\":True,\"Sampler\" :SamplerFactorization} \n",
    "HOPE_AA = {\"Name\": \"HOPE_AdamicAdar\",\"C\":\"AA\",\"loss var\": \"Factorization\",\"flag\":True,\"Sampler\" :SamplerFactorization} \n",
    "\n",
    "LapEigen = {\"Name\": \"LaplacianEigenMaps\", \"C\":\"Adj\",\"loss var\": \"Laplacian EigenMaps\",\"flag\":True,\"Sampler\" :SamplerFactorization}\n",
    "GraphFactorization = {\"Name\": \"Graph Factorization\",\"C\":\"Adj\",\"loss var\": \"Factorization\",\"flag\":True,\"Sampler\" :SamplerFactorization} \n",
    "VERSE_PPR =  {\"Name\": \"VERSE_PPR\",\"C\": \"PPR\",\"num_negative_samples\":[1,21,5],\"loss var\": \"Context Matrix\",\"flag\":True,\"alpha\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\"Sampler\" :SamplerContextMatrix}\n",
    "\n",
    "VERSE_SR =  {\"Name\": \"VERSE_SimRank\",\"C\": \"SR\",\"num_negative_samples\":[1,21,5],\"loss var\": \"Context Matrix\",\"flag\":True,\"Sampler\":SamplerContextMatrix} \n",
    "VERSE_Adj =  {\"Name\": \"VERSE_Adj\",\"C\": \"Adj\",\"num_negative_samples\":[1,21,5],\"loss var\": \"Context Matrix\",\"flag\":True,\"Sampler\" :SamplerContextMatrix} \n",
    "\n",
    "APP ={\"Name\": \"APP\",\"C\": \"PPR\",\"num_negative_samples\":[1,21,5],\"loss var\": \"Context Matrix\",\"flag\":True,\"alpha\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\"Sampler\" :SamplerAPP}\n",
    "\n",
    "Struc2Vec ={} #Implement\n",
    "Energy = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-23 13:10:28,948]\u001b[0m A new study created in memory with name: APP loss,GCN conv\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 13:11:03,135]\u001b[0m Trial 0 finished with value: 0.7855822550831792 and parameters: {'hidden_layer': 128, 'out_layer': 32, 'dropout': 0.5, 'size of network, number of convs': 2, 'num_negative_samples': 16, 'alpha': 0.6, 'lr': 0.007243741241773637}. Best is trial 0 with value: 0.7855822550831792.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 13:11:28,497]\u001b[0m Trial 1 finished with value: 0.8059149722735675 and parameters: {'hidden_layer': 256, 'out_layer': 64, 'dropout': 0.4, 'size of network, number of convs': 1, 'num_negative_samples': 16, 'alpha': 0.7, 'lr': 0.007496539781198254}. Best is trial 1 with value: 0.8059149722735675.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 13:12:24,186]\u001b[0m Trial 2 finished with value: 0.7504621072088724 and parameters: {'hidden_layer': 64, 'out_layer': 64, 'dropout': 0.4, 'size of network, number of convs': 1, 'num_negative_samples': 1, 'alpha': 0.1, 'lr': 0.0024596532665980813}. Best is trial 1 with value: 0.8059149722735675.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 13:13:03,636]\u001b[0m Trial 3 finished with value: 0.6635859519408502 and parameters: {'hidden_layer': 256, 'out_layer': 32, 'dropout': 0.30000000000000004, 'size of network, number of convs': 2, 'num_negative_samples': 16, 'alpha': 0.3, 'lr': 0.0007809086369205455}. Best is trial 1 with value: 0.8059149722735675.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 13:13:43,947]\u001b[0m Trial 4 finished with value: 0.6321626617375231 and parameters: {'hidden_layer': 32, 'out_layer': 32, 'dropout': 0.5, 'size of network, number of convs': 2, 'num_negative_samples': 6, 'alpha': 0.4, 'lr': 0.007529972828208366}. Best is trial 1 with value: 0.8059149722735675.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 13:14:20,199]\u001b[0m Trial 5 finished with value: 0.7929759704251387 and parameters: {'hidden_layer': 256, 'out_layer': 64, 'dropout': 0.2, 'size of network, number of convs': 3, 'num_negative_samples': 6, 'alpha': 0.5, 'lr': 0.0009146297267866116}. Best is trial 1 with value: 0.8059149722735675.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 13:14:54,924]\u001b[0m Trial 6 finished with value: 0.6913123844731978 and parameters: {'hidden_layer': 32, 'out_layer': 128, 'dropout': 0.1, 'size of network, number of convs': 2, 'num_negative_samples': 1, 'alpha': 0.7, 'lr': 0.009870277617731972}. Best is trial 1 with value: 0.8059149722735675.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 13:15:24,398]\u001b[0m Trial 7 finished with value: 0.5859519408502772 and parameters: {'hidden_layer': 256, 'out_layer': 32, 'dropout': 0.5, 'size of network, number of convs': 1, 'num_negative_samples': 21, 'alpha': 0.4, 'lr': 0.0013925928114120669}. Best is trial 1 with value: 0.8059149722735675.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 13:15:53,324]\u001b[0m Trial 8 finished with value: 0.7929759704251387 and parameters: {'hidden_layer': 32, 'out_layer': 128, 'dropout': 0.1, 'size of network, number of convs': 1, 'num_negative_samples': 11, 'alpha': 0.8, 'lr': 0.0017457701318286994}. Best is trial 1 with value: 0.8059149722735675.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 13:16:22,405]\u001b[0m Trial 9 finished with value: 0.7042513863216266 and parameters: {'hidden_layer': 64, 'out_layer': 64, 'dropout': 0.0, 'size of network, number of convs': 3, 'num_negative_samples': 6, 'alpha': 0.8, 'lr': 0.002565934318069099}. Best is trial 1 with value: 0.8059149722735675.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      " Value:  0.8059149722735675\n",
      " Params: \n",
      " hidden_layer: 256\n",
      " out_layer: 64\n",
      " dropout: 0.4\n",
      " size of network, number of convs: 1\n",
      " num_negative_samples: 16\n",
      " alpha: 0.7\n",
      " lr: 0.007496539781198254\n"
     ]
    }
   ],
   "source": [
    "device =  torch.device('cuda',0)# if torch.cuda.is_available() else 'cpu')\n",
    "loss = APP\n",
    "MO = MainOptuna('GCN', device, loss , mode = 'unsupervised')\n",
    "MO.run(number_of_trials=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-23 13:16:22,425]\u001b[0m A new study created in memory with name: APP loss,SAGE conv\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 13:16:59,323]\u001b[0m Trial 0 finished with value: 0.8077634011090573 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.5, 'size of network, number of convs': 1, 'num_negative_samples': 6, 'alpha': 0.6, 'lr': 0.004945026627110915}. Best is trial 0 with value: 0.8077634011090573.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 13:17:41,552]\u001b[0m Trial 1 finished with value: 0.6524953789279113 and parameters: {'hidden_layer': 256, 'out_layer': 32, 'dropout': 0.4, 'size of network, number of convs': 2, 'num_negative_samples': 16, 'alpha': 0.4, 'lr': 0.009714838227828357}. Best is trial 0 with value: 0.8077634011090573.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 13:18:11,525]\u001b[0m Trial 2 finished with value: 0.7615526802218114 and parameters: {'hidden_layer': 32, 'out_layer': 128, 'dropout': 0.4, 'size of network, number of convs': 2, 'num_negative_samples': 21, 'alpha': 0.8, 'lr': 0.003994580350444216}. Best is trial 0 with value: 0.8077634011090573.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 13:18:40,078]\u001b[0m Trial 3 finished with value: 0.800369685767098 and parameters: {'hidden_layer': 128, 'out_layer': 128, 'dropout': 0.2, 'size of network, number of convs': 1, 'num_negative_samples': 11, 'alpha': 0.9, 'lr': 0.00478363672853256}. Best is trial 0 with value: 0.8077634011090573.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 13:18:59,437]\u001b[0m Trial 4 finished with value: 0.6802218114602587 and parameters: {'hidden_layer': 64, 'out_layer': 32, 'dropout': 0.2, 'size of network, number of convs': 1, 'num_negative_samples': 11, 'alpha': 0.8, 'lr': 0.007479163227399848}. Best is trial 0 with value: 0.8077634011090573.\u001b[0m\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "\u001b[32m[I 2021-04-23 13:19:58,790]\u001b[0m Trial 5 finished with value: 0.6155268022181146 and parameters: {'hidden_layer': 128, 'out_layer': 32, 'dropout': 0.0, 'size of network, number of convs': 3, 'num_negative_samples': 6, 'alpha': 0.2, 'lr': 0.0036220852127857974}. Best is trial 0 with value: 0.8077634011090573.\u001b[0m\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "\u001b[32m[I 2021-04-23 13:21:11,431]\u001b[0m Trial 6 finished with value: 0.7763401109057301 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.0, 'size of network, number of convs': 2, 'num_negative_samples': 1, 'alpha': 0.1, 'lr': 0.0051475351020595864}. Best is trial 0 with value: 0.8077634011090573.\u001b[0m\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-23 13:21:49,575]\u001b[0m Trial 7 finished with value: 0.6839186691312384 and parameters: {'hidden_layer': 32, 'out_layer': 64, 'dropout': 0.1, 'size of network, number of convs': 2, 'num_negative_samples': 11, 'alpha': 0.8, 'lr': 0.0037430904316092042}. Best is trial 0 with value: 0.8077634011090573.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 13:22:22,081]\u001b[0m Trial 8 finished with value: 0.6487985212569316 and parameters: {'hidden_layer': 64, 'out_layer': 32, 'dropout': 0.30000000000000004, 'size of network, number of convs': 2, 'num_negative_samples': 21, 'alpha': 0.7, 'lr': 0.004930307164841467}. Best is trial 0 with value: 0.8077634011090573.\u001b[0m\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "\u001b[32m[I 2021-04-23 13:23:02,937]\u001b[0m Trial 9 finished with value: 0.6950092421441775 and parameters: {'hidden_layer': 64, 'out_layer': 64, 'dropout': 0.1, 'size of network, number of convs': 3, 'num_negative_samples': 6, 'alpha': 0.6, 'lr': 0.0007649876865025744}. Best is trial 0 with value: 0.8077634011090573.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      " Value:  0.8077634011090573\n",
      " Params: \n",
      " hidden_layer: 256\n",
      " out_layer: 128\n",
      " dropout: 0.5\n",
      " size of network, number of convs: 1\n",
      " num_negative_samples: 6\n",
      " alpha: 0.6\n",
      " lr: 0.004945026627110915\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "MO = MainOptuna('SAGE', device, loss, mode = 'unsupervised',number_of_trials=20)\n",
    "MO.run(number_of_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-23 13:23:02,957]\u001b[0m A new study created in memory with name: APP loss,GAT conv\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 13:24:04,047]\u001b[0m Trial 0 finished with value: 0.7744916820702403 and parameters: {'hidden_layer': 128, 'out_layer': 32, 'dropout': 0.2, 'size of network, number of convs': 3, 'num_negative_samples': 21, 'alpha': 0.1, 'lr': 0.002698142346164739}. Best is trial 0 with value: 0.7744916820702403.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 13:24:28,949]\u001b[0m Trial 1 finished with value: 0.6062846580406654 and parameters: {'hidden_layer': 32, 'out_layer': 32, 'dropout': 0.2, 'size of network, number of convs': 3, 'num_negative_samples': 1, 'alpha': 0.6, 'lr': 0.0026745598108492133}. Best is trial 0 with value: 0.7744916820702403.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 13:24:59,467]\u001b[0m Trial 2 finished with value: 0.6524953789279113 and parameters: {'hidden_layer': 32, 'out_layer': 32, 'dropout': 0.4, 'size of network, number of convs': 3, 'num_negative_samples': 11, 'alpha': 0.7, 'lr': 0.009908519865637382}. Best is trial 0 with value: 0.7744916820702403.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 13:25:19,231]\u001b[0m Trial 3 finished with value: 0.8502772643253235 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.1, 'size of network, number of convs': 3, 'num_negative_samples': 21, 'alpha': 0.9, 'lr': 0.0011565208277352422}. Best is trial 3 with value: 0.8502772643253235.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 13:26:10,651]\u001b[0m Trial 4 finished with value: 0.8428835489833642 and parameters: {'hidden_layer': 64, 'out_layer': 128, 'dropout': 0.0, 'size of network, number of convs': 1, 'num_negative_samples': 16, 'alpha': 0.2, 'lr': 0.004686838052705382}. Best is trial 3 with value: 0.8502772643253235.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 13:26:44,478]\u001b[0m Trial 5 finished with value: 0.8428835489833642 and parameters: {'hidden_layer': 256, 'out_layer': 64, 'dropout': 0.0, 'size of network, number of convs': 2, 'num_negative_samples': 6, 'alpha': 0.7, 'lr': 0.005946036138626919}. Best is trial 3 with value: 0.8502772643253235.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 13:27:18,898]\u001b[0m Trial 6 finished with value: 0.8595194085027726 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.30000000000000004, 'size of network, number of convs': 1, 'num_negative_samples': 1, 'alpha': 0.7, 'lr': 0.009816383252834324}. Best is trial 6 with value: 0.8595194085027726.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 13:27:33,335]\u001b[0m Trial 7 finished with value: 0.5637707948243993 and parameters: {'hidden_layer': 256, 'out_layer': 32, 'dropout': 0.30000000000000004, 'size of network, number of convs': 1, 'num_negative_samples': 16, 'alpha': 0.7, 'lr': 0.0016262207309523846}. Best is trial 6 with value: 0.8595194085027726.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 13:28:15,709]\u001b[0m Trial 8 finished with value: 0.7393715341959335 and parameters: {'hidden_layer': 64, 'out_layer': 128, 'dropout': 0.5, 'size of network, number of convs': 3, 'num_negative_samples': 21, 'alpha': 0.5, 'lr': 0.005804345907497389}. Best is trial 6 with value: 0.8595194085027726.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 13:28:32,829]\u001b[0m Trial 9 finished with value: 0.6266173752310537 and parameters: {'hidden_layer': 32, 'out_layer': 64, 'dropout': 0.1, 'size of network, number of convs': 3, 'num_negative_samples': 21, 'alpha': 0.9, 'lr': 0.003843712821842675}. Best is trial 6 with value: 0.8595194085027726.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      " Value:  0.8595194085027726\n",
      " Params: \n",
      " hidden_layer: 256\n",
      " out_layer: 128\n",
      " dropout: 0.30000000000000004\n",
      " size of network, number of convs: 1\n",
      " num_negative_samples: 1\n",
      " alpha: 0.7\n",
      " lr: 0.009816383252834324\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "MO = MainOptuna('GAT', device, loss, mode = 'unsupervised')\n",
    "MO.run(number_of_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "My__RW_Neighbour.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
