{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-gN3S4NhAjs",
    "outputId": "fef06879-cb42-4f09-db2d-88671e8c4315"
   },
   "outputs": [],
   "source": [
    "#!pip install -q torch==1.7.1+cpu torchvision==0.8.2+cpu torchaudio==0.7.2 torchtext==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -q --no-index torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html\n",
    "#!pip install -q --no-index torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html\n",
    "#!pip install -q --no-index torch-cluster -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html\n",
    "#!pip install -q --no-index torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html\n",
    "#!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# node classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from torch_geometric.data import NeighborSampler\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import numpy as np\n",
    "from modules.model import Net\n",
    "from modules.sampling import Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x = data.x.to(device)\n",
    "y = data.y.squeeze().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = torch.tensor([True]*int(0.8*len(data.x)+1) + [False]*int(0.2*len(data.x)))\n",
    "#val_mask = torch.tensor([False]*int(0.6*len(data.x)+1) + [True]*int(0.2*len(data.x)+1)+[False]*int(0.2*len(data.x)))\n",
    "test_mask = torch.tensor([False]*int(0.8*len(data.x)+1) + [True]*int(0.2*len(data.x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_mask = data.train_mask\n",
    "#val_mask = data.val_mask\n",
    "#test_mask = data.test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "def train(model,data,optimizer,Sampler,train_loader,dropout):\n",
    "    model.train()        \n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    if model.mode == 'unsupervised':\n",
    "        if model.conv=='GCN':\n",
    "            arr=torch.nonzero(train_mask == True)\n",
    "            indices_of_train_data = ([item for sublist in arr for item in sublist])\n",
    "            out = model.inference(data.to(device),dp=dropout)\n",
    "            samples = Sampler.sample(indices_of_train_data)\n",
    "            loss = model.loss(out[train_mask], samples)\n",
    "            total_loss+=loss\n",
    "        else:\n",
    "            for batch_size, n_id, adjs in train_loader:\n",
    "                # adjs holds a list of (edge_index, e_id, size) tuples.\n",
    "                adjs = [adj for adj in adjs]\n",
    "                out = model.forward(data.x[n_id].to(device), adjs)\n",
    "                samples = Sampler.sample((n_id.numpy().tolist())[:batch_size])\n",
    "                loss = model.loss(out, samples)#pos_batch.to(device), neg_batch.to(device))\n",
    "                #print(out.shape, samples[0].shape,samples[1].shape)\n",
    "                total_loss+=loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()      \n",
    "        return total_loss /len(train_loader)\n",
    "    elif model.mode== 'supervised':\n",
    "        if model.conv=='GCN':\n",
    "            out = model.inference(data.to(device),dp=dropout)\n",
    "            loss = model.loss_sup(out[train_mask],y[train_mask])\n",
    "            total_loss+=loss\n",
    "        else:\n",
    "            for batch_size, n_id, adjs in train_loader:\n",
    "                adjs = [adj for adj in adjs]\n",
    "                out = model.forward(data.x[n_id].to(device), adjs)\n",
    "                loss = model.loss_sup(out,y[n_id[:batch_size]])\n",
    "                total_loss+=loss\n",
    "        total_loss.backward(retain_graph=True)\n",
    "        optimizer.step()      \n",
    "        return total_loss /len(train_loader)       \n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model,data,classifier): \n",
    "    model.eval()\n",
    "    out = model.inference(data.to(device))\n",
    "    y_true = y.cpu().detach().numpy()\n",
    "    if model.mode == 'supervised':\n",
    "        y_true = y.cpu().unsqueeze(-1)\n",
    "        y_pred = out.cpu().argmax(dim=-1, keepdim=True)\n",
    "        accs = []\n",
    "        for mask in [train_mask, val_mask, test_mask]:    \n",
    "            accs+=[int(y_pred[mask].eq(y_true[mask]).sum()) / int(mask.sum())]\n",
    "        return accs\n",
    "    elif model.mode == 'unsupervised': \n",
    "        if classifier == 'logistic regression':\n",
    "            clf = LogisticRegressionCV(cv = 5, max_iter = 3000).fit(out.cpu().detach()[train_mask].numpy(), y.cpu().detach()[train_mask].numpy())\n",
    "        else:\n",
    "            clf = GradientBoostingClassifier(n_estimators=20, learning_rate=0.1, max_depth=3, random_state=0).fit(out.cpu().detach()[train_mask].numpy(), y.cpu().detach()[train_mask].numpy())\n",
    "        #clf = MLPClassifier(random_state=1, max_iter=3000).fit(out.cpu().detach()[train_mask].numpy(), y.cpu().detach()[train_mask].numpy())\n",
    "        accs = []\n",
    "        for mask in [train_mask, test_mask]:    \n",
    "            accs += [(clf.score(out.cpu().detach()[mask].numpy(), y.cpu().detach()[mask].cpu().numpy()))]\n",
    "        return accs\n",
    "        \n",
    "        #return [precision_score(y_true[train_mask.cpu()], best_preds_train, average='macro'), precision_score(y_true[val_mask.cpu()], best_preds_val, average='macro'),precision_score(y_true[test_mask.cpu()], best_preds, average='macro')]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Для начала посмотрим, что получится, если передать только фичи. Без обучения**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Теперь сравним с результатом после обучения эмбедингов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAGE = {\"Name\":\"SAGE\" , \"walk length\":5,\"walks per node\":50,\"num negative samples\":20,\"context size\" : 10,\"p\":1,\"q\":1, \"loss var\": \"Random Walks\"}\n",
    "DeepWalk = {\"Name\": \"DeepWalk\",\"walk length\":10,\"walks per node\":10,\"num negative samples\":10,\"context size\" : 10,\"p\":1,\"q\":1,\"loss var\": \"Random Walks\" } #Проблемы с памятью после того, как увеличила количество тренировочных данных\n",
    "#DeepWalk = {\"Name\": \"DeepWalk\",\"walk length\":40,\"walks per node\":80,\"num negative samples\":20,\"context size\" : 10,\"p\":1,\"q\":1,\"loss var\": \"Random Walks\" } #Проблемы с памятью после того, как увеличила количество тренировочных данных\n",
    "Node2Vec = {\"Name\": \"Node2Vec\",\"walk length\":20,\"walks per node\":10,\"num negative samples\":1,\"context size\" : 10,\"p\":1.414 ,\"q\":1.414, \"loss var\": \"Random Walks\"}#то же самое \n",
    "#Node2Vec = {\"Name\": \"Node2Vec\",\"walk length\":100,\"walks per node\":18,\"num negative samples\":20,\"context size\" : 16,\"p\":1.414 ,\"q\":1.414, \"loss var\": \"Random Walks\"}#то же самое \n",
    "\n",
    "LINE2 = {\"Name\": \"LINE2\",\"C\": \"Adj\",\"num negative samples\":10,\"loss var\": \"Context Matrix\"} #настроить параметры в принципе при 0.003 что то выходит  \n",
    "HOPE_RPR = {\"Name\": \"HOPE: RPR\",\"C\":\"RPR\",\"loss var\": \"Factorization\"}#оч большой loss\n",
    "\n",
    "\n",
    "GraphFactorization = {\"Name\": \"Graph Factorization\",\"C\":\"Adj\",\"loss var\": \"Factorization\"} #jоч большой loss\n",
    "HOPE_Katz = {\"Name\": \"HOPE: Katz\",\"C\":\"Katz\",\"loss var\": \"Factorization\"}#Проблемы оч большой loss\n",
    "VERSE_PPR =  {\"Name\": \"VERSE\",\"C\": \"PPR\",\"num negative samples\":10,\"loss var\": \"Context Matrix\"}  #проблеиы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAGE [5]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-11 14:00:08,566]\u001b[0m A new study created in memory with name: SAGE loss\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7215617f91ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[0mstudy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"maximize\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstudy_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"SAGE loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_trials\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[0mpruned_trials\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0moptuna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPRUNED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[0mcomplete_trials\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0moptuna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOMPLETE\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\anpolol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\optuna\\study.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m             \u001b[0mshow_progress_bar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m         )\n\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\anpolol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     74\u001b[0m                 \u001b[0mreseed_sampler_rng\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[0mtime_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m                 \u001b[0mprogress_bar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m             )\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\anpolol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mtrial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\anpolol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m         \u001b[0mvalue_or_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-7215617f91ca>\u001b[0m in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m#training of the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLossSampler\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-50d416b69fa0>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, data, optimizer, Sampler, train_loader, dropout)\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[1;31m# adjs holds a list of (edge_index, e_id, size) tuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0madjs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0madj\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0madj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0madjs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m                 \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madjs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m                 \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSampler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_id\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#pos_batch.to(device), neg_batch.to(device))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MasterDegree\\Code\\modules\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, adjs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madjs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[0mx_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Target nodes are always placed first.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_layers\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\anpolol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\anpolol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch_geometric\\nn\\conv\\sage_conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, edge_index, size)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;31m# propagate_type: (x: OptPairTensor)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpropagate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlin_l\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\anpolol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py\u001b[0m in \u001b[0;36mpropagate\u001b[1;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfuse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m             coll_dict = self.__collect__(self.__user_args__, edge_index, size,\n\u001b[1;32m--> 234\u001b[1;33m                                          kwargs)\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[0mmsg_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minspector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'message'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoll_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\anpolol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py\u001b[0m in \u001b[0;36m__collect__\u001b[1;34m(self, args, edge_index, size, kwargs)\u001b[0m\n\u001b[0;32m    156\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__set_size__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m                     data = self.__lift__(data, edge_index,\n\u001b[1;32m--> 158\u001b[1;33m                                          j if arg[-2:] == '_j' else i)\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m                 \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\anpolol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py\u001b[0m in \u001b[0;36m__lift__\u001b[1;34m(self, src, edge_index, dim)\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0medge_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "def objective(trial):\n",
    "    # Integer parameter\n",
    "    hidden_layer = trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "    out_layer = trial.suggest_categorical(\"out_layer\", [32,64,128])\n",
    "    # Floating point parameter\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "    size = [5]#list(map(lambda x: int(x),b))\n",
    "    Conv =\"SAGE\"# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "    \n",
    "    walk_length = trial.suggest_int(\"walk length\",5,20,step = 5)\n",
    "    walks_per_node = trial.suggest_int(\"walks per node\", 5,10,step = 5)\n",
    "    context_size = trial.suggest_int(\"context size\",5,20,step = 5)\n",
    "    q = trial.suggest_float(\"q\", 0.0,0.9,step = 0.1)\n",
    "    p = trial.suggest_float(\"p\", 0.0,0.9,step = 0.1)\n",
    "    num_negative_samples = walks_per_node\n",
    "    loss = {\"Name\": \"Node2Vec\",\"walk length\":walk_length,\"walks per node\":walks_per_node,\"num negative samples\":num_negative_samples,\"context size\" : context_size ,\"p\":p ,\"q\":q, \"loss var\": \"Random Walks\"}#то же самое \n",
    "\n",
    "    model = Net(dataset = dataset,mode='unsupervised',conv=Conv,loss_function=loss[\"loss var\"],device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = len(size),dropout = dropout)\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_mask, batch_size = 677, sizes=size)\n",
    "    LossSampler = Sampler(data,device=device,mask=train_mask,loss_info=loss)\n",
    "    \n",
    "    model.to(device)\n",
    "    #Generate the optimizers\n",
    "#optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "    learning_rate=trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "    #optimizer = getattr(optim,optimizer_name)(model.parameters(),lr = learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "    #classifier = trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "    classifier = \"logistic regression\"\n",
    "    #classifier = \"catboost\"\n",
    "    #training of the model\n",
    "    for epoch in range(50):\n",
    "        loss = train(model,data,optimizer,LossSampler,train_loader,dropout)\n",
    "        train_acc, test_acc = test(model,data,classifier)\n",
    "        \n",
    "    trial.report(test_acc,epoch)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialsPruned()\n",
    "    return test_acc\n",
    "        \n",
    "    \n",
    "study = optuna.create_study(direction=\"maximize\",study_name=\"SAGE loss\")\n",
    "study.optimize(objective,n_trials = 3)\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\" Number of finid=shed trials: \",len(study.trials))\n",
    "print(\" Number of pruned trials: \", len(pruned_trials))\n",
    "print(\" Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key,value))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "def objective(trial):\n",
    "    # Integer parameter\n",
    "    hidden_layer = trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "    out_layer = trial.suggest_categorical(\"out_layer\", [32,64,128])\n",
    "    # Floating point parameter\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "    size = [5]#list(map(lambda x: int(x),b))\n",
    "    Conv =\"SAGE\"# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "    \n",
    "    walk_length = trial.suggest_int(\"walk length\",5,20,step = 5)\n",
    "    walks_per_node = trial.suggest_int(\"walks per node\", 5,10,step = 5)\n",
    "    context_size = trial.suggest_int(\"context size\",5,20,step = 5)\n",
    "    q = trial.suggest_float(\"q\", 0.0,0.9,step = 0.1)\n",
    "    p = trial.suggest_float(\"p\", 0.0,0.9,step = 0.1)\n",
    "    num_negative_samples = walks_per_node\n",
    "    loss = {\"Name\": \"Node2Vec\",\"walk length\":walk_length,\"walks per node\":walks_per_node,\"num negative samples\":num_negative_samples,\"context size\" : context_size ,\"p\":p ,\"q\":q, \"loss var\": \"Random Walks\"}#то же самое \n",
    "\n",
    "    model = Net(dataset = dataset,mode='unsupervised',conv=Conv,loss_function=loss[\"loss var\"],device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = len(size),dropout = dropout)\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_mask, batch_size = 677, sizes=size)\n",
    "    LossSampler = Sampler(data,device=device,mask=train_mask,loss_info=loss)\n",
    "    \n",
    "    model.to(device)\n",
    "    #Generate the optimizers\n",
    "#optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "    learning_rate=trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "    #optimizer = getattr(optim,optimizer_name)(model.parameters(),lr = learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "    #classifier = trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "    #classifier = \"logistic regression\"\n",
    "    classifier = \"catboost\"\n",
    "    #training of the model\n",
    "    for epoch in range(50):\n",
    "        loss = train(model,data,optimizer,LossSampler,train_loader,dropout)\n",
    "        train_acc, test_acc = test(model,data,classifier)\n",
    "        \n",
    "    trial.report(test_acc,epoch)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialsPruned()\n",
    "    return test_acc\n",
    "        \n",
    "    \n",
    "study = optuna.create_study(direction=\"maximize\",study_name=\"SAGE loss\")\n",
    "study.optimize(objective,n_trials = 3)\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\" Number of finid=shed trials: \",len(study.trials))\n",
    "print(\" Number of pruned trials: \", len(pruned_trials))\n",
    "print(\" Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **[10]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "def objective(trial):\n",
    "    # Integer parameter\n",
    "    hidden_layer = trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "    out_layer = trial.suggest_categorical(\"out_layer\", [32,64,128])\n",
    "    # Floating point parameter\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "    size = [10]#list(map(lambda x: int(x),b))\n",
    "    Conv =\"SAGE\"# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "    \n",
    "    walk_length = trial.suggest_int(\"walk length\",5,20,step = 5)\n",
    "    walks_per_node = trial.suggest_int(\"walks per node\", 5,10,step = 5)\n",
    "    context_size = trial.suggest_int(\"context size\",5,20,step = 5)\n",
    "    q = trial.suggest_float(\"q\", 0.0,0.9,step = 0.1)\n",
    "    p = trial.suggest_float(\"p\", 0.0,0.9,step = 0.1)\n",
    "    num_negative_samples = walks_per_node\n",
    "    loss = {\"Name\": \"Node2Vec\",\"walk length\":walk_length,\"walks per node\":walks_per_node,\"num negative samples\":num_negative_samples,\"context size\" : context_size ,\"p\":p ,\"q\":q, \"loss var\": \"Random Walks\"}#то же самое \n",
    "\n",
    "    model = Net(dataset = dataset,mode='unsupervised',conv=Conv,loss_function=loss[\"loss var\"],device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = len(size),dropout = dropout)\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_mask, batch_size = 677, sizes=size)\n",
    "    LossSampler = Sampler(data,device=device,mask=train_mask,loss_info=loss)\n",
    "    \n",
    "    model.to(device)\n",
    "    #Generate the optimizers\n",
    "#optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "    learning_rate=trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "    #optimizer = getattr(optim,optimizer_name)(model.parameters(),lr = learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "    #classifier = trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "    classifier = \"logistic regression\"\n",
    "    #classifier = \"catboost\"\n",
    "    #training of the model\n",
    "    for epoch in range(50):\n",
    "        loss = train(model,data,optimizer,LossSampler,train_loader,dropout)\n",
    "        train_acc, test_acc = test(model,data,classifier)\n",
    "        \n",
    "    trial.report(test_acc,epoch)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialsPruned()\n",
    "    return test_acc\n",
    "        \n",
    "    \n",
    "study = optuna.create_study(direction=\"maximize\",study_name=\"SAGE loss\")\n",
    "study.optimize(objective,n_trials = 3)\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\" Number of finid=shed trials: \",len(study.trials))\n",
    "print(\" Number of pruned trials: \", len(pruned_trials))\n",
    "print(\" Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "def objective(trial):\n",
    "    # Integer parameter\n",
    "    hidden_layer = 32#trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "    out_layer = trial.suggest_categorical(\"out_layer\", [32,64,128])\n",
    "    # Floating point parameter\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "    size = [10]#list(map(lambda x: int(x),b))\n",
    "    Conv =\"SAGE\"# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "    \n",
    "    walk_length = trial.suggest_int(\"walk length\",5,20,step = 5)\n",
    "    walks_per_node = trial.suggest_int(\"walks per node\", 5,10,step = 5)\n",
    "    context_size = trial.suggest_int(\"context size\",5,20,step = 5)\n",
    "    q = trial.suggest_float(\"q\", 0.0,0.9,step = 0.1)\n",
    "    p = trial.suggest_float(\"p\", 0.0,0.9,step = 0.1)\n",
    "    num_negative_samples = walks_per_node\n",
    "    loss = {\"Name\": \"Node2Vec\",\"walk length\":walk_length,\"walks per node\":walks_per_node,\"num negative samples\":num_negative_samples,\"context size\" : context_size ,\"p\":p ,\"q\":q, \"loss var\": \"Random Walks\"}#то же самое \n",
    "\n",
    "    model = Net(dataset = dataset,mode='unsupervised',conv=Conv,loss_function=loss[\"loss var\"],device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = len(size),dropout = dropout)\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_mask, batch_size = 677, sizes=size)\n",
    "    LossSampler = Sampler(data,device=device,mask=train_mask,loss_info=loss)\n",
    "    \n",
    "    model.to(device)\n",
    "    #Generate the optimizers\n",
    "#optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "    learning_rate=trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "    #optimizer = getattr(optim,optimizer_name)(model.parameters(),lr = learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "    #classifier = trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "    #classifier = \"logistic regression\"\n",
    "    classifier = \"catboost\"\n",
    "    #training of the model\n",
    "    for epoch in range(50):\n",
    "        loss = train(model,data,optimizer,LossSampler,train_loader,dropout)\n",
    "        train_acc, test_acc = test(model,data,classifier)\n",
    "        \n",
    "    trial.report(test_acc,epoch)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialsPruned()\n",
    "    return test_acc\n",
    "        \n",
    "    \n",
    "study = optuna.create_study(direction=\"maximize\",study_name=\"SAGE loss\")\n",
    "study.optimize(objective,n_trials = 1)\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\" Number of finid=shed trials: \",len(study.trials))\n",
    "print(\" Number of pruned trials: \", len(pruned_trials))\n",
    "print(\" Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Trial 0 finished with value: 0.7467652495378928 and parameters: {'out_layer': 32, 'dropout': 0.4, 'walk length': 5, 'walks per node': 5, 'context size': 10, 'q': 0.6000000000000001, 'p': 0.4, 'lr': 0.008830752825922258}. Best is trial 0 with value: 0.7467652495378928.\n",
    "\n",
    "2) d            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[25]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "def objective(trial):\n",
    "    # Integer parameter\n",
    "    hidden_layer = 32#trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "    out_layer = trial.suggest_categorical(\"out_layer\", [32,64,128])\n",
    "    # Floating point parameter\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "    size = [25]#list(map(lambda x: int(x),b))\n",
    "    Conv =\"SAGE\"# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "    \n",
    "    walk_length = trial.suggest_int(\"walk length\",5,20,step = 5)\n",
    "    walks_per_node = trial.suggest_int(\"walks per node\", 5,10,step = 5)\n",
    "    context_size = trial.suggest_int(\"context size\",5,20,step = 5)\n",
    "    q = trial.suggest_float(\"q\", 0.0,0.9,step = 0.1)\n",
    "    p = trial.suggest_float(\"p\", 0.0,0.9,step = 0.1)\n",
    "    num_negative_samples = walks_per_node\n",
    "    loss = {\"Name\": \"Node2Vec\",\"walk length\":walk_length,\"walks per node\":walks_per_node,\"num negative samples\":num_negative_samples,\"context size\" : context_size ,\"p\":p ,\"q\":q, \"loss var\": \"Random Walks\"}#то же самое \n",
    "\n",
    "    model = Net(dataset = dataset,mode='unsupervised',conv=Conv,loss_function=loss[\"loss var\"],device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = len(size),dropout = dropout)\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_mask, batch_size = 677, sizes=size)\n",
    "    LossSampler = Sampler(data,device=device,mask=train_mask,loss_info=loss)\n",
    "    \n",
    "    model.to(device)\n",
    "    #Generate the optimizers\n",
    "#optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "    learning_rate=trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "    #optimizer = getattr(optim,optimizer_name)(model.parameters(),lr = learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "    #classifier = trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "    classifier = \"logistic regression\"\n",
    "    #classifier = \"catboost\"\n",
    "    #training of the model\n",
    "    for epoch in range(50):\n",
    "        loss = train(model,data,optimizer,LossSampler,train_loader,dropout)\n",
    "        train_acc, test_acc = test(model,data,classifier)\n",
    "        \n",
    "    trial.report(test_acc,epoch)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialsPruned()\n",
    "    return test_acc\n",
    "        \n",
    "    \n",
    "study = optuna.create_study(direction=\"maximize\",study_name=\"SAGE loss\")\n",
    "study.optimize(objective,n_trials = 1)\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\" Number of finid=shed trials: \",len(study.trials))\n",
    "print(\" Number of pruned trials: \", len(pruned_trials))\n",
    "print(\" Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "def objective(trial):\n",
    "    # Integer parameter\n",
    "    hidden_layer = 32#trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "    out_layer = trial.suggest_categorical(\"out_layer\", [32,64,128])\n",
    "    # Floating point parameter\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "    size = [25]#list(map(lambda x: int(x),b))\n",
    "    Conv =\"SAGE\"# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "    \n",
    "    walk_length = trial.suggest_int(\"walk length\",5,20,step = 5)\n",
    "    walks_per_node = trial.suggest_int(\"walks per node\", 5,10,step = 5)\n",
    "    context_size = trial.suggest_int(\"context size\",5,20,step = 5)\n",
    "    q = trial.suggest_float(\"q\", 0.0,0.9,step = 0.1)\n",
    "    p = trial.suggest_float(\"p\", 0.0,0.9,step = 0.1)\n",
    "    num_negative_samples = walks_per_node\n",
    "    loss = {\"Name\": \"Node2Vec\",\"walk length\":walk_length,\"walks per node\":walks_per_node,\"num negative samples\":num_negative_samples,\"context size\" : context_size ,\"p\":p ,\"q\":q, \"loss var\": \"Random Walks\"}#то же самое \n",
    "\n",
    "    model = Net(dataset = dataset,mode='unsupervised',conv=Conv,loss_function=loss[\"loss var\"],device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = len(size),dropout = dropout)\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_mask, batch_size = 677, sizes=size)\n",
    "    LossSampler = Sampler(data,device=device,mask=train_mask,loss_info=loss)\n",
    "    \n",
    "    model.to(device)\n",
    "    #Generate the optimizers\n",
    "#optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "    learning_rate=trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "    #optimizer = getattr(optim,optimizer_name)(model.parameters(),lr = learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "    #classifier = trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "    #classifier = \"logistic regression\"\n",
    "    classifier = \"catboost\"\n",
    "    #training of the model\n",
    "    for epoch in range(50):\n",
    "        loss = train(model,data,optimizer,LossSampler,train_loader,dropout)\n",
    "        train_acc, test_acc = test(model,data,classifier)\n",
    "        \n",
    "    trial.report(test_acc,epoch)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialsPruned()\n",
    "    return test_acc\n",
    "        \n",
    "    \n",
    "study = optuna.create_study(direction=\"maximize\",study_name=\"SAGE loss\")\n",
    "study.optimize(objective,n_trials = 1)\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\" Number of finid=shed trials: \",len(study.trials))\n",
    "print(\" Number of pruned trials: \", len(pruned_trials))\n",
    "print(\" Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5,10]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "def objective(trial):\n",
    "    # Integer parameter\n",
    "    hidden_layer = trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "    out_layer = trial.suggest_categorical(\"out_layer\", [32,64,128])\n",
    "    # Floating point parameter\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "    size = [10,25]#list(map(lambda x: int(x),b))\n",
    "    Conv =\"SAGE\"# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "    \n",
    "    walk_length = trial.suggest_int(\"walk length\",5,20,step = 5)\n",
    "    walks_per_node = trial.suggest_int(\"walks per node\", 5,10,step = 5)\n",
    "    context_size = trial.suggest_int(\"context size\",5,20,step = 5)\n",
    "    q =1# trial.suggest_float(\"q\", 0.0,0.9,step = 0.1)\n",
    "    p = 1#trial.suggest_float(\"p\", 0.0,0.9,step = 0.1)\n",
    "    num_negative_samples = walks_per_node\n",
    "    loss = {\"Name\": \"SAGE\",\"walk length\":walk_length,\"walks per node\":walks_per_node,\"num negative samples\":num_negative_samples,\"context size\" : context_size ,\"p\":p ,\"q\":q, \"loss var\": \"Random Walks\"}#то же самое \n",
    "\n",
    "    model = Net(dataset = dataset,mode='unsupervised',conv=Conv,loss_function=loss[\"loss var\"],device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = len(size),dropout = dropout)\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_mask, batch_size = 677, sizes=size)\n",
    "    LossSampler = Sampler(data,device=device,mask=train_mask,loss_info=loss)\n",
    "    \n",
    "    model.to(device)\n",
    "    #Generate the optimizers\n",
    "#optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "    learning_rate=trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "    #optimizer = getattr(optim,optimizer_name)(model.parameters(),lr = learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "    #classifier = trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "    classifier = \"logistic regression\"\n",
    "    #classifier = \"catboost\"\n",
    "    #training of the model\n",
    "    for epoch in range(50):\n",
    "        loss = train(model,data,optimizer,LossSampler,train_loader,dropout)\n",
    "        train_acc, test_acc = test(model,data,classifier)\n",
    "        \n",
    "    trial.report(test_acc,epoch)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialsPruned()\n",
    "    return test_acc\n",
    "        \n",
    "    \n",
    "study = optuna.create_study(direction=\"maximize\",study_name=\"SAGE loss\")\n",
    "study.optimize(objective,n_trials = 1)\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\" Number of finid=shed trials: \",len(study.trials))\n",
    "print(\" Number of pruned trials: \", len(pruned_trials))\n",
    "print(\" Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "def objective(trial):\n",
    "    # Integer parameter\n",
    "    hidden_layer = trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "    out_layer = trial.suggest_categorical(\"out_layer\", [32,64,128])\n",
    "    # Floating point parameter\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "    size = [5,10]#list(map(lambda x: int(x),b))\n",
    "    Conv =\"SAGE\"# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "    \n",
    "    walk_length = trial.suggest_int(\"walk length\",5,20,step = 5)\n",
    "    walks_per_node = trial.suggest_int(\"walks per node\", 5,10,step = 5)\n",
    "    context_size = trial.suggest_int(\"context size\",5,20,step = 5)\n",
    "    q = trial.suggest_float(\"q\", 0.0,0.9,step = 0.1)\n",
    "    p = trial.suggest_float(\"p\", 0.0,0.9,step = 0.1)\n",
    "    num_negative_samples = walks_per_node\n",
    "    loss = {\"Name\": \"Node2Vec\",\"walk length\":walk_length,\"walks per node\":walks_per_node,\"num negative samples\":num_negative_samples,\"context size\" : context_size ,\"p\":p ,\"q\":q, \"loss var\": \"Random Walks\"}#то же самое \n",
    "\n",
    "    model = Net(dataset = dataset,mode='unsupervised',conv=Conv,loss_function=loss[\"loss var\"],device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = len(size),dropout = dropout)\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_mask, batch_size = 677, sizes=size)\n",
    "    LossSampler = Sampler(data,device=device,mask=train_mask,loss_info=loss)\n",
    "    \n",
    "    model.to(device)\n",
    "    #Generate the optimizers\n",
    "#optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "    learning_rate=trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "    #optimizer = getattr(optim,optimizer_name)(model.parameters(),lr = learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "    #classifier = trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "    #classifier = \"logistic regression\"\n",
    "    classifier = \"catboost\"\n",
    "    #training of the model\n",
    "    for epoch in range(50):\n",
    "        loss = train(model,data,optimizer,LossSampler,train_loader,dropout)\n",
    "        train_acc, test_acc = test(model,data,classifier)\n",
    "        \n",
    "    trial.report(test_acc,epoch)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialsPruned()\n",
    "    return test_acc\n",
    "        \n",
    "    \n",
    "study = optuna.create_study(direction=\"maximize\",study_name=\"SAGE loss\")\n",
    "study.optimize(objective,n_trials = 1)\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\" Number of finid=shed trials: \",len(study.trials))\n",
    "print(\" Number of pruned trials: \", len(pruned_trials))\n",
    "print(\" Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[10,25]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "def objective(trial):\n",
    "    # Integer parameter\n",
    "    hidden_layer = trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "    out_layer = trial.suggest_categorical(\"out_layer\", [32,64,128])\n",
    "    # Floating point parameter\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "    size = [10,25]\n",
    "    #list(map(lambda x: int(x),b))\n",
    "    Conv =\"SAGE\"# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "    \n",
    "    walk_length = trial.suggest_int(\"walk length\",5,20,step = 5)\n",
    "    walks_per_node = trial.suggest_int(\"walks per node\", 5,10,step = 5)\n",
    "    context_size = trial.suggest_int(\"context size\",5,20,step = 5)\n",
    "    q = trial.suggest_float(\"q\", 0.0,0.9,step = 0.1)\n",
    "    p = trial.suggest_float(\"p\", 0.0,0.9,step = 0.1)\n",
    "    num_negative_samples = walks_per_node\n",
    "    loss = {\"Name\": \"Node2Vec\",\"walk length\":walk_length,\"walks per node\":walks_per_node,\"num negative samples\":num_negative_samples,\"context size\" : context_size ,\"p\":p ,\"q\":q, \"loss var\": \"Random Walks\"}#то же самое \n",
    "\n",
    "    model = Net(dataset = dataset,mode='unsupervised',conv=Conv,loss_function=loss[\"loss var\"],device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = len(size),dropout = dropout)\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_mask, batch_size = 677, sizes=size)\n",
    "    LossSampler = Sampler(data,device=device,mask=train_mask,loss_info=loss)\n",
    "    \n",
    "    model.to(device)\n",
    "    #Generate the optimizers\n",
    "#optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "    learning_rate=trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "    #optimizer = getattr(optim,optimizer_name)(model.parameters(),lr = learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "    #classifier = trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "    classifier = \"logistic regression\"\n",
    "    #classifier = \"catboost\"\n",
    "    #training of the model\n",
    "    for epoch in range(50):\n",
    "        loss = train(model,data,optimizer,LossSampler,train_loader,dropout)\n",
    "        train_acc, test_acc = test(model,data,classifier)\n",
    "        \n",
    "    trial.report(test_acc,epoch)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialsPruned()\n",
    "    return test_acc\n",
    "        \n",
    "    \n",
    "study = optuna.create_study(direction=\"maximize\",study_name=\"SAGE loss\")\n",
    "study.optimize(objective,n_trials = 1)\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\" Number of finid=shed trials: \",len(study.trials))\n",
    "print(\" Number of pruned trials: \", len(pruned_trials))\n",
    "print(\" Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "def objective(trial):\n",
    "    # Integer parameter\n",
    "    hidden_layer = trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "    out_layer = trial.suggest_categorical(\"out_layer\", [32,64,128])\n",
    "    # Floating point parameter\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "    size = [10,25]#list(map(lambda x: int(x),b))\n",
    "    Conv =\"SAGE\"# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "    \n",
    "    walk_length = trial.suggest_int(\"walk length\",5,20,step = 5)\n",
    "    walks_per_node = trial.suggest_int(\"walks per node\", 5,10,step = 5)\n",
    "    context_size = trial.suggest_int(\"context size\",5,20,step = 5)\n",
    "    q = 1#trial.suggest_float(\"q\", 0.0,0.9,step = 0.1)\n",
    "    p = 1#trial.suggest_float(\"p\", 0.0,0.9,step = 0.1)\n",
    "    num_negative_samples = walks_per_node\n",
    "    loss = {\"Name\": \"SAGE\",\"walk length\":walk_length,\"walks per node\":walks_per_node,\"num negative samples\":num_negative_samples,\"context size\" : context_size ,\"p\":p ,\"q\":q, \"loss var\": \"Random Walks\"}#то же самое \n",
    "\n",
    "    model = Net(dataset = dataset,mode='unsupervised',conv=Conv,loss_function=loss[\"loss var\"],device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = len(size),dropout = dropout)\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_mask, batch_size = 677, sizes=size)\n",
    "    LossSampler = Sampler(data,device=device,mask=train_mask,loss_info=loss)\n",
    "    \n",
    "    model.to(device)\n",
    "    #Generate the optimizers\n",
    "#optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "    learning_rate=trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "    #optimizer = getattr(optim,optimizer_name)(model.parameters(),lr = learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "    #classifier = trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "    #classifier = \"logistic regression\"\n",
    "    classifier = \"catboost\"\n",
    "    #training of the model\n",
    "    for epoch in range(50):\n",
    "        loss = train(model,data,optimizer,LossSampler,train_loader,dropout)\n",
    "        train_acc, test_acc = test(model,data,classifier)\n",
    "        \n",
    "    trial.report(test_acc,epoch)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialsPruned()\n",
    "    return test_acc\n",
    "        \n",
    "    \n",
    "study = optuna.create_study(direction=\"maximize\",study_name=\"SAGE loss\")\n",
    "study.optimize(objective,n_trials = 1)\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\" Number of finid=shed trials: \",len(study.trials))\n",
    "print(\" Number of pruned trials: \", len(pruned_trials))\n",
    "print(\" Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5,10,25]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "def objective(trial):\n",
    "    # Integer parameter\n",
    "    hidden_layer = trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "    out_layer = trial.suggest_categorical(\"out_layer\", [32,64,128])\n",
    "    # Floating point parameter\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "    size = [5,10,25]#list(map(lambda x: int(x),b))\n",
    "    Conv =\"SAGE\"# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "    \n",
    "    walk_length = trial.suggest_int(\"walk length\",5,20,step = 5)\n",
    "    walks_per_node = trial.suggest_int(\"walks per node\", 5,10,step = 5)\n",
    "    context_size = trial.suggest_int(\"context size\",5,20,step = 5)\n",
    "    q = trial.suggest_float(\"q\", 0.0,0.9,step = 0.1)\n",
    "    p = trial.suggest_float(\"p\", 0.0,0.9,step = 0.1)\n",
    "    num_negative_samples = walks_per_node\n",
    "    loss = {\"Name\": \"Node2Vec\",\"walk length\":walk_length,\"walks per node\":walks_per_node,\"num negative samples\":num_negative_samples,\"context size\" : context_size ,\"p\":p ,\"q\":q, \"loss var\": \"Random Walks\"}#то же самое \n",
    "\n",
    "    model = Net(dataset = dataset,mode='unsupervised',conv=Conv,loss_function=loss[\"loss var\"],device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = len(size),dropout = dropout)\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_mask, batch_size = 677, sizes=size)\n",
    "    LossSampler = Sampler(data,device=device,mask=train_mask,loss_info=loss)\n",
    "    \n",
    "    model.to(device)\n",
    "    #Generate the optimizers\n",
    "#optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "    learning_rate=trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "    #optimizer = getattr(optim,optimizer_name)(model.parameters(),lr = learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "    #classifier = trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "    classifier = \"logistic regression\"\n",
    "    #classifier = \"catboost\"\n",
    "    #training of the model\n",
    "    for epoch in range(50):\n",
    "        loss = train(model,data,optimizer,LossSampler,train_loader,dropout)\n",
    "        train_acc, test_acc = test(model,data,classifier)\n",
    "        \n",
    "    trial.report(test_acc,epoch)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialsPruned()\n",
    "    return test_acc\n",
    "        \n",
    "    \n",
    "study = optuna.create_study(direction=\"maximize\",study_name=\"SAGE loss\")\n",
    "study.optimize(objective,n_trials = 1)\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\" Number of finid=shed trials: \",len(study.trials))\n",
    "print(\" Number of pruned trials: \", len(pruned_trials))\n",
    "print(\" Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "def objective(trial):\n",
    "    # Integer parameter\n",
    "    hidden_layer = trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "    out_layer = trial.suggest_categorical(\"out_layer\", [32,64,128])\n",
    "    # Floating point parameter\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "    size = [5,10,25]#list(map(lambda x: int(x),b))\n",
    "    Conv =\"SAGE\"# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "    \n",
    "    walk_length = trial.suggest_int(\"walk length\",5,20,step = 5)\n",
    "    walks_per_node = trial.suggest_int(\"walks per node\", 5,10,step = 5)\n",
    "    context_size = trial.suggest_int(\"context size\",5,20,step = 5)\n",
    "    q = trial.suggest_float(\"q\", 0.0,0.9,step = 0.1)\n",
    "    p = trial.suggest_float(\"p\", 0.0,0.9,step = 0.1)\n",
    "    num_negative_samples = walks_per_node\n",
    "    loss = {\"Name\": \"Node2Vec\",\"walk length\":walk_length,\"walks per node\":walks_per_node,\"num negative samples\":num_negative_samples,\"context size\" : context_size ,\"p\":p ,\"q\":q, \"loss var\": \"Random Walks\"}#то же самое \n",
    "\n",
    "    model = Net(dataset = dataset,mode='unsupervised',conv=Conv,loss_function=loss[\"loss var\"],device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = len(size),dropout = dropout)\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_mask, batch_size = 677, sizes=size)\n",
    "    LossSampler = Sampler(data,device=device,mask=train_mask,loss_info=loss)\n",
    "    \n",
    "    model.to(device)\n",
    "    #Generate the optimizers\n",
    "#optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "    learning_rate=trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "    #optimizer = getattr(optim,optimizer_name)(model.parameters(),lr = learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "    #classifier = trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "    #classifier = \"logistic regression\"\n",
    "    classifier = \"catboost\"\n",
    "    #training of the model\n",
    "    for epoch in range(50):\n",
    "        loss = train(model,data,optimizer,LossSampler,train_loader,dropout)\n",
    "        train_acc, test_acc = test(model,data,classifier)\n",
    "        \n",
    "    trial.report(test_acc,epoch)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialsPruned()\n",
    "    return test_acc\n",
    "        \n",
    "    \n",
    "study = optuna.create_study(direction=\"maximize\",study_name=\"SAGE loss\")\n",
    "study.optimize(objective,n_trials = 1)\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\" Number of finid=shed trials: \",len(study.trials))\n",
    "print(\" Number of pruned trials: \", len(pruned_trials))\n",
    "print(\" Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "My__RW_Neighbour.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
