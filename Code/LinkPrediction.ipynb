{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-gN3S4NhAjs",
    "outputId": "fef06879-cb42-4f09-db2d-88671e8c4315"
   },
   "outputs": [],
   "source": [
    "#!pip install -q torch==1.7.1+cpu torchvision==0.8.2+cpu torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15]])\n",
    "b = torch.tensor([[6,7,8,9,10],[11,12,13,14,15],[1,2,3,4,5]])\n",
    "torch.cat((a,b),dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -q --no-index torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html\n",
    "#!pip install -q --no-index torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html\n",
    "#!pip install -q --no-index torch-cluster -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html\n",
    "#!pip install -q --no-index torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html\n",
    "#!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# link prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from torch_geometric.data import NeighborSampler\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n",
    "from modules.model import Net\n",
    "from modules.sampling import Sampler\n",
    "import random\n",
    "from modules.negativeSampling import NegativeSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randint(10,(8,10)).to('cuda')\n",
    "b = torch.randint(10,(8,10)).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3326, 0.5794, 0.4753, 0.4698, 0.6283, 0.2083, 0.2654, 0.8725, 0.6891,\n",
       "         0.4477],\n",
       "        [0.4337, 0.6463, 0.4055, 0.5853, 0.0628, 0.0786, 0.8766, 0.6512, 0.5506,\n",
       "         0.8531],\n",
       "        [0.5603, 0.1819, 0.3964, 0.0639, 0.3416, 0.8900, 0.4981, 0.4739, 0.6932,\n",
       "         0.8974],\n",
       "        [0.6672, 0.0271, 0.3291, 0.9649, 0.3474, 0.8942, 0.9534, 0.6314, 0.1395,\n",
       "         0.6212],\n",
       "        [0.7194, 0.3476, 0.7613, 0.7796, 0.8874, 0.3902, 0.8320, 0.9551, 0.5834,\n",
       "         0.2160],\n",
       "        [0.2752, 0.7583, 0.6043, 0.3503, 0.0339, 0.7028, 0.4337, 0.8955, 0.7161,\n",
       "         0.1990],\n",
       "        [0.8454, 0.9602, 0.3244, 0.0704, 0.7366, 0.1887, 0.2671, 0.6973, 0.8424,\n",
       "         0.3890],\n",
       "        [0.8389, 0.4397, 0.6772, 0.6053, 0.5383, 0.3185, 0.5720, 0.9564, 0.2286,\n",
       "         0.8860]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = torch.rand(a.shape)\n",
    "prob=prob.to('cuda')\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 0., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 0., 1., 0., 0., 1., 1., 1.],\n",
       "        [0., 1., 0., 0., 0., 1., 0., 0., 1., 1.],\n",
       "        [1., 0., 1., 1., 1., 0., 1., 0., 0., 1.],\n",
       "        [0., 0., 1., 1., 0., 1., 1., 1., 0., 1.],\n",
       "        [0., 1., 1., 1., 1., 1., 1., 0., 1., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3,3,,6, ,3,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 0, 5, 7, 0, 2, 7, 5, 2],\n",
      "        [2, 6, 4, 7, 5, 7, 8, 0, 6],\n",
      "        [1, 8, 7, 8, 4, 3, 3, 7, 8],\n",
      "        [1, 9, 4, 2, 3, 2, 5, 3, 1],\n",
      "        [3, 2, 9, 0, 4, 2, 6, 2, 8],\n",
      "        [7, 2, 3, 3, 4, 5, 9, 8, 6],\n",
      "        [0, 1, 7, 7, 1, 3, 9, 7, 6],\n",
      "        [1, 5, 6, 6, 0, 0, 7, 7, 0]], device='cuda:0')\n",
      "tensor([[4, 2, 4, 5, 8, 3, 4, 1, 9],\n",
      "        [1, 1, 9, 4, 1, 2, 5, 4, 1],\n",
      "        [3, 0, 5, 3, 0, 7, 1, 7, 9],\n",
      "        [3, 7, 2, 7, 1, 1, 6, 1, 2],\n",
      "        [8, 4, 1, 5, 5, 9, 0, 9, 4],\n",
      "        [1, 8, 5, 3, 9, 6, 6, 8, 3],\n",
      "        [4, 2, 5, 7, 1, 2, 3, 1, 9],\n",
      "        [1, 9, 2, 9, 2, 3, 2, 2, 4]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([7, 3, 7, 3, 4, 0], device='cuda:0')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(a[:,1:])\n",
    "print(b[:,1:])\n",
    "ten= (a[:,1:] == b[:,1:])\n",
    "indices = (ten == True).nonzero().t()[1]\n",
    "import collections\n",
    "indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(a==b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True, False,  True,  True, False,  True,  True],\n",
       "        [ True,  True,  True,  True, False,  True, False,  True,  True,  True],\n",
       "        [ True,  True,  True, False,  True,  True,  True,  True, False,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True, False, False,  True,  True,  True, False,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True, False, False,  True]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(a!=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9-len(torch.unique((a==b).nonzero().t()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = a!=7\n",
    "a2 = a==b\n",
    "a3 = b!=7\n",
    "len(torch.unique((a1*a2*a3).nonzero().t()[0]))/len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False,  True, False, False, False,  True, False],\n",
       "        [ True, False, False, False, False,  True, False, False, False, False],\n",
       "        [False,  True, False, False, False, False, False, False, False, False]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c*d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0932, 0.7630, 0.4104, 0.0950])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=torch.rand(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1381, 0.8874, 0.0345, 0.3131],\n",
       "        [0.8470, 0.2617, 0.7583, 0.6486],\n",
       "        [0.6139, 0.4134, 0.0813, 0.4948],\n",
       "        [0.4997, 0.3430, 0.6893, 0.7920]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1381, 0.8874, 0.0345, 0.3131],\n",
       "        [0.8470, 0.2617, 0.7583, 0.6486],\n",
       "        [0.8470, 0.2617, 0.7583, 0.6486],\n",
       "        [0.4997, 0.3430, 0.6893, 0.7920]])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[ind1.t()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = torch.tensor([1, 0, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 3])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind1=(b>c).nonzero()\n",
    "ind1.t()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8470, 0.2617, 0.7583, 0.6486],\n",
       "        [0.1381, 0.8874, 0.0345, 0.3131],\n",
       "        [0.8470, 0.2617, 0.7583, 0.6486],\n",
       "        [0.4997, 0.3430, 0.6893, 0.7920]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_nonzero = torch.nonzero((b>c),as_tuple=True)\n",
    "first_nonzero[0]\n",
    "b[first_nonzero[0]][first_nonzero[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 0, 1])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_new=torch.tensor([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=0.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x = data.x.to(device)\n",
    "y = data.y.squeeze().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_sparse import SparseTensor\n",
    "E = data.num_edges\n",
    "N = data.num_nodes\n",
    "adj = SparseTensor(\n",
    "            row=data.edge_index[0], col=data.edge_index[1],\n",
    "            value=torch.arange(E, device=data.edge_index.device),\n",
    "            sparse_sizes=(N, N))\n",
    "walk_length = 2\n",
    "batch_size=3\n",
    "start = torch.randint(0, N, (batch_size, ), dtype=torch.long)\n",
    "node_idx = adj.random_walk(start.flatten(), walk_length)\n",
    "print(start)\n",
    "print(node_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = torch.tensor([True]*int(0.8*len(data.x)+1) + [False]*int(0.2*len(data.x)))\n",
    "#val_mask = torch.tensor([False]*int(0.6*len(data.x)+1) + [True]*int(0.2*len(data.x)+1)+[False]*int(0.2*len(data.x)))\n",
    "test_mask = torch.tensor([False]*int(0.8*len(data.x)+1) + [True]*int(0.2*len(data.x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data to train and test\n",
    "train_edge_index = []\n",
    "test_edge_index = []\n",
    "indices_to_delete  = random.choices(list(range(len(data.edge_index[0]))), k = int(len(data.edge_index[0])*0.3))\n",
    "for i,x in enumerate(list(zip(*data.edge_index))):\n",
    "    if i in indices_to_delete:\n",
    "        test_edge_index.append(x)\n",
    "    else:\n",
    "        train_edge_index.append(x)\n",
    "test_edge_index = torch.tensor(np.array(list(zip(*test_edge_index))))\n",
    "train_edge_index = torch.tensor(np.array(list(zip(*train_edge_index))))\n",
    "#data.edge_index = train_edge_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSampler_LP(NegativeSampler):\n",
    "     def negative_sampling(self, batch, num_negative_samples):\n",
    "        a = self.data.edge_index\n",
    "        f = self.adj_list(a)\n",
    "        g = dict()\n",
    "        l = batch.tolist()\n",
    "        for e in l:\n",
    "            if e not in g:\n",
    "                g[e] = []\n",
    "                g[e].append(random.choice(f[e]))\n",
    "            else: \n",
    "                g[e].append(random.choice(f[e]))\n",
    "\n",
    "        return self.torch_list(g)\n",
    "num_neg_pairs = len(test_edge_index[0])\n",
    "NS = NegativeSampler(data)\n",
    "NS_LP= NegativeSampler_LP(data)\n",
    "if num_neg_pairs>len(data.x):\n",
    "    indices_start_neg_pairs = list(range(len(data.x)))\n",
    "    l = random.choices(list(range(len(data.x))), k = num_neg_pairs-len(data.x))\n",
    "    indices_start_neg_pairs+=l\n",
    "else:\n",
    "    indices_start_neg_pairs = random.choices(list(range(len(data.x))), k = num_neg_pairs-len(data.x) )\n",
    "nei = NS_LP.negative_sampling(torch.tensor(indices_start_neg_pairs),1)\n",
    "nei = torch.transpose(nei, 0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append negative samples to test set\n",
    "y_true = [1]*len(test_edge_index[0])\n",
    "test_edge_index=torch.cat((test_edge_index,nei),1)\n",
    "y_true+=[0]*len(nei[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.edge_index = train_edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Для начала посмотрим, что получится, если передать только фичи. Без обучения**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Теперь сравним с результатом после обучения эмбедингов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAGE = {\"Name\":\"SAGE\" , \"walk length\":5,\"walks per node\":50,\"num negative samples\":20,\"context size\" : 10,\"p\":1,\"q\":1, \"loss var\": \"Random Walks\"}\n",
    "DeepWalk = {\"Name\": \"DeepWalk\",\"walk length\":10,\"walks per node\":10,\"num negative samples\":10,\"context size\" : 10,\"p\":1,\"q\":1,\"loss var\": \"Random Walks\" } #Проблемы с памятью после того, как увеличила количество тренировочных данных\n",
    "#DeepWalk = {\"Name\": \"DeepWalk\",\"walk length\":40,\"walks per node\":80,\"num negative samples\":20,\"context size\" : 10,\"p\":1,\"q\":1,\"loss var\": \"Random Walks\" } #Проблемы с памятью после того, как увеличила количество тренировочных данных\n",
    "Node2Vec = {\"Name\": \"Node2Vec\",\"walk length\":20,\"walks per node\":10,\"num negative samples\":1,\"context size\" : 10,\"p\":1.414 ,\"q\":1.414, \"loss var\": \"Random Walks\"}#то же самое \n",
    "#Node2Vec = {\"Name\": \"Node2Vec\",\"walk length\":100,\"walks per node\":18,\"num negative samples\":20,\"context size\" : 16,\"p\":1.414 ,\"q\":1.414, \"loss var\": \"Random Walks\"}#то же самое \n",
    "\n",
    "LINE2 = {\"Name\": \"LINE2\",\"C\": \"Adj\",\"num negative samples\":10,\"loss var\": \"Context Matrix\"} #настроить параметры в принципе при 0.003 что то выходит  \n",
    "HOPE_RPR = {\"Name\": \"HOPE: RPR\",\"C\":\"RPR\",\"loss var\": \"Factorization\"}#оч большой loss\n",
    "\n",
    "\n",
    "GraphFactorization = {\"Name\": \"Graph Factorization\",\"C\":\"Adj\",\"loss var\": \"Factorization\"} #jоч большой loss\n",
    "HOPE_Katz = {\"Name\": \"HOPE: Katz\",\"C\":\"Katz\",\"loss var\": \"Factorization\"}#Проблемы оч большой loss\n",
    "VERSE_PPR =  {\"Name\": \"VERSE\",\"C\": \"PPR\",\"num negative samples\":10,\"loss var\": \"Context Matrix\"}  #проблеиы\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, roc_auc_score\n",
    "\n",
    "def train_lp(model,data,optimizer,Sampler,train_loader,dropout):\n",
    "    model.train()        \n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    if model.mode == 'unsupervised':\n",
    "        if model.conv=='GCN':\n",
    "            out = model.inference(data.to(device),dp=dropout)\n",
    "            samples = Sampler.sample(torch.tensor(list(range(len(data.x)))))\n",
    "            loss = model.loss(out, samples)\n",
    "            total_loss+=loss\n",
    "        else:\n",
    "            for batch_size, n_id, adjs in train_loader:\n",
    "                # adjs holds a list of (edge_index, e_id, size) tuples.\n",
    "                adjs = [adj.to(device) for adj in adjs]\n",
    "                out = model.forward(data.x[n_id].to(device), adjs)\n",
    "                samples = Sampler.sample((n_id.numpy().tolist())[:batch_size])\n",
    "                loss = model.loss(out, samples)#pos_batch.to(device), neg_batch.to(device))\n",
    "                #print(out.shape, samples[0].shape,samples[1].shape)\n",
    "                total_loss+=loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()      \n",
    "        return total_loss /len(train_loader)\n",
    "    elif model.mode== 'supervised':\n",
    "        if model.conv=='GCN':\n",
    "            out = model.inference(data.to(device),dp=dropout)\n",
    "            loss = model.loss_sup(out[train_mask],y[train_mask])\n",
    "            total_loss+=loss \n",
    "        else:\n",
    "            for batch_size, n_id, adjs in train_loader:\n",
    "                adjs = [adj.to(device) for adj in adjs]\n",
    "                out = model.forward(data.x[n_id].to(device), adjs)\n",
    "                loss = model.loss_sup(out,y[n_id[:batch_size]])\n",
    "                total_loss+=loss\n",
    "        total_loss.backward(retain_graph=True)\n",
    "        optimizer.step()      \n",
    "        return total_loss /len(train_loader)       \n",
    "\n",
    "@torch.no_grad()\n",
    "def test_lp(model,data,classifier,y_true): \n",
    "    model.eval()\n",
    "    out = model.inference(data.to(device))\n",
    "    y_true = np.array(y_true)\n",
    "    if model.mode == 'supervised':\n",
    "        y_true = y.cpu().unsqueeze(-1)\n",
    "        y_pred = out.cpu().argmax(dim=-1, keepdim=True)\n",
    "        accs = []\n",
    "        \n",
    "        accs=[int(y_pred.eq(y_true[mas]).sum()) / int(mask.sum())]\n",
    "        return accs\n",
    "    \n",
    "    elif model.mode == 'unsupervised':\n",
    "        y_pred = []\n",
    "        for x in list(zip(*test_edge_index)):\n",
    "            if torch.sigmoid(torch.dot(out[x[0]],out[x[1]])):\n",
    "                y_pred.append( 1 )\n",
    "            else:\n",
    "                y_pred.append(0)\n",
    "            print(torch.sigmoid(torch.dot(out[x[0]],out[x[1]])))\n",
    "\n",
    "        return roc_auc_score(y_true,np.array(y_pred)) \n",
    "        #return [precision_score(y_true[train_mask.cpu()], best_preds_train, average='macro'), precision_score(y_true[val_mask.cpu()], best_preds_val, average='macro'),precision_score(y_true[test_mask.cpu()], best_preds, average='macro')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models = [Net(dataset = dataset,mode='supervised',conv='GCN',device=device), Net(dataset = dataset,mode='supervised',conv='GAT',device=device),Net(dataset = dataset,mode='supervised',conv='SGC',device=device), Net(dataset = dataset,mode='unsupervised',conv='GCN',device=device),Net(dataset = dataset,mode='unsupervised',conv='GAT',device=device),Net(dataset = dataset,mode='unsupervised',conv='SAGE',device=device)]\n",
    "# lr=0.005\n",
    "SAGE = {\"Name\":\"SAGE\" , \"walk length\":10,\"walks per node\":1,\"num negative samples\":10,\"context size\" :5 ,\"p\":1,\"q\":1, \"loss var\": \"Random Walks\"}\n",
    "DeepWalk = {\"Name\": \"DeepWalk\",\"walk length\":10,\"walks per node\":10,\"num negative samples\":10,\"context size\" : 10,\"p\":1,\"q\":1,\"loss var\": \"Random Walks\" } #Проблемы с памятью после того, как увеличила количество тренировочных данных\n",
    "#DeepWalk = {\"Name\": \"DeepWalk\",\"walk length\":40,\"walks per node\":80,\"num negative samples\":20,\"context size\" : 10,\"p\":1,\"q\":1,\"loss var\": \"Random Walks\" } #Проблемы с памятью после того, как увеличила количество тренировочных данных\n",
    "Node2Vec = {\"Name\": \"Node2Vec\",\"walk length\":20,\"walks per node\":10,\"num negative samples\":1,\"context size\" : 10,\"p\":1.414 ,\"q\":1.414, \"loss var\": \"Random Walks\"}#то же самое \n",
    "#Node2Vec = {\"Name\": \"Node2Vec\",\"walk length\":100,\"walks per node\":18,\"num negative samples\":20,\"context size\" : 16,\"p\":1.414 ,\"q\":1.414, \"loss var\": \"Random Walks\"}#то же самое \n",
    "\n",
    "LINE2 = {\"Name\": \"LINE2\",\"C\": \"Adj\",\"num negative samples\":10,\"loss var\": \"Context Matrix\"} #настроить параметры в принципе при 0.003 что то выходит  \n",
    "HOPE_RPR = {\"Name\": \"HOPE: RPR\",\"C\":\"RPR\",\"loss var\": \"Factorization\"}#оч большой loss\n",
    "\n",
    "\n",
    "GraphFactorization = {\"Name\": \"Graph Factorization\",\"C\":\"Adj\",\"loss var\": \"Factorization\"} #jоч большой loss\n",
    "HOPE_Katz = {\"Name\": \"HOPE: Katz\",\"C\":\"Katz\",\"loss var\": \"Factorization\"}#Проблемы оч большой loss\n",
    "VERSE_PPR =  {\"Name\": \"VERSE\",\"C\": \"PPR\",\"num negative samples\":10,\"loss var\": \"Context Matrix\"}  #проблеиы\n",
    "\n",
    "sizes = [[5],[10],[25],[5,10],[10,25],[5,10,25]]\n",
    "\n",
    "hidden_layer = 64\n",
    "out_layer = 128\n",
    "dropout = 0.5\n",
    "size = [10,25]\n",
    "learning_rate = 0.003\n",
    "\n",
    "classifier = \"logistic regression\"\n",
    "\n",
    "from datetime import datetime\n",
    "d = datetime.now()\n",
    "for Conv in ['GCN']:\n",
    "    for loss in [SAGE]:\n",
    "        #train_loader = NeighborSampler(data.edge_index, batch_size = 2708, sizes=size)\n",
    "       \n",
    "        #LossSampler = Sampler(data,device=device,mask=train_mask,loss_info=loss)\n",
    "        #model = Net(dataset = dataset,mode='unsupervised',conv=Conv,loss_function=loss[\"loss var\"],device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = len(size),dropout = dropout)\n",
    "        #model.to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001,weight_decay = 1e-5)\n",
    "        #scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.01, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "       # scheduler=lr_scheduler.StepLR(optimizer, step_size=25,gamma=0.1)\n",
    "        losses=[]\n",
    "        train_accs=[]\n",
    "        test_accs=[]\n",
    "        val_accs=[]\n",
    "        name_of_plot='conv: '+model.conv+', mode: '+model.mode+', loss from '+loss[\"Name\"]\n",
    "       \n",
    "        print(name_of_plot)\n",
    "        \n",
    "        for epoch in range(100):\n",
    "            print(epoch)\n",
    "            loss = train_lp(model,data,optimizer,LossSampler,train_loader,0)\n",
    "            losses.append(loss)\n",
    "            #print(loss)\n",
    "            \n",
    "            #train_accs.append(train_acc)\n",
    "#test_accs.append(test_acc)\n",
    "        test_acc = test_lp(model,data,'logistic regression',y_true)\n",
    "           # val_accs.append(val_acc)\n",
    "           # log = 'Loss: {:.4f}, Epoch: {:03d}, Train: {:.4f}, Test: {:.4f}'\n",
    "            #scheduler.step()\n",
    "            #print(log.format(loss, epoch, train_acc, test_acc))\n",
    "        print('Test acc on the last epoch ', test_acc)\n",
    "        plt.plot(losses)\n",
    "        plt.title(name_of_plot+' loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lp(model,data,'logistic regression',y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = test_lp(model,data,'logistic regression',y_true)\n",
    "print('Test acc on the last epoch ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "def objective(trial):\n",
    "    # Integer parameter\n",
    "    hidden_layer = 32#trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "    out_layer = trial.suggest_int(\"out_layer\", 32,128,step = 32,log = False)\n",
    "    # Floating point parameter\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "   # size_str = trial.suggest_categorical(\"size\", [\"[10]\",\"[25]\",\"[5,10]\",\"[10,25]\",\"[5,10,25]\"])\n",
    "   # b = size_str.split(\"[\")[1].split(\"]\")[0].split(',')\n",
    "    size = [5]#list(map(lambda x: int(x),b))\n",
    "\n",
    "    Conv =\"GAT\"# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "    \n",
    "    walk_length = trial.suggest_int(\"walk length\",5,20,step = 5)\n",
    "    walks_per_node = trial.suggest_int(\"walks per node\", 5,20,step = 5)\n",
    "    context_size = trial.suggest_int(\"context size\",5,20,step = 5)\n",
    "    num_negative_samples = walks_per_node\n",
    "    loss = {\"Name\":\"SAGE\" , \"walk length\":walk_length,\"walks per node\":walks_per_node,\"num negative samples\":num_negative_samples,\"context size\" : context_size,\"p\":1,\"q\":1, \"loss var\": \"Random Walks\"}\n",
    "\n",
    "    model = Net(dataset = dataset,mode='unsupervised',conv=Conv,loss_function=loss[\"loss var\"],device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = len(size),dropout = dropout)\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_mask, batch_size = 677, sizes=size)\n",
    "    LossSampler = Sampler(data,device=device,mask=train_mask,loss_info=loss)\n",
    "    \n",
    "    model.to(device)\n",
    "    #Generate the optimizers\n",
    "#optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "    learning_rate=trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "    #optimizer = getattr(optim,optimizer_name)(model.parameters(),lr = learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "    #classifier = trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "    #classifier = \"logistic regression\"\n",
    "    classifier = \"catboost\"\n",
    "    #training of the model\n",
    "    for epoch in range(50):\n",
    "        loss = train(model,data,optimizer,LossSampler,train_loader,dropout)\n",
    "        train_acc, test_acc = test(model,data,classifier)\n",
    "        \n",
    "    trial.report(test_acc,epoch)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialsPruned()\n",
    "    return test_acc\n",
    "        \n",
    "    \n",
    "study = optuna.create_study(direction=\"maximize\",study_name=\"SAGE loss\")\n",
    "study.optimize(objective,n_trials = 3)\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\" Number of finid=shed trials: \",len(study.trials))\n",
    "print(\" Number of pruned trials: \", len(pruned_trials))\n",
    "print(\" Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "def objective(trial):\n",
    "    # Integer parameter\n",
    "    hidden_layer = 32#trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "    out_layer = trial.suggest_int(\"out_layer\", 32,128,step = 32,log = False)\n",
    "    # Floating point parameter\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "   # size_str = trial.suggest_categorical(\"size\", [\"[10]\",\"[25]\",\"[5,10]\",\"[10,25]\",\"[5,10,25]\"])\n",
    "   # b = size_str.split(\"[\")[1].split(\"]\")[0].split(',')\n",
    "    size = [5]#list(map(lambda x: int(x),b))\n",
    "\n",
    "    Conv =\"GAT\"# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "    \n",
    "    walk_length = trial.suggest_int(\"walk length\",5,20,step = 5)\n",
    "    walks_per_node = trial.suggest_int(\"walks per node\", 5,20,step = 5)\n",
    "    context_size = trial.suggest_int(\"context size\",5,20,step = 5)\n",
    "    num_negative_samples = walks_per_node\n",
    "    loss = {\"Name\":\"SAGE\" , \"walk length\":walk_length,\"walks per node\":walks_per_node,\"num negative samples\":num_negative_samples,\"context size\" : context_size,\"p\":1,\"q\":1, \"loss var\": \"Random Walks\"}\n",
    "\n",
    "    model = Net(dataset = dataset,mode='unsupervised',conv=Conv,loss_function=loss[\"loss var\"],device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = len(size),dropout = dropout)\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_mask, batch_size = 677, sizes=size)\n",
    "    LossSampler = Sampler(data,device=device,mask=train_mask,loss_info=loss)\n",
    "    \n",
    "    model.to(device)\n",
    "    #Generate the optimizers\n",
    "#optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "    learning_rate=trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "    #optimizer = getattr(optim,optimizer_name)(model.parameters(),lr = learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "    #classifier = trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "    classifier = \"logistic regression\"\n",
    "    #classifier = \"catboost\"\n",
    "    #training of the model\n",
    "    for epoch in range(50):\n",
    "        loss = train(model,data,optimizer,LossSampler,train_loader,dropout)\n",
    "        train_acc, test_acc = test(model,data,classifier)\n",
    "        \n",
    "    trial.report(test_acc,epoch)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialsPruned()\n",
    "    return test_acc\n",
    "        \n",
    "    \n",
    "study = optuna.create_study(direction=\"maximize\",study_name=\"SAGE loss\")\n",
    "study.optimize(objective,n_trials = 3)\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\" Number of finid=shed trials: \",len(study.trials))\n",
    "print(\" Number of pruned trials: \", len(pruned_trials))\n",
    "print(\" Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "def objective(trial):\n",
    "    # Integer parameter\n",
    "    hidden_layer =32# trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "    out_layer = trial.suggest_int(\"out_layer\", 32,128,step = 32,log = False)\n",
    "    # Floating point parameter\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "   # size_str = trial.suggest_categorical(\"size\", [\"[10]\",\"[25]\",\"[5,10]\",\"[10,25]\",\"[5,10,25]\"])\n",
    "   # b = size_str.split(\"[\")[1].split(\"]\")[0].split(',')\n",
    "    size = [10]#list(map(lambda x: int(x),b))\n",
    "\n",
    "    Conv =\"GAT\"# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "    \n",
    "    walk_length = trial.suggest_int(\"walk length\",5,20,step = 5)\n",
    "    walks_per_node = trial.suggest_int(\"walks per node\", 5,20,step = 5)\n",
    "    context_size = trial.suggest_int(\"context size\",5,20,step = 5)\n",
    "    num_negative_samples = walks_per_node\n",
    "    loss = {\"Name\":\"SAGE\" , \"walk length\":walk_length,\"walks per node\":walks_per_node,\"num negative samples\":num_negative_samples,\"context size\" : context_size,\"p\":1,\"q\":1, \"loss var\": \"Random Walks\"}\n",
    "\n",
    "    model = Net(dataset = dataset,mode='unsupervised',conv=Conv,loss_function=loss[\"loss var\"],device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = len(size),dropout = dropout)\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_mask, batch_size = 677, sizes=size)\n",
    "    LossSampler = Sampler(data,device=device,mask=train_mask,loss_info=loss)\n",
    "    \n",
    "    model.to(device)\n",
    "    #Generate the optimizers\n",
    "#optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "    learning_rate=trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "    #optimizer = getattr(optim,optimizer_name)(model.parameters(),lr = learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "    #classifier = trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "    #classifier = \"logistic regression\"\n",
    "    classifier = \"catboost\"\n",
    "    #training of the model\n",
    "    for epoch in range(50):\n",
    "        loss = train(model,data,optimizer,LossSampler,train_loader,dropout)\n",
    "        train_acc, test_acc = test(model,data,classifier)\n",
    "        \n",
    "    trial.report(test_acc,epoch)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialsPruned()\n",
    "    return test_acc\n",
    "        \n",
    "    \n",
    "study = optuna.create_study(direction=\"maximize\",study_name=\"SAGE loss\")\n",
    "study.optimize(objective,n_trials = 3)\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\" Number of finid=shed trials: \",len(study.trials))\n",
    "print(\" Number of pruned trials: \", len(pruned_trials))\n",
    "print(\" Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "def objective(trial):\n",
    "    # Integer parameter\n",
    "    hidden_layer =32# trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "    out_layer = trial.suggest_int(\"out_layer\", 32,128,step = 32,log = False)\n",
    "    # Floating point parameter\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "   # size_str = trial.suggest_categorical(\"size\", [\"[10]\",\"[25]\",\"[5,10]\",\"[10,25]\",\"[5,10,25]\"])\n",
    "   # b = size_str.split(\"[\")[1].split(\"]\")[0].split(',')\n",
    "    size = [10]#list(map(lambda x: int(x),b))\n",
    "\n",
    "    Conv =\"GAT\"# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "    \n",
    "    walk_length = trial.suggest_int(\"walk length\",5,20,step = 5)\n",
    "    walks_per_node = trial.suggest_int(\"walks per node\", 5,10,step = 5)\n",
    "    context_size = trial.suggest_int(\"context size\",5,20,step = 5)\n",
    "    num_negative_samples = walks_per_node\n",
    "    loss = {\"Name\":\"SAGE\" , \"walk length\":walk_length,\"walks per node\":walks_per_node,\"num negative samples\":num_negative_samples,\"context size\" : context_size,\"p\":1,\"q\":1, \"loss var\": \"Random Walks\"}\n",
    "\n",
    "    model = Net(dataset = dataset,mode='unsupervised',conv=Conv,loss_function=loss[\"loss var\"],device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = len(size),dropout = dropout)\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_mask, batch_size = 677, sizes=size)\n",
    "    LossSampler = Sampler(data,device=device,mask=train_mask,loss_info=loss)\n",
    "    \n",
    "    model.to(device)\n",
    "    #Generate the optimizers\n",
    "#optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "    learning_rate=trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "    #optimizer = getattr(optim,optimizer_name)(model.parameters(),lr = learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "    #classifier = trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "    classifier = \"logistic regression\"\n",
    "    #classifier = \"catboost\"\n",
    "    #training of the model\n",
    "    for epoch in range(50):\n",
    "        loss = train(model,data,optimizer,LossSampler,train_loader,dropout)\n",
    "        train_acc, test_acc = test(model,data,classifier)\n",
    "        \n",
    "    trial.report(test_acc,epoch)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialsPruned()\n",
    "    return test_acc\n",
    "        \n",
    "    \n",
    "study = optuna.create_study(direction=\"maximize\",study_name=\"SAGE loss\")\n",
    "study.optimize(objective,n_trials = 3)\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\" Number of finid=shed trials: \",len(study.trials))\n",
    "print(\" Number of pruned trials: \", len(pruned_trials))\n",
    "print(\" Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "def objective(trial):\n",
    "    # Integer parameter\n",
    "    hidden_layer = 32#trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "    out_layer = trial.suggest_int(\"out_layer\", 32,128,step = 32,log = False)\n",
    "    # Floating point parameter\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "   # size_str = trial.suggest_categorical(\"size\", [\"[10]\",\"[25]\",\"[5,10]\",\"[10,25]\",\"[5,10,25]\"])\n",
    "   # b = size_str.split(\"[\")[1].split(\"]\")[0].split(',')\n",
    "    size = [25]#list(map(lambda x: int(x),b))\n",
    "\n",
    "    Conv =\"GAT\"# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "    \n",
    "    walk_length = trial.suggest_int(\"walk length\",5,20,step = 5)\n",
    "    walks_per_node = trial.suggest_int(\"walks per node\", 5,10,step = 5)\n",
    "    context_size = trial.suggest_int(\"context size\",5,20,step = 5)\n",
    "    num_negative_samples = walks_per_node\n",
    "    loss = {\"Name\":\"SAGE\" , \"walk length\":walk_length,\"walks per node\":walks_per_node,\"num negative samples\":num_negative_samples,\"context size\" : context_size,\"p\":1,\"q\":1, \"loss var\": \"Random Walks\"}\n",
    "\n",
    "    model = Net(dataset = dataset,mode='unsupervised',conv=Conv,loss_function=loss[\"loss var\"],device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = len(size),dropout = dropout)\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_mask, batch_size = 677, sizes=size)\n",
    "    LossSampler = Sampler(data,device=device,mask=train_mask,loss_info=loss)\n",
    "    \n",
    "    model.to(device)\n",
    "    #Generate the optimizers\n",
    "#optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "    learning_rate=trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "    #optimizer = getattr(optim,optimizer_name)(model.parameters(),lr = learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "    #classifier = trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "    #classifier = \"logistic regression\"\n",
    "    classifier = \"catboost\"\n",
    "    #training of the model\n",
    "    for epoch in range(50):\n",
    "        loss = train(model,data,optimizer,LossSampler,train_loader,dropout)\n",
    "        train_acc, test_acc = test(model,data,classifier)\n",
    "        \n",
    "    trial.report(test_acc,epoch)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialsPruned()\n",
    "    return test_acc\n",
    "        \n",
    "    \n",
    "study = optuna.create_study(direction=\"maximize\",study_name=\"SAGE loss\")\n",
    "study.optimize(objective,n_trials = 3)\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\" Number of finid=shed trials: \",len(study.trials))\n",
    "print(\" Number of pruned trials: \", len(pruned_trials))\n",
    "print(\" Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "def objective(trial):\n",
    "    # Integer parameter\n",
    "    hidden_layer = 32#trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "    out_layer = trial.suggest_int(\"out_layer\", 32,128,step = 32,log = False)\n",
    "    # Floating point parameter\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "   # size_str = trial.suggest_categorical(\"size\", [\"[10]\",\"[25]\",\"[5,10]\",\"[10,25]\",\"[5,10,25]\"])\n",
    "   # b = size_str.split(\"[\")[1].split(\"]\")[0].split(',')\n",
    "    size = [25]#list(map(lambda x: int(x),b))\n",
    "\n",
    "    Conv =\"GAT\"# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "    \n",
    "    walk_length = trial.suggest_int(\"walk length\",5,20,step = 5)\n",
    "    walks_per_node = trial.suggest_int(\"walks per node\", 5,10,step = 5)\n",
    "    context_size = trial.suggest_int(\"context size\",5,20,step = 5)\n",
    "    num_negative_samples = walks_per_node\n",
    "    loss = {\"Name\":\"SAGE\" , \"walk length\":walk_length,\"walks per node\":walks_per_node,\"num negative samples\":num_negative_samples,\"context size\" : context_size,\"p\":1,\"q\":1, \"loss var\": \"Random Walks\"}\n",
    "\n",
    "    model = Net(dataset = dataset,mode='unsupervised',conv=Conv,loss_function=loss[\"loss var\"],device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = len(size),dropout = dropout)\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_mask, batch_size = 677, sizes=size)\n",
    "    LossSampler = Sampler(data,device=device,mask=train_mask,loss_info=loss)\n",
    "    \n",
    "    model.to(device)\n",
    "    #Generate the optimizers\n",
    "#optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "    learning_rate=trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "    #optimizer = getattr(optim,optimizer_name)(model.parameters(),lr = learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "    #classifier = trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "    classifier = \"logistic regression\"\n",
    "    #classifier = \"catboost\"\n",
    "    #training of the model\n",
    "    for epoch in range(50):\n",
    "        loss = train(model,data,optimizer,LossSampler,train_loader,dropout)\n",
    "        train_acc, test_acc = test(model,data,classifier)\n",
    "        \n",
    "    trial.report(test_acc,epoch)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialsPruned()\n",
    "    return test_acc\n",
    "        \n",
    "    \n",
    "study = optuna.create_study(direction=\"maximize\",study_name=\"SAGE loss\")\n",
    "study.optimize(objective,n_trials = 3)\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\" Number of finid=shed trials: \",len(study.trials))\n",
    "print(\" Number of pruned trials: \", len(pruned_trials))\n",
    "print(\" Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key,value))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "My__RW_Neighbour.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
