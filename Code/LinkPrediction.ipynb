{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-gN3S4NhAjs",
    "outputId": "fef06879-cb42-4f09-db2d-88671e8c4315"
   },
   "outputs": [],
   "source": [
    "#!pip install -q torch==1.7.1+cpu torchvision==0.8.2+cpu torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -q --no-index torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html\n",
    "#!pip install -q --no-index torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html\n",
    "#!pip install -q --no-index torch-cluster -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html\n",
    "#!pip install -q --no-index torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html\n",
    "#!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# link prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from torch_geometric.data import NeighborSampler\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n",
    "from modules.model import Net\n",
    "import random\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import optuna\n",
    "from modules.negativeSampling import NegativeSampler\n",
    "from modules.sampling import Sampler, SamplerContextMatrix, SamplerRandomWalk,SamplerFactorization,SamplerAPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "class Main():\n",
    "    def __init__(self,conv, device, loss_function, mode = 'unsupervised',**kwargs):\n",
    "        self.dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "        data = self.dataset[0]\n",
    "        self.Conv = conv\n",
    "        self.device = device\n",
    "        self.x = data.x\n",
    "        self.y = data.y.squeeze()\n",
    "        self.data=data.to(device)\n",
    "        self.dataprocess(data)\n",
    "        self.loss = loss_function\n",
    "        self.mode = mode\n",
    "        self.train_mask = torch.tensor([True]*int(0.8*len(data.x)+1) + [False]*int(0.2*len(data.x)))\n",
    "        #val_mask = torch.tensor([False]*int(0.6*len(data.x)+1) + [True]*int(0.2*len(data.x)+1)+[False]*int(0.2*len(data.x)))\n",
    "        self.test_mask = torch.tensor([False]*int(0.8*len(data.x)+1) + [True]*int(0.2*len(data.x)))\n",
    "        self.flag = self.loss[\"flag\"]\n",
    "        \n",
    "        super(Main, self).__init__()\n",
    "    def dataprocess(self,data):\n",
    "        #splitting data to train and test\n",
    "        train_edge_index = []\n",
    "        test_edge_index = []\n",
    "        indices_to_delete_for_test  = random.choices(list(range(len(data.edge_index[0]))), k = int(len(data.edge_index[0])*0.1))\n",
    "        indices_to_delete_for_val  = random.choices(list(range(len(data.edge_index[0]))), k = int(len(data.edge_index[0])*0.1))\n",
    "        \n",
    "        for i,x in enumerate(list(zip(*data.edge_index.tolist()))):\n",
    "            if i in indices_to_delete:\n",
    "                test_edge_index.append(x)\n",
    "            else:\n",
    "                train_edge_index.append(x)\n",
    "        val_edge_index = torch.tensor(np.array(list(zip(*val_edge_index))))\n",
    "        test_edge_index = torch.tensor(np.array(list(zip(*test_edge_index))))\n",
    "        train_edge_index = torch.tensor(np.array(list(zip(*train_edge_index))), dtype = torch.long)\n",
    "        #data.edge_index = train_edge_index \n",
    "        s=set()\n",
    "        for i in range(len(data.x)):\n",
    "            for j in range(len(data.x)):\n",
    "                s.add((i,j) )\n",
    "                s_of_edges = set()\n",
    "        for pair in (data.edge_index.t().tolist()):\n",
    "            s_of_edges.add(tuple(pair))\n",
    "        s_of_non_edges = s - s_of_edges\n",
    "        #append negative samples to test set\n",
    "        non_edges=[]\n",
    "        for pair in list(s_of_non_edges):\n",
    "            non_edges.append(list(pair))\n",
    "        self.non_edges_to_train=torch.tensor(random.choices(non_edges, k = len(test_edge_index[0]))).t()\n",
    "        self.y_true = [1]*len(test_edge_index[0])\n",
    "        self.test_edge_index=torch.cat((test_edge_index,self.non_edges_to_train),1)\n",
    "        self.y_true += [0]*len(self.non_edges_to_train[0])\n",
    "        self.data.edge_index = train_edge_index\n",
    "    \n",
    "    def sampling(self,Sampler,epoch,nodes):\n",
    "        if (epoch == 0): \n",
    "            if self.flag:  \n",
    "                if \"alpha\" in self.loss: \n",
    "                    name_of_file = \"samples_\"+self.loss[\"Name\"]+\"_alpha_\"+str(self.loss[\"alpha\"])+\".pickle\"\n",
    "                else:\n",
    "                    name_of_file = \"samples_\"+self.loss[\"Name\"]+\".pickle\"\n",
    "                \n",
    "                if os.path.exists(name_of_file):\n",
    "                    with open(name_of_file,'rb') as f:\n",
    "                        self.samples = pickle.load(f)\n",
    "                else:\n",
    "                    self.samples = Sampler.sample(nodes) \n",
    "                    with open(name_of_file,'wb') as f:\n",
    "                        self.samples = pickle.dump(self.samples,f)\n",
    "            else:\n",
    "                \n",
    "                self.samples = Sampler.sample(nodes)\n",
    "        return self.samples\n",
    "    def train(self, model,data,optimizer,Sampler,train_loader,dropout,epoch):\n",
    "        model.train()   \n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        if model.mode == 'unsupervised':\n",
    "            if model.conv=='GCN':\n",
    "                arr=torch.nonzero(self.train_mask == True)\n",
    "                indices_of_train_data = ([item for sublist in arr for item in sublist])\n",
    "                out = model.inference(data.to(device),dp=dropout)\n",
    "                samples = self.sampling(Sampler,epoch, indices_of_train_data)\n",
    "                loss = model.loss(out[self.train_mask], self.samples)\n",
    "                total_loss+=loss\n",
    "            else:\n",
    "                for batch_size, n_id, adjs in train_loader:\n",
    "                    if len(train_loader.sizes) == 1:\n",
    "                        adjs = [adjs]\n",
    "                    adjs = [adj.to(device) for adj in adjs]\n",
    "                    out = model.forward(data.x[n_id.to(device)].to(device), adjs)\n",
    "                    samples = self.sampling(Sampler,epoch,n_id[:batch_size])                 \n",
    "                    loss = model.loss(out, self.samples)#pos_batch.to(device), neg_batch.to(device))\n",
    "                    total_loss+=loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()      \n",
    "            return total_loss /len(train_loader)\n",
    "        elif model.mode== 'supervised':\n",
    "            if model.conv=='GCN':\n",
    "                out = model.inference(data.to(device),dp=dropout)\n",
    "                loss = model.loss_sup(out[self.train_mask],y[self.train_mask])\n",
    "                total_loss+=loss\n",
    "            else:\n",
    "                for batch_size, n_id, adjs in train_loader:\n",
    "                    adjs = [adj for adj in adjs]\n",
    "                    out = model.forward(data.x[n_id].to(device), adjs)\n",
    "                    loss = model.loss_sup(out,y[n_id[:batch_size]])\n",
    "                    total_loss+=loss\n",
    "            total_loss.backward(retain_graph=True)\n",
    "            optimizer.step()      \n",
    "            return total_loss /len(train_loader)       \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test_lp(self,model,data,classifier): \n",
    "        model.eval()\n",
    "        out = model.inference(data.to(device))\n",
    "        y_true = np.array(self.y_true)\n",
    "        if model.mode == 'supervised':\n",
    "            y_true = y.cpu().unsqueeze(-1)\n",
    "            y_pred = out.cpu().argmax(dim=-1, keepdim=True)\n",
    "            accs = []\n",
    "\n",
    "            accs=[int(y_pred.eq(y_true[mas]).sum()) / int(mask.sum())]\n",
    "            return accs\n",
    "\n",
    "        elif model.mode == 'unsupervised':\n",
    "            y_pred = []\n",
    "            for x in list(zip(*self.test_edge_index)):\n",
    "                y_pred.append(torch.sigmoid(torch.dot(out[x[0]],out[x[1]])))#print(torch.sigmoid(torch.dot(out[x[0]],out[x[1]])))\n",
    "            return roc_auc_score(y_true,np.array(y_pred)) \n",
    "            #return [precision_score(y_true[train_mask.cpu()], best_preds_train, average='macro'), precision_score(y_true[val_mask.cpu()], best_preds_val, average='macro'),precision_score(y_true[test_mask.cpu()], best_preds, average='macro')]\n",
    "\n",
    "    def run(self,**kwargs):\n",
    "        \n",
    "        hidden_layer = 64\n",
    "        out_layer = 128\n",
    "        dropout = 0.3\n",
    "        size = 1\n",
    "        learning_rate = 0.00685\n",
    "\n",
    "        classifier = \"logistic regression\"\n",
    "        train_loader = 3 #NeighborSampler(self.data.edge_index, node_idx=self.train_mask, batch_size = int(sum(self.train_mask)), sizes=[-1]*size)\n",
    "        Sampler=self.loss[\"Sampler\"]\n",
    "        LossSampler = Sampler(self.data,device=device,mask=self.train_mask,loss_info=self.loss)\n",
    "        model = Net(dataset = self.dataset,mode=self.mode,conv=self.Conv,loss_function=self.loss,device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = (size),dropout = dropout)\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)\n",
    "                #scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.01, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        scheduler=lr_scheduler.StepLR(optimizer, step_size=25,gamma=0.1)\n",
    "        losses=[]\n",
    "        train_accs=[]\n",
    "        test_accs=[]\n",
    "        val_accs=[]\n",
    "        name_of_plot='conv: '+model.conv+', mode: '+model.mode+', loss from '+self.loss[\"Name\"]\n",
    "\n",
    "        print(name_of_plot)\n",
    "\n",
    "        for epoch in range(50):\n",
    "                    print('epoch',epoch)\n",
    "                    loss = self.train(model,self.data,optimizer,LossSampler,train_loader,dropout,epoch)\n",
    "                    losses.append(loss)\n",
    "                    d_test = datetime.now()\n",
    "                    test_acc = self.test_lp(model,self.data,'logistic regression')\n",
    "                    test_accs.append(test_acc)\n",
    "                    log = 'Loss: {:.4f}, Epoch: {:03d}, Test: {:.4f}'\n",
    "                    #scheduler.step()\n",
    "                    print(log.format(loss, epoch, test_acc))\n",
    "        print('Test acc on the last epoch ', test_acc)\n",
    "        plt.plot(losses)\n",
    "        plt.title(name_of_plot+' loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "        plt.plot(test_accs)\n",
    "        plt.title(name_of_plot+' loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "                    #return [precision_score(y_true[train_mask.cpu()], best_preds_train, average='macro'), precision_score(y_true[val_mask.cpu()], best_preds_val, average='macro'),precision_score(y_true[test_mask.cpu()], best_preds, average='macro')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainOptuna(Main):\n",
    "    def objective(self,trial):\n",
    "        # Integer parameter\n",
    "        hidden_layer = trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "        out_layer = trial.suggest_categorical(\"out_layer\", [32,64,128])\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "        size = trial.suggest_categorical(\"size of network, number of convs\", [1,2,3])\n",
    "        Conv = self.Conv# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "\n",
    "        # варьируем параметры\n",
    "        loss_to_train={}\n",
    "        for name in self.loss:\n",
    "            \n",
    "            if type(self.loss[name]) == list :\n",
    "                if len(self.loss[name]) == 3:\n",
    "                    var = trial.suggest_int(name,self.loss[name][0],self.loss[name][1],step=self.loss[name][2])\n",
    "                    loss_to_train[name] = var\n",
    "                elif len(self.loss[name]) == 2:\n",
    "                    var_2 = trial.suggest_float(name,self.loss[name][0],self.loss[name][1])\n",
    "                    loss_to_train[name] = var_2\n",
    "                else:\n",
    "                    var_3 = trial.suggest_categorical(name, self.loss[name])\n",
    "                    loss_to_train[name] = var_3\n",
    "            else:\n",
    "                loss_to_train[name] = self.loss[name]\n",
    "        Sampler =loss_to_train[\"Sampler\"]\n",
    "        model = Net(dataset = self.dataset,mode='unsupervised',conv=Conv,loss_function=loss_to_train,device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = size,dropout = dropout)\n",
    "\n",
    "        train_loader = NeighborSampler(self.data.edge_index, batch_size =int(sum(self.train_mask)),node_idx=self.train_mask, sizes=[-1]*size)\n",
    "        \n",
    "        #train_loader = GraphSAINTRandomWalkSampler(data, batch_size=2176, walk_length=2,num_steps=5, sample_coverage=100,save_dir=dataset.processed_dir,num_workers=4)\n",
    "\n",
    "        LossSampler = Sampler(self.data,device=self.device,mask=self.train_mask,loss_info=loss_to_train)\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        learning_rate= trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "\n",
    "        classifier = \"logistic regression\" #trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "\n",
    "        if classifier == \"catboost\":\n",
    "            n_estimators = trial.suggest_int(\"n of estimators\", 10,40,5)\n",
    "            learning_rate_catboost = trial.suggest_float(\"lr_catboost\",5e-4,1e-2)\n",
    "            max_depth = trial.suggest_int(\"max_depth\",1,10,2)\n",
    "        else:\n",
    "            n_estimators = -1\n",
    "            learning_rate_catboost =-1\n",
    "            max_depth = -1\n",
    "        #training of the model\n",
    "        for epoch in range(50):\n",
    "            loss = self.train(model,self.data,optimizer,LossSampler,train_loader,dropout,epoch)\n",
    "            #print(loss)\n",
    "\n",
    "        test_acc = self.test_lp(model,self.data,classifier)\n",
    "\n",
    "\n",
    "        trial.report(test_acc,epoch)\n",
    "\n",
    "        return test_acc\n",
    "\n",
    "    \n",
    "    def run(self,number_of_trials):\n",
    "\n",
    "        study = optuna.create_study(direction=\"maximize\",study_name=self.loss[\"Name\"]+\" loss,\"+str(self.Conv)+\" conv\")\n",
    "        study.optimize(self.objective,n_trials = number_of_trials)\n",
    "\n",
    "        print('Best trial:')\n",
    "        trial = study.best_trial\n",
    "        print(\" Value: \", trial.value)\n",
    "        print(\" Params: \")\n",
    "        for key, value in trial.params.items():\n",
    "            print(\" {}: {}\".format(key,value))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #SAGE = {\"Name\":\"SAGE\" , \"walk_length\":[5,20,5],\"walks_per_node\":[5,20,5],\"num_negative_samples\":20,\"context size\" : [5,20,5],\"p\":1,\"q\":1, \"loss var\": \"Random Walks\",\"flag\":False,\"Sampler\" =SamplerRandomWalk }\n",
    "from modules.sampling import Sampler, SamplerContextMatrix, SamplerRandomWalk,SamplerFactorization,SamplerAPP\n",
    "DeepWalk = {\"Name\": \"DeepWalk\",\"walk_length\":[5,20,5],\"walks_per_node\":[5,20,5],\"num_negative_samples\":[1,21,5],\"context_size\" : [5,20,5],\"p\":1,\"q\":1,\"loss var\": \"Random Walks\",\"flag_tosave\":False,\"Sampler\" : SamplerRandomWalk } #Проблемы с памятью после того, как увеличила количество тренировочных данных\n",
    "Node2Vec = {\"Name\": \"Node2Vec\",\"walk_length\":[5,20,5],\"walks_per_node\":[5,20,5],\"num_negative_samples\":[1,21,5],\"context_size\" : [5,20,5],\"p\": [0.0,0.9] ,\"q\":[0.0,0.9], \"loss var\": \"Random Walks\",\"flag_tosave\":False,\"Sampler\": SamplerRandomWalk}#то же самое \n",
    "\n",
    "LINE = {\"Name\": \"LINE\",\"C\": \"Adj\",\"num_negative_samples\":[1,21,5],\"loss var\": \"Context Matrix\",\"flag_tosave\":True,\"Sampler\" :SamplerContextMatrix} \n",
    "HOPE_RPR = {\"Name\": \"HOPE_RPR\",\"C\":\"RPR\",\"loss var\": \"Factorization\",\"flag_tosave\":True,\"alpha\": [0,1],\"Sampler\" :SamplerFactorization} #проверить\n",
    "HOPE_Katz = {\"Name\": \"HOPE_Katz\",\"C\":\"Katz\",\"loss var\": \"Factorization\",\"flag_tosave\":True,\"betta\": [0,1],\"Sampler\" :SamplerFactorization,} #проверить\n",
    "HOPE_CN = {\"Name\": \"HOPE_CommonNeighbors\",\"C\":\"CN\",\"loss var\": \"Factorization\",\"flag_tosave\":True,\"Sampler\" :SamplerFactorization} \n",
    "HOPE_AA = {\"Name\": \"HOPE_AdamicAdar\",\"C\":\"AA\",\"loss var\": \"Factorization\",\"flag_tosave\":True,\"Sampler\" :SamplerFactorization} \n",
    "\n",
    "LapEigen = {\"Name\": \"LaplacianEigenMaps\", \"C\":\"Adj\",\"loss var\": \"Laplacian EigenMaps\",\"flag_tosave\":True,\"Sampler\" :SamplerFactorization}\n",
    "GraphFactorization = {\"Name\": \"Graph Factorization\",\"C\":\"Adj\",\"loss var\": \"Factorization\",\"flag_tosave\":True,\"Sampler\" :SamplerFactorization} \n",
    "\n",
    "VERSE_PPR =  {\"Name\": \"VERSE_PPR\",\"C\": \"PPR\",\"num_negative_samples\":[1,21,5],\"loss var\": \"Context Matrix\",\"flag_tosave\":True,\"alpha\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\"Sampler\" :SamplerContextMatrix}\n",
    "VERSE_SR =  {\"Name\": \"VERSE_SimRank\",\"C\": \"SR\",\"num_negative_samples\":[1,21,5],\"loss var\": \"Context Matrix\",\"flag_tosave\":True,\"Sampler\":SamplerContextMatrix} \n",
    "VERSE_Adj =  {\"Name\": \"VERSE_Adj\",\"C\": \"Adj\",\"num_negative_samples\":[1,21,5],\"loss var\": \"Context Matrix\",\"flag_tosave\":True,\"Sampler\" :SamplerContextMatrix} \n",
    "\n",
    "APP ={\"Name\": \"APP\",\"C\": \"PPR\",\"num_negative_samples\":[1,21,5],\"loss var\": \"Context Matrix\",\"flag_tosave\":False,\"alpha\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\"Sampler\" :SamplerAPP}\n",
    "\n",
    "Struc2Vec ={} #Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv: GCN, mode: unsupervised, loss from APP\n",
      "epoch 0\n",
      "Loss: 5.4423, Epoch: 000, Test: 0.8299\n",
      "epoch 1\n",
      "Loss: 2.6299, Epoch: 001, Test: 0.7851\n",
      "epoch 2\n",
      "Loss: 1.6503, Epoch: 002, Test: 0.7719\n",
      "epoch 3\n",
      "Loss: 1.4792, Epoch: 003, Test: 0.7663\n",
      "epoch 4\n",
      "Loss: 1.4194, Epoch: 004, Test: 0.7635\n",
      "epoch 5\n",
      "Loss: 1.3948, Epoch: 005, Test: 0.7618\n",
      "epoch 6\n",
      "Loss: 1.3905, Epoch: 006, Test: 0.7607\n",
      "epoch 7\n",
      "Loss: 1.3994, Epoch: 007, Test: 0.7601\n",
      "epoch 8\n",
      "Loss: 1.4168, Epoch: 008, Test: 0.7596\n",
      "epoch 9\n",
      "Loss: 1.4390, Epoch: 009, Test: 0.7593\n",
      "epoch 10\n",
      "Loss: 1.4631, Epoch: 010, Test: 0.7592\n",
      "epoch 11\n",
      "Loss: 1.4868, Epoch: 011, Test: 0.7591\n",
      "epoch 12\n",
      "Loss: 1.5084, Epoch: 012, Test: 0.7592\n",
      "epoch 13\n",
      "Loss: 1.5266, Epoch: 013, Test: 0.7594\n",
      "epoch 14\n",
      "Loss: 1.5407, Epoch: 014, Test: 0.7596\n",
      "epoch 15\n",
      "Loss: 1.5502, Epoch: 015, Test: 0.7600\n",
      "epoch 16\n",
      "Loss: 1.5551, Epoch: 016, Test: 0.7603\n",
      "epoch 17\n",
      "Loss: 1.5556, Epoch: 017, Test: 0.7606\n",
      "epoch 18\n",
      "Loss: 1.5519, Epoch: 018, Test: 0.7610\n",
      "epoch 19\n",
      "Loss: 1.5448, Epoch: 019, Test: 0.7614\n",
      "epoch 20\n",
      "Loss: 1.5347, Epoch: 020, Test: 0.7618\n",
      "epoch 21\n",
      "Loss: 1.5225, Epoch: 021, Test: 0.7623\n",
      "epoch 22\n",
      "Loss: 1.5087, Epoch: 022, Test: 0.7629\n",
      "epoch 23\n",
      "Loss: 1.4941, Epoch: 023, Test: 0.7634\n",
      "epoch 24\n",
      "Loss: 1.4793, Epoch: 024, Test: 0.7642\n",
      "epoch 25\n",
      "Loss: 1.4647, Epoch: 025, Test: 0.7649\n",
      "epoch 26\n",
      "Loss: 1.4508, Epoch: 026, Test: 0.7656\n",
      "epoch 27\n",
      "Loss: 1.4379, Epoch: 027, Test: 0.7663\n",
      "epoch 28\n",
      "Loss: 1.4264, Epoch: 028, Test: 0.7671\n",
      "epoch 29\n",
      "Loss: 1.4162, Epoch: 029, Test: 0.7678\n",
      "epoch 30\n",
      "Loss: 1.4075, Epoch: 030, Test: 0.7685\n",
      "epoch 31\n",
      "Loss: 1.4002, Epoch: 031, Test: 0.7692\n",
      "epoch 32\n",
      "Loss: 1.3944, Epoch: 032, Test: 0.7701\n",
      "epoch 33\n",
      "Loss: 1.3899, Epoch: 033, Test: 0.7709\n",
      "epoch 34\n",
      "Loss: 1.3866, Epoch: 034, Test: 0.7717\n",
      "epoch 35\n",
      "Loss: 1.3843, Epoch: 035, Test: 0.7725\n",
      "epoch 36\n",
      "Loss: 1.3828, Epoch: 036, Test: 0.7733\n",
      "epoch 37\n",
      "Loss: 1.3821, Epoch: 037, Test: 0.7740\n",
      "epoch 38\n",
      "Loss: 1.3819, Epoch: 038, Test: 0.7747\n",
      "epoch 39\n",
      "Loss: 1.3821, Epoch: 039, Test: 0.7753\n",
      "epoch 40\n",
      "Loss: 1.3825, Epoch: 040, Test: 0.7758\n",
      "epoch 41\n",
      "Loss: 1.3832, Epoch: 041, Test: 0.7763\n",
      "epoch 42\n",
      "Loss: 1.3838, Epoch: 042, Test: 0.7767\n",
      "epoch 43\n",
      "Loss: 1.3845, Epoch: 043, Test: 0.7771\n",
      "epoch 44\n",
      "Loss: 1.3851, Epoch: 044, Test: 0.7774\n",
      "epoch 45\n",
      "Loss: 1.3855, Epoch: 045, Test: 0.7776\n",
      "epoch 46\n",
      "Loss: 1.3858, Epoch: 046, Test: 0.7777\n",
      "epoch 47\n",
      "Loss: 1.3859, Epoch: 047, Test: 0.7777\n",
      "epoch 48\n",
      "Loss: 1.3858, Epoch: 048, Test: 0.7776\n",
      "epoch 49\n",
      "Loss: 1.3856, Epoch: 049, Test: 0.7774\n",
      "Test acc on the last epoch  0.7773975891685848\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "device =  torch.device('cpu')# if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "APP ={\"Name\": \"APP\",\"C\": \"PPR\",\"num_negative_samples\":6,\"loss var\": \"Context Matrix\",\"flag\":True,\"alpha\":0.8 ,\"Sampler\" :SamplerAPP}\n",
    "loss = APP\n",
    "MO = Main('GCN', device, loss , mode = 'unsupervised')\n",
    "MO.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-23 17:53:24,039]\u001b[0m A new study created in memory with name: APP loss,GCN conv\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 17:53:39,025]\u001b[0m Trial 0 finished with value: 0.7879110597590343 and parameters: {'hidden_layer': 64, 'out_layer': 128, 'dropout': 0.30000000000000004, 'size of network, number of convs': 1, 'num_negative_samples': 6, 'alpha': 0.8, 'lr': 0.006850436452683323}. Best is trial 0 with value: 0.7879110597590343.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 17:53:52,400]\u001b[0m Trial 1 finished with value: 0.7545341549665628 and parameters: {'hidden_layer': 64, 'out_layer': 32, 'dropout': 0.0, 'size of network, number of convs': 3, 'num_negative_samples': 11, 'alpha': 0.7, 'lr': 0.0031263523414701783}. Best is trial 0 with value: 0.7879110597590343.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 17:55:05,999]\u001b[0m Trial 2 finished with value: 0.7459414352905478 and parameters: {'hidden_layer': 256, 'out_layer': 128, 'dropout': 0.5, 'size of network, number of convs': 3, 'num_negative_samples': 21, 'alpha': 0.1, 'lr': 0.0010838845296519978}. Best is trial 0 with value: 0.7879110597590343.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 17:55:41,125]\u001b[0m Trial 3 finished with value: 0.7546425973258016 and parameters: {'hidden_layer': 256, 'out_layer': 32, 'dropout': 0.5, 'size of network, number of convs': 1, 'num_negative_samples': 16, 'alpha': 0.2, 'lr': 0.006766302762949946}. Best is trial 0 with value: 0.7879110597590343.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 17:56:08,818]\u001b[0m Trial 4 finished with value: 0.74755591346397 and parameters: {'hidden_layer': 128, 'out_layer': 64, 'dropout': 0.30000000000000004, 'size of network, number of convs': 1, 'num_negative_samples': 6, 'alpha': 0.3, 'lr': 0.008927368625681551}. Best is trial 0 with value: 0.7879110597590343.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 17:56:25,929]\u001b[0m Trial 5 finished with value: 0.7596717940937332 and parameters: {'hidden_layer': 256, 'out_layer': 64, 'dropout': 0.0, 'size of network, number of convs': 2, 'num_negative_samples': 16, 'alpha': 0.9, 'lr': 0.0014810577135775404}. Best is trial 0 with value: 0.7879110597590343.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 17:56:35,592]\u001b[0m Trial 6 finished with value: 0.7643814992472252 and parameters: {'hidden_layer': 64, 'out_layer': 32, 'dropout': 0.30000000000000004, 'size of network, number of convs': 1, 'num_negative_samples': 1, 'alpha': 0.7, 'lr': 0.0014012174585788467}. Best is trial 0 with value: 0.7879110597590343.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 17:56:51,300]\u001b[0m Trial 7 finished with value: 0.7526711833152434 and parameters: {'hidden_layer': 32, 'out_layer': 32, 'dropout': 0.0, 'size of network, number of convs': 3, 'num_negative_samples': 11, 'alpha': 0.6, 'lr': 0.003927398353323142}. Best is trial 0 with value: 0.7879110597590343.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 17:57:23,027]\u001b[0m Trial 8 finished with value: 0.7788777626055734 and parameters: {'hidden_layer': 128, 'out_layer': 128, 'dropout': 0.30000000000000004, 'size of network, number of convs': 1, 'num_negative_samples': 21, 'alpha': 0.7, 'lr': 0.006222438375308273}. Best is trial 0 with value: 0.7879110597590343.\u001b[0m\n",
      "\u001b[32m[I 2021-04-23 17:57:36,216]\u001b[0m Trial 9 finished with value: 0.7496985010639995 and parameters: {'hidden_layer': 128, 'out_layer': 32, 'dropout': 0.30000000000000004, 'size of network, number of convs': 3, 'num_negative_samples': 11, 'alpha': 0.8, 'lr': 0.0008691231161913253}. Best is trial 0 with value: 0.7879110597590343.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      " Value:  0.7879110597590343\n",
      " Params: \n",
      " hidden_layer: 64\n",
      " out_layer: 128\n",
      " dropout: 0.30000000000000004\n",
      " size of network, number of convs: 1\n",
      " num_negative_samples: 6\n",
      " alpha: 0.8\n",
      " lr: 0.006850436452683323\n"
     ]
    }
   ],
   "source": [
    "device =  torch.device('cpu')# if torch.cuda.is_available() else 'cpu')\n",
    "loss = APP\n",
    "MO = MainOptuna('GCN', device, loss , mode = 'unsupervised')\n",
    "MO.run(number_of_trials=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, roc_auc_score\n",
    "\n",
    "def train_lp(model,data,optimizer,Sampler,train_loader,dropout,epoch):\n",
    "    model.train()        \n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    if model.mode == 'unsupervised':\n",
    "        if model.conv=='GCN':\n",
    "            out = model.inference(data.to(device),dp=dropout)\n",
    "            if epoch == 0:\n",
    "                samples = Sampler.sample(torch.tensor(list(range(len(data.x)))))\n",
    "            loss = model.loss(out, samples)\n",
    "            total_loss+=loss\n",
    "        else:\n",
    "            for batch_size, n_id, adjs in train_loader:\n",
    "                # adjs holds a list of (edge_index, e_id, size) tuples.\n",
    "                if len(train_loader.sizes) == 1:\n",
    "                        adjs = [adjs]\n",
    "                adjs = [adj.to(device) for adj in adjs]\n",
    "                out = model.forward(data.x[n_id].to(device), adjs)\n",
    "                if epoch == 0:\n",
    "                    samples = Sampler.sample((n_id.numpy().tolist())[:batch_size])\n",
    "                loss = model.loss(out, samples)#pos_batch.to(device), neg_batch.to(device))\n",
    "                #print(out.shape, samples[0].shape,samples[1].shape)\n",
    "                total_loss+=loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()      \n",
    "        return total_loss# /len(train_loader)\n",
    "    elif model.mode== 'supervised':\n",
    "        if model.conv=='GCN':\n",
    "            out = model.inference(data.to(device),dp=dropout)\n",
    "            loss = model.loss_sup(out[train_mask],y[train_mask])\n",
    "            total_loss+=loss \n",
    "        else:\n",
    "            for batch_size, n_id, adjs in train_loader:\n",
    "                if len(train_loader.sizes) == 1:\n",
    "                        adjs = [adjs]    \n",
    "                adjs = [adj.to(device) for adj in adjs]\n",
    "                out = model.forward(data.x[n_id].to(device), adjs)\n",
    "                loss = model.loss_sup(out,y[n_id[:batch_size]])\n",
    "                total_loss+=loss\n",
    "        total_loss.backward(retain_graph=True)\n",
    "        optimizer.step()      \n",
    "        return total_loss# /len(train_loader)       \n",
    "\n",
    "@torch.no_grad()\n",
    "def test_lp(model,data,classifier,y_true): \n",
    "    model.eval()\n",
    "    out = model.inference(data.to(device))\n",
    "    y_true = np.array(y_true)\n",
    "    if model.mode == 'supervised':\n",
    "        y_true = y.cpu().unsqueeze(-1)\n",
    "        y_pred = out.cpu().argmax(dim=-1, keepdim=True)\n",
    "        accs = []\n",
    "        \n",
    "        accs=[int(y_pred.eq(y_true[mas]).sum()) / int(mask.sum())]\n",
    "        return accs\n",
    "    \n",
    "    elif model.mode == 'unsupervised':\n",
    "        y_pred = []\n",
    "        for x in list(zip(*test_edge_index)):\n",
    "            y_pred.append(torch.sigmoid(torch.dot(out[x[0]],out[x[1]])))#print(torch.sigmoid(torch.dot(out[x[0]],out[x[1]])))\n",
    "\n",
    "        return roc_auc_score(y_true,np.array(y_pred)) \n",
    "        #return [precision_score(y_true[train_mask.cpu()], best_preds_train, average='macro'), precision_score(y_true[val_mask.cpu()], best_preds_val, average='macro'),precision_score(y_true[test_mask.cpu()], best_preds, average='macro')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv: GAT, mode: unsupervised, loss from APP\n",
      "epoch 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'int' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-261bd29fc899>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_lp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLossSampler\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m                     \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m                     \u001b[0md_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-d23594e9f7fe>\u001b[0m in \u001b[0;36mtrain_lp\u001b[1;34m(model, data, optimizer, Sampler, train_loader, dropout, epoch)\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madjs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                     \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSampler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_id\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#pos_batch.to(device), neg_batch.to(device))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[1;31m#print(out.shape, samples[0].shape,samples[1].shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MasterDegree\\Code\\modules\\sampling.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, batch, **kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneg_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpos_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[0md_pb\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MasterDegree\\Code\\modules\\sampling.py\u001b[0m in \u001b[0;36mpos_sample\u001b[1;34m(self, batch, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[0mASparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m         \u001b[0mpos_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_PPR_approx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mASparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m         \u001b[0mpos_batch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mpos_pair\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpos_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MasterDegree\\Code\\modules\\sampling.py\u001b[0m in \u001b[0;36mfind_PPR_approx\u001b[1;34m(self, batch, Adj, device, r, alpha)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_PPR_approx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mAdj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m         \u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m         \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m        \u001b[1;31m# length.append(r-sum(length))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'int' and 'list'"
     ]
    }
   ],
   "source": [
    "#models = [Net(dataset = dataset,mode='supervised',conv='GCN',device=device), Net(dataset = dataset,mode='supervised',conv='GAT',device=device),Net(dataset = dataset,mode='supervised',conv='SGC',device=device), Net(dataset = dataset,mode='unsupervised',conv='GCN',device=device),Net(dataset = dataset,mode='unsupervised',conv='GAT',device=device),Net(dataset = dataset,mode='unsupervised',conv='SAGE',device=device)]\n",
    "\n",
    "sizes = [-1]\n",
    "\n",
    "hidden_layer = 64\n",
    "out_layer = 128\n",
    "dropout = 0.5\n",
    "size = [10,25]\n",
    "learning_rate = 0.003\n",
    "\n",
    "classifier = \"logistic regression\"\n",
    "loss=APP\n",
    "from datetime import datetime\n",
    "d = datetime.now()\n",
    "train_mask = torch.tensor([True]*len(data.x))\n",
    "if True:\n",
    "        hidden_layer = 64\n",
    "        out_layer = 128\n",
    "        dropout = 0.4\n",
    "        size = 1\n",
    "        learning_rate = 0.001\n",
    "        Conv = \"GAT\"\n",
    "        loss = APP\n",
    "        device = torch.device('cuda')\n",
    "        classifier = \"logistic regression\"\n",
    "        train_loader = NeighborSampler(data.edge_index, node_idx = train_mask, batch_size = len(data.x), sizes=[-1]*size)\n",
    "        Sampler = loss[\"Sampler\"]\n",
    "        LossSampler = Sampler(data,device=device,mask=train_mask,loss_info=loss)\n",
    "        model = Net(dataset = dataset,mode='unsupervised',conv=Conv,loss_function=loss,device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = (size),dropout = dropout)\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)\n",
    "                #scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.01, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        scheduler=lr_scheduler.StepLR(optimizer, step_size=25,gamma=0.1)\n",
    "        losses=[]\n",
    "        train_accs=[]\n",
    "        test_accs=[]\n",
    "        val_accs=[]\n",
    "        name_of_plot='conv: '+model.conv+', mode: '+model.mode+', loss from '+loss[\"Name\"]\n",
    "\n",
    "        print(name_of_plot)\n",
    "\n",
    "        for epoch in range(50):\n",
    "                    print('epoch',epoch)\n",
    "                    loss = train_lp(model,data,optimizer,LossSampler,train_loader,dropout,epoch)\n",
    "                    losses.append(loss)\n",
    "                    d_test = datetime.now()\n",
    "                    test_acc = test_lp(model,data,'logistic regression',y_true)\n",
    "                   # print('тестирование заняло: ', datetime.now()-d_test)\n",
    "                    test_accs.append(test_acc)\n",
    "                   # val_accs.append(val_acc)\n",
    "                    log = 'Loss: {:.4f}, Epoch: {:03d}, Test: {:.4f}'\n",
    "                    #scheduler.step()\n",
    "                    print(log.format(loss, epoch, test_acc))\n",
    "        print('Test acc on the last epoch ', test_acc)\n",
    "        plt.plot(losses)\n",
    "        plt.title(name_of_plot+' loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "        plt.plot(test_accs)\n",
    "        plt.title(name_of_plot+' loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = test_lp(model,data,'logistic regression',y_true)\n",
    "print('Test acc on the last epoch ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "def objective(trial):\n",
    "    # Integer parameter\n",
    "    hidden_layer = 32#trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "    out_layer = trial.suggest_int(\"out_layer\", 32,128,step = 32,log = False)\n",
    "    # Floating point parameter\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "   # size_str = trial.suggest_categorical(\"size\", [\"[10]\",\"[25]\",\"[5,10]\",\"[10,25]\",\"[5,10,25]\"])\n",
    "   # b = size_str.split(\"[\")[1].split(\"]\")[0].split(',')\n",
    "    size = [5]#list(map(lambda x: int(x),b))\n",
    "\n",
    "    Conv =\"GAT\"# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "    \n",
    "    walk_length = trial.suggest_int(\"walk length\",5,20,step = 5)\n",
    "    walks_per_node = trial.suggest_int(\"walks per node\", 5,20,step = 5)\n",
    "    context_size = trial.suggest_int(\"context size\",5,20,step = 5)\n",
    "    num_negative_samples = walks_per_node\n",
    "    loss = {\"Name\":\"SAGE\" , \"walk length\":walk_length,\"walks per node\":walks_per_node,\"num negative samples\":num_negative_samples,\"context size\" : context_size,\"p\":1,\"q\":1, \"loss var\": \"Random Walks\"}\n",
    "\n",
    "    model = Net(dataset = dataset,mode='unsupervised',conv=Conv,loss_function=loss[\"loss var\"],device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = len(size),dropout = dropout)\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_mask, batch_size = 677, sizes=size)\n",
    "    LossSampler = Sampler(data,device=device,mask=train_mask,loss_info=loss)\n",
    "    \n",
    "    model.to(device)\n",
    "    #Generate the optimizers\n",
    "#optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "    learning_rate=trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "    #optimizer = getattr(optim,optimizer_name)(model.parameters(),lr = learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "    #classifier = trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "    #classifier = \"logistic regression\"\n",
    "    classifier = \"catboost\"\n",
    "    #training of the model\n",
    "    for epoch in range(50):\n",
    "        loss = train(model,data,optimizer,LossSampler,train_loader,dropout)\n",
    "        train_acc, test_acc = test(model,data,classifier)\n",
    "        \n",
    "    trial.report(test_acc,epoch)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialsPruned()\n",
    "    return test_acc\n",
    "        \n",
    "    \n",
    "study = optuna.create_study(direction=\"maximize\",study_name=\"SAGE loss\")\n",
    "study.optimize(objective,n_trials = 3)\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\" Number of finid=shed trials: \",len(study.trials))\n",
    "print(\" Number of pruned trials: \", len(pruned_trials))\n",
    "print(\" Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "def objective(trial):\n",
    "    # Integer parameter\n",
    "    hidden_layer = 32#trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "    out_layer = trial.suggest_int(\"out_layer\", 32,128,step = 32,log = False)\n",
    "    # Floating point parameter\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "   # size_str = trial.suggest_categorical(\"size\", [\"[10]\",\"[25]\",\"[5,10]\",\"[10,25]\",\"[5,10,25]\"])\n",
    "   # b = size_str.split(\"[\")[1].split(\"]\")[0].split(',')\n",
    "    size = [5]#list(map(lambda x: int(x),b))\n",
    "\n",
    "    Conv =\"GAT\"# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "    \n",
    "    walk_length = trial.suggest_int(\"walk length\",5,20,step = 5)\n",
    "    walks_per_node = trial.suggest_int(\"walks per node\", 5,20,step = 5)\n",
    "    context_size = trial.suggest_int(\"context size\",5,20,step = 5)\n",
    "    num_negative_samples = walks_per_node\n",
    "    loss = {\"Name\":\"SAGE\" , \"walk length\":walk_length,\"walks per node\":walks_per_node,\"num negative samples\":num_negative_samples,\"context size\" : context_size,\"p\":1,\"q\":1, \"loss var\": \"Random Walks\"}\n",
    "\n",
    "    model = Net(dataset = dataset,mode='unsupervised',conv=Conv,loss_function=loss[\"loss var\"],device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = len(size),dropout = dropout)\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_mask, batch_size = 677, sizes=size)\n",
    "    LossSampler = Sampler(data,device=device,mask=train_mask,loss_info=loss)\n",
    "    \n",
    "    model.to(device)\n",
    "    #Generate the optimizers\n",
    "#optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "    learning_rate=trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "    #optimizer = getattr(optim,optimizer_name)(model.parameters(),lr = learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "    #classifier = trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "    classifier = \"logistic regression\"\n",
    "    #classifier = \"catboost\"\n",
    "    #training of the model\n",
    "    for epoch in range(50):\n",
    "        loss = train(model,data,optimizer,LossSampler,train_loader,dropout)\n",
    "        train_acc, test_acc = test(model,data,classifier)\n",
    "        \n",
    "    trial.report(test_acc,epoch)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialsPruned()\n",
    "    return test_acc\n",
    "        \n",
    "    \n",
    "study = optuna.create_study(direction=\"maximize\",study_name=\"SAGE loss\")\n",
    "study.optimize(objective,n_trials = 3)\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\" Number of finid=shed trials: \",len(study.trials))\n",
    "print(\" Number of pruned trials: \", len(pruned_trials))\n",
    "print(\" Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "def objective(trial):\n",
    "    # Integer parameter\n",
    "    hidden_layer =32# trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "    out_layer = trial.suggest_int(\"out_layer\", 32,128,step = 32,log = False)\n",
    "    # Floating point parameter\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "   # size_str = trial.suggest_categorical(\"size\", [\"[10]\",\"[25]\",\"[5,10]\",\"[10,25]\",\"[5,10,25]\"])\n",
    "   # b = size_str.split(\"[\")[1].split(\"]\")[0].split(',')\n",
    "    size = [10]#list(map(lambda x: int(x),b))\n",
    "\n",
    "    Conv =\"GAT\"# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "    \n",
    "    walk_length = trial.suggest_int(\"walk length\",5,20,step = 5)\n",
    "    walks_per_node = trial.suggest_int(\"walks per node\", 5,20,step = 5)\n",
    "    context_size = trial.suggest_int(\"context size\",5,20,step = 5)\n",
    "    num_negative_samples = walks_per_node\n",
    "    loss = {\"Name\":\"SAGE\" , \"walk length\":walk_length,\"walks per node\":walks_per_node,\"num negative samples\":num_negative_samples,\"context size\" : context_size,\"p\":1,\"q\":1, \"loss var\": \"Random Walks\"}\n",
    "\n",
    "    model = Net(dataset = dataset,mode='unsupervised',conv=Conv,loss_function=loss[\"loss var\"],device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = len(size),dropout = dropout)\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_mask, batch_size = 677, sizes=size)\n",
    "    LossSampler = Sampler(data,device=device,mask=train_mask,loss_info=loss)\n",
    "    \n",
    "    model.to(device)\n",
    "    #Generate the optimizers\n",
    "#optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "    learning_rate=trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "    #optimizer = getattr(optim,optimizer_name)(model.parameters(),lr = learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "    #classifier = trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "    #classifier = \"logistic regression\"\n",
    "    classifier = \"catboost\"\n",
    "    #training of the model\n",
    "    for epoch in range(50):\n",
    "        loss = train(model,data,optimizer,LossSampler,train_loader,dropout)\n",
    "        train_acc, test_acc = test(model,data,classifier)\n",
    "        \n",
    "    trial.report(test_acc,epoch)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialsPruned()\n",
    "    return test_acc\n",
    "        \n",
    "    \n",
    "study = optuna.create_study(direction=\"maximize\",study_name=\"SAGE loss\")\n",
    "study.optimize(objective,n_trials = 3)\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\" Number of finid=shed trials: \",len(study.trials))\n",
    "print(\" Number of pruned trials: \", len(pruned_trials))\n",
    "print(\" Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "def objective(trial):\n",
    "    # Integer parameter\n",
    "    hidden_layer =32# trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "    out_layer = trial.suggest_int(\"out_layer\", 32,128,step = 32,log = False)\n",
    "    # Floating point parameter\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "   # size_str = trial.suggest_categorical(\"size\", [\"[10]\",\"[25]\",\"[5,10]\",\"[10,25]\",\"[5,10,25]\"])\n",
    "   # b = size_str.split(\"[\")[1].split(\"]\")[0].split(',')\n",
    "    size = [10]#list(map(lambda x: int(x),b))\n",
    "\n",
    "    Conv =\"GAT\"# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "    \n",
    "    walk_length = trial.suggest_int(\"walk length\",5,20,step = 5)\n",
    "    walks_per_node = trial.suggest_int(\"walks per node\", 5,10,step = 5)\n",
    "    context_size = trial.suggest_int(\"context size\",5,20,step = 5)\n",
    "    num_negative_samples = walks_per_node\n",
    "    loss = {\"Name\":\"SAGE\" , \"walk length\":walk_length,\"walks per node\":walks_per_node,\"num negative samples\":num_negative_samples,\"context size\" : context_size,\"p\":1,\"q\":1, \"loss var\": \"Random Walks\"}\n",
    "\n",
    "    model = Net(dataset = dataset,mode='unsupervised',conv=Conv,loss_function=loss[\"loss var\"],device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = len(size),dropout = dropout)\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_mask, batch_size = 677, sizes=size)\n",
    "    LossSampler = Sampler(data,device=device,mask=train_mask,loss_info=loss)\n",
    "    \n",
    "    model.to(device)\n",
    "    #Generate the optimizers\n",
    "#optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "    learning_rate=trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "    #optimizer = getattr(optim,optimizer_name)(model.parameters(),lr = learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "    #classifier = trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "    classifier = \"logistic regression\"\n",
    "    #classifier = \"catboost\"\n",
    "    #training of the model\n",
    "    for epoch in range(50):\n",
    "        loss = train(model,data,optimizer,LossSampler,train_loader,dropout)\n",
    "        train_acc, test_acc = test(model,data,classifier)\n",
    "        \n",
    "    trial.report(test_acc,epoch)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialsPruned()\n",
    "    return test_acc\n",
    "        \n",
    "    \n",
    "study = optuna.create_study(direction=\"maximize\",study_name=\"SAGE loss\")\n",
    "study.optimize(objective,n_trials = 3)\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\" Number of finid=shed trials: \",len(study.trials))\n",
    "print(\" Number of pruned trials: \", len(pruned_trials))\n",
    "print(\" Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "def objective(trial):\n",
    "    # Integer parameter\n",
    "    hidden_layer = 32#trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "    out_layer = trial.suggest_int(\"out_layer\", 32,128,step = 32,log = False)\n",
    "    # Floating point parameter\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "   # size_str = trial.suggest_categorical(\"size\", [\"[10]\",\"[25]\",\"[5,10]\",\"[10,25]\",\"[5,10,25]\"])\n",
    "   # b = size_str.split(\"[\")[1].split(\"]\")[0].split(',')\n",
    "    size = [25]#list(map(lambda x: int(x),b))\n",
    "\n",
    "    Conv =\"GAT\"# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "    \n",
    "    walk_length = trial.suggest_int(\"walk length\",5,20,step = 5)\n",
    "    walks_per_node = trial.suggest_int(\"walks per node\", 5,10,step = 5)\n",
    "    context_size = trial.suggest_int(\"context size\",5,20,step = 5)\n",
    "    num_negative_samples = walks_per_node\n",
    "    loss = {\"Name\":\"SAGE\" , \"walk length\":walk_length,\"walks per node\":walks_per_node,\"num negative samples\":num_negative_samples,\"context size\" : context_size,\"p\":1,\"q\":1, \"loss var\": \"Random Walks\"}\n",
    "\n",
    "    model = Net(dataset = dataset,mode='unsupervised',conv=Conv,loss_function=loss[\"loss var\"],device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = len(size),dropout = dropout)\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_mask, batch_size = 677, sizes=size)\n",
    "    LossSampler = Sampler(data,device=device,mask=train_mask,loss_info=loss)\n",
    "    \n",
    "    model.to(device)\n",
    "    #Generate the optimizers\n",
    "#optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "    learning_rate=trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "    #optimizer = getattr(optim,optimizer_name)(model.parameters(),lr = learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "    #classifier = trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "    #classifier = \"logistic regression\"\n",
    "    classifier = \"catboost\"\n",
    "    #training of the model\n",
    "    for epoch in range(50):\n",
    "        loss = train(model,data,optimizer,LossSampler,train_loader,dropout)\n",
    "        train_acc, test_acc = test(model,data,classifier)\n",
    "        \n",
    "    trial.report(test_acc,epoch)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialsPruned()\n",
    "    return test_acc\n",
    "        \n",
    "    \n",
    "study = optuna.create_study(direction=\"maximize\",study_name=\"SAGE loss\")\n",
    "study.optimize(objective,n_trials = 3)\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\" Number of finid=shed trials: \",len(study.trials))\n",
    "print(\" Number of pruned trials: \", len(pruned_trials))\n",
    "print(\" Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset =Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "def objective(trial):\n",
    "    # Integer parameter\n",
    "    hidden_layer = 32#trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "    out_layer = trial.suggest_int(\"out_layer\", 32,128,step = 32,log = False)\n",
    "    # Floating point parameter\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "   # size_str = trial.suggest_categorical(\"size\", [\"[10]\",\"[25]\",\"[5,10]\",\"[10,25]\",\"[5,10,25]\"])\n",
    "   # b = size_str.split(\"[\")[1].split(\"]\")[0].split(',')\n",
    "    size = [25]#list(map(lambda x: int(x),b))\n",
    "\n",
    "    Conv =\"GAT\"# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "    \n",
    "    walk_length = trial.suggest_int(\"walk length\",5,20,step = 5)\n",
    "    walks_per_node = trial.suggest_int(\"walks per node\", 5,10,step = 5)\n",
    "    context_size = trial.suggest_int(\"context size\",5,20,step = 5)\n",
    "    num_negative_samples = walks_per_node\n",
    "    loss = {\"Name\":\"SAGE\" , \"walk length\":walk_length,\"walks per node\":walks_per_node,\"num negative samples\":num_negative_samples,\"context size\" : context_size,\"p\":1,\"q\":1, \"loss var\": \"Random Walks\"}\n",
    "\n",
    "    model = Net(dataset = dataset,mode='unsupervised',conv=Conv,loss_function=loss[\"loss var\"],device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = len(size),dropout = dropout)\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_mask, batch_size = 677, sizes=size)\n",
    "    LossSampler = Sampler(data,device=device,mask=train_mask,loss_info=loss)\n",
    "    \n",
    "    model.to(device)\n",
    "    #Generate the optimizers\n",
    "#optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\",\"RMSprop\",\"SGD\"])\n",
    "    learning_rate=trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "    #optimizer = getattr(optim,optimizer_name)(model.parameters(),lr = learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "    #classifier = trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "    classifier = \"logistic regression\"\n",
    "    #classifier = \"catboost\"\n",
    "    #training of the model\n",
    "    for epoch in range(50):\n",
    "        loss = train(model,data,optimizer,LossSampler,train_loader,dropout)\n",
    "        train_acc, test_acc = test(model,data,classifier)\n",
    "        \n",
    "    trial.report(test_acc,epoch)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialsPruned()\n",
    "    return test_acc\n",
    "        \n",
    "    \n",
    "study = optuna.create_study(direction=\"maximize\",study_name=\"SAGE loss\")\n",
    "study.optimize(objective,n_trials = 3)\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\" Number of finid=shed trials: \",len(study.trials))\n",
    "print(\" Number of pruned trials: \", len(pruned_trials))\n",
    "print(\" Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key,value))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "My__RW_Neighbour.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
