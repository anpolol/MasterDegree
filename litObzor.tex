\documentclass[11pt, titlepage]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[T2A]{fontenc}
\usepackage{cite}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx} 
\usepackage{indentfirst}

\title{Литературный обзор по теме <<Разработка алгоритма обучения с переносом для графовых структур данных>>}
\author{Андреева Полина}
%\date{}

\begin{document}
\maketitle

\clearpage

\begin{center}
			{\section*{Введение}}
		\end{center}
		
Графовое представление данных это естественный, понятный человеку с одной стороны, и удобный, подходящий для машинной обработки с другой, способ визуализации данных из различных областей: от генетики до банковского дела. И хотя большинство самых распространенных архитектур нейронных сетей (напримен CNN, RNN) ориентировано на векторные структуры, в последнее время появились новые подходы глубокого обучения для представления и моделирования графово-структурированных данных. 

Большинство успехов в глубоком убучении достигнуто с применением одной из двух парадигм - обучения с учителем (контролируемое обучение) или обучение с подкреплением. В обоих случаях пределы обучения определяются людьми. Для истинного интеллекта необходимы более независимые стретегии обучения. Например, младенцы изучают мир из любопытсва, через наблюдение. Существует одна парадигма, которая углубляет понимание интеллекта - обучение без учителя, когда агент обучается с целью обучиться.  Во взрослой жизни, люди постоянно приспосабливаются к текущей ситуации, используя имеющиеся навыки и знания. Такая адаптация позволяет не обучаться с нуля какому-либо ремеслу, если уже имеется опыт в похожих сферах, а лишь разобраться в относительно небольшом объеме специфических деталей.   В связи с чем в глубоком обучении придумана следующая парадигма, углубляющая понимание работы истинного интеллекта и развивающая работу искусственного - обучение с переносом. С точки зрения затраченных ресурсов, такой способ решения задач намного эффективней, так как не требует большого объема новых данных.  

Формальное определение обучения с переносом, сформулированное в \cite{Survey on TL}, будет дано в следующем пункте. А в последующих разделах будут рассмотрены алгоритмы обучения с переносом на графовых структурах данных. 

\newpage
\begin{center}
			{\section*{Обучение с переносом}}
		\end{center}

		\subsection*{Постановка задачи}

Строгое определение задачи обучения с переносом, сформулирвоанное S. Pan и Q. Yang \cite{Survey on TL}: 

Домен $\mathcal{D}$ состоит из пространства признаков $\mathcal{X}$ и маргинального вероятностного распределения P(X), где X=$\{x_1, ..., x_n\} \in \mathcal{X}$.
При заданном домене $\mathcal{D} = \{ \mathcal{X}, P(X)\}$, задача $\mathcal{T}$ задается также двумя комонентами: пространство меток $\mathcal{Y}$ и целевая предсказательная функция $f(\cdot)$, которая находится из тренеровочных данных, состоящих из пар $\{x_i,y_i\}$, где $x_i \in X$ и $y_i \in \mathcal{Y}$  

В обучении с переносом существует два домена $\mathcal{D}_s, \mathcal{D}_t$ и две задачи $\mathcal{T}_s, \mathcal{T}_t$ - исходный и целевой. Задача: улучшить обучение целевой предсказательной функции $f_T(\cdot)$ в целевом домене, используя знания в исходных $\mathcal{D}_s, \mathcal{T}_s$.

\subsection*{Классификация по подходу переноса}

В зависимости от того одинаковые ли или различаются признаки/ распределения вероятностей/метки существуют несколько подходов в обучении с переносом:

\begin{enumerate}

\item Inductive transfer learng

Исходный и целевой домен один и тот же, а исходная и целевая задача различаются. Идея состоит в применении того набора допущений, который был использован для решения в исходном домене, при решении целевой задачи. В зависимости от того содержит ли исходный домен помеченные данные или нет, данный тип обучения с переносом делится на две соответвующие подкатегории.

\item Unsupervised  transfer learning

Исходный и целевой домен один и тот же, а исходная и целевая задача различаются. Фокус направлен на неконтролируемое обучение в целовой задаче. Соотвественно данные не помечены ни в одном из доменов. 

\item Transductive transfer learning

Исходная и целевая задача похожи, но соотвествующие домены различаются. Исходный домен содержит помеченные данные, целевой - нет. В зависимости от того, отличается ли пространство признаков или распределения вероятностней у исходного и целевого домена, можно классифицировать задачи далее. 

\end{enumerate}

\subsection*{Классификация по предмету переноса}

Существует несколько важных вопросов, на которое необходимо ответить прежде чем приступать к разработке алгоритма для задач обучения с переносом: что переносить? как переносить? 
Рассмотренные в предыдущем пункте подходы переноса отвечают на последний вопрос. А в данном разделе будет рассмотрена классификация задач, в зависимости от ответа на первый вопрос - что переносить?

\begin{enumerate}

\item Instance transfer

Определенная часть данных в исходном домене может быть передана для обучения в целевой домен посредством переназначения весов.

\item Parameter transfer 

Идея основана на предположении, что модели имеют некоторые общие параметры или априорные распределения гиперпараметров, которые и переносятся из исходной задачи в целевую. 


\item Feature representations transfer

Идея состоит в том, чтобы минимизировать расхождения в исходном и целевом домене посредством определения <<хорошего>> представления признаков (например <<хороших>> эмбдингов), которое и передается из исходного и в целевой домен. 


\item Relational Knowledge transfer.

Используется для данных, которые не независимы и не одинаково распределены, т.е. для таких, у которых для каждой точки есть связь с другой точкой. Предполагается, что некоторые отношения между данными аналогичны в целевом и исходном домене. Эти соотношения и есть знания, которые передаются.

\end{enumerate}
\newpage
В таблице 1 показаны какие случаи обучения с переносом могут быть применены в каждом подходе. 

\begin{figure} [h!]
	\begin{minipage}{\linewidth}
	 		\center {\includegraphics[width =1\linewidth]{different_approaches_used_in_different_settings.png}} \\ \small{Таблица 1 \cite{Survey on TL}}
 	\end{minipage}
	 \end{figure}

\begin{center}
			{\section*{Обучение с переносом на графах}}
		\end{center}

Графы -- структуры, состоящие из уникальных сущностей (узлов графа) и связей между ними (ребер графа), являются основной  формой  представления  и  хранения  знаний и удобным иснтрументом для работы с большинством видов данных. Среди основных преимуществ графового представления данных это логическая строгость и масштабируемость  на  большие  объемы  информации. 
Последнее время было сделано много попыток расширить сверточные нейронные сети на графовые структуры данных. Проблема графовых структур данных в самом не-векторном представлении. В связи с чем, основным подходом в задачах на графах стало представление элементов графа в некотором высокоразмерном пространстве, в виде векторов, так называемых эмбедингов \cite{GNN}. В таком виде задача решается как для любой другой обычной сверточной нейронной сети.  Методы, использующие эмбединги, называются Graph Neural Networks (GNNs) \cite{GNN} и были применены к различных задачам относящимся  к графам.  

Задачи на графах можно разделить на следующие наиболее общие группы:

\begin{itemize} 

\item Предсказание связи между вершинами

\item Классификация вершин

\item Классификация ребер %(FNN - feed forward neural network)

\item Классификация графов % (FNN - feed forward neural network) Few-Shot Graph Classification with Model Agnostic Meta-Learning Ning Ma
\end{itemize}
%??????? упоминать ли методы представления графов в виде эмбедингов (node2vec, random walk)
\newpage
\begin{center}
			{\subsection*{Различные методы обучения с переносом на графах}}
		\end{center}
	\subsubsection*{Обучение с переносом внутренней геометрии на графово - структурированных данных \cite{Intristic}}

\textit{Подход}: Inductive. Feature representatuion transfer. 

\textit{Какую проблему решает}:
Методы CNN и RNN ориентированы на матричные структуры данных. А данный подход улучшает глубокое обучение данных с графовой структурой с помощью обучения с переносом. Передавая внутреннюю геометрическую информацию, полученную в исходной области, этот метод позволяет построить модель для новой, но связанной задачи в целевой области без сбора новых данных и без обучения новой модели с нуля.

\textit{Основная идея}: Обучить сверточную нейронную сеть на исходных данных, а затем последний слой переобучить на целевой задаче для перенастройки весов. 

\textit{Метод по шагам}:
\begin{itemize}
\item Строится граф из входных данных. Есть два варианта построения графов: co-occurrence graph estimation (CoGE) \cite{Sonawane} и supervised graph estimation (SGE) \cite{Henaff}. CoGE напрямую определяет близость элементов данных основываясь на частоте смежности. SGE автоматически запоминает признаки сходства элементов. 

\item Строится лапласиан полученного графа. Определяется операция свертки.

\item Применяется сверточная нейронная сеть к графам. 
Модель SCNN (спектральная сверточная нейронная сеть) обучается на информации, полученной на предыдущем шаге. Обучение определят веса каждого слоя путем минимизации функции потерь, специфичной для каждой задачи.

\item Выделение признаков для переноса. 

\item Обучение переносом в спектральном домене. 
Строится модель для целевой задачи копированием сверточных и  субдискретизирующих слоев, которые содержат признаки обученные для исходного домена и исходной задачи. А затем, последний, полносвязный слой модели переобучается на новом, целевом домене для настройки весов. 

\end{itemize}

\textit{Результаты}: передача знаний между доменами более эффективна, когда представление графа в исходном и целевом домене более схожи.

\textit{Будущее развитие}: применить подход к различным наборам данных в разных областях.

		
		
\subsection*{SR2LR \cite{SR2LR}}
 
\textit{Задача}: предсказание связей между вершинами.

\textit{Подход}: Inductive transfer learning. Relational Knowledge transfer.

%Какую проблему решает: увеличивает скорость и точность по сравнению с SRL, а также справляется с ситуацией, когда сущносотей совсем мало, в экстремальном варианте - один.  

\textit{Основная идея}: 

Данные в исходном и целевом доменах описываются марковскими логическими сетями. Предикаты описывают отношения между сущностями. Предполагается, что хорошая модель исходного домена содержит дизъюнкты двух типов - короткодействующие (short-range clauses), которые касаются свойств единственной сущности и  дальнодействующие (long-range clauses), относящийся к свойствам нескольких сущностей. Возможные отображения дизъюнктов первого типа в целевой домен могут быть вычислены на доступных целевых данных непосредственно. Поэтому метод основан на том, чтобы использовать короткодействующие дизъюнкты с целью найти отображения между отношениями в двух доменах, а затем эти отображения использовать для передачи дальнодействующих дизъюнктов. Отсюда и название метода SR2LR - short-range to long-range.

\textit{Сравнение с другими методами}:

\begin{itemize}
 \item Одна из областей, где обучение с перносом особенно эффективно работает - это статичтическое относительное обучение (statistical relational learning SRL). В SRL обучаются вероятностные модели на основе сложных реляционных данных \cite{Getoor and Taskar}, \cite{Richardson and Domingos}. Примеры обучения (они называются мега-примеры) обычно огромные, содежрат сотни сущностей и связей различной длины. Из-за этого, как правило, алгоритмы SRL имеют длительное время обучения и часто чувствительны к локальным максимумам. 

\item Для решения проблем точности и скорости обучения и эффективен подход обучения с переносом \cite{Mihalkova et al} \cite{Davis and Domingos}. Но эти подходы требуют большого количества данных, как минимум один целевой мега-пример, состоящий из большого количества сущностей. А метод  SR2LR справляется с проблемой и решает задачу в случае нембольшого количества сущностей, в экстремальном варианте - всего с одним.  

\item Structure-mapping engine (SME) \cite{Forbus and Oblinger} тоже переносит исходные знания в целевой домен. Метод производит отображение предикатов на основе синтаксического структурного критерия, называемого системностью, и не учитывает точность результирующих выводов в целевых данных. В отличие от SR2LR, который оценивает отображения на основе того, производят ли они эмпирически адекватные дизъюнкты в целевой области.

\end{itemize}

\textit{Будущее развитие}: новые способы отображения исходных знаний, такие как, отображение предикатов разной арности друг в друга, отображение конъюкций двух предикатов в исходном домене в один предикат в целевом домене и наоборот. 


\subsection*{AdaGCN \cite{AdaGCN}}

Алгоритмы доменной адаптации (domain adaptation) справляются с задачами обучения с переносом в ситуациях, когда распределение вероятностей в исходном и целевом домене различаются. 

\textit{Задача}: Классификация узлов. Использование частично помеченной исходной сети для того, чтоб облегчить классификацию узлов в другой, полностью не помеченной сети.

\textit{Подход}: Transductive transfer learning, feauture representation transfer.

\textit{Основная идея}: метод состоит из двух компнент - компонента полуконтролируемого обучения (найти границу классов с помощью сверточных сетей) и компонента состязательной доменной адаптации (найти представления вершин, инвариантные относительно доменов с помощью состязательного обучения)  

1 компонента:  представляем $k$ - ый сверточный слой в исправленном виде относительно обычного GCN:

\begin{equation}
\textbf{H}_g^{k} = \sigma \left( \hat{\textbf{A}}^{n_I} \textbf{H}_g^{k-1} \textbf{W}_g^{k}\right),
\end{equation}
где $\hat{A}$ -- сумма матрицы смежности графа и диагональной матрицы степеней вершин графа, $H_g^k$ -- k-ый сверточный слой, $W_g^k$-- матрица весов, $n_I$ -- параметр сглаживания. С помощью установления подходящего данного параметра, можно контролировать силу сглажвания свертки графа для облегчения переноса знаний и классификации, избегая переобучения. 

Далее, после представления вершин исходного и целевого домена в виде эмбедингов, эти данные передаются в классификатор для прогнозирования меток. Классификатор может быть и однослойными классификатором логистической регрессии и многослойным перцептроном. 

2 компонента: доменная адаптация моделирутеся как игра двух игроков, так же как и в GAN. Сеть для генерации эмбедингов вершин играет роль генератора, а играющий роль дискриминатора доменный критик оптимизируется для задача классификаций эмбедингов вершин на два класса: вершины из исходной сети и вершины из целевой. 

После состязательного обучения, могут быть получены инвариантные представления сети и информация о классах может быть передана из исходного в целевой домен. Итого, для обучения инвариантных узлов решается следующая minimax проблема:

\begin{equation}
\underset{\theta_g}{min } \underset{\theta_d}{max} \{ \mathcal{L}_d - \gamma \mathcal{L}_{grad} \} ,
\end{equation}
где $\theta_g, \theta_d$ параметры обучения модели , $\mathcal{L}_d$ функция потерь доменного критика по отношению к $\theta_d$, а $\mathcal{L}_{grad}$ это штрафной градиент для параметра $\theta_d$, $\gamma$ это штрафной коэффициент, который должен быть устанолвен нулем для генератора.  

\textit{Сравнение с другими методами}:

\begin{itemize}

\item 
Существующие эмбединговые методы \cite{Deepwalk}, \cite{LINE}, \cite{node2vec} сначала изучают компактные представления узлов для сохранения информации о структуре сети, а затем обучают классификатор с помощью изученных представлений для классификации узлов. Такие методы не позволяют решить следующие проблемы:

- значительное расхождение между исходным и целевым доменом и отсутсвие общих свойств.

- отсутвие межсетевых ребер для распространения знаний из исходной в целевую сеть. 

- если в исходном домене помечена только малая часть вершин. 

\item Методы полуконтролируемого обучения основанные на графах \cite{Salakhutdiniv}, \cite{Kipf} показали высокую эффективность для классификации вершин на единственной сети даже в случае когда помеченных вершин не очень много. Методы GCN \cite{Kipf}, GraphSAGE \cite{Hamilton}, GAT \cite{Velickovic} показали высокую производительность при классификации вершин, посредством интеграции топологии графов, свойств вершин и наблюдаемых меток вршин в сквозную среду обучения. Но эти методы построены для обучения задания в единственном домене и имеют проблемы в обобщении на другие домены, которые могут иметь существенно отличающийся набор атрибутов. 

\item Несколько методов \cite{Ni},\cite{Xu} предложены для использования оношений между сетями для повышения эффективности. Они обучают эмбединги для нескольких сетей одновременно, но они сильно зависят от существования межсетевых соединений. Как уже было сказано выше, они не справятся с ситуацией отсутвия этих связей.

\item Обычный метод доменной адаптации использует знания исходных доменов для того, чтоб решить такую же задачу в целевом домене \cite{Yang},\cite{Wang}. Практически все методы в основном ориентированы на векторные данные, такие как картинки и текст, а не графовые структуры данных. 

\end{itemize}

\textit{Будущее развитие}: Исследование передачи знаний из множества исходных сетей в одну целевую сеть и изучение условной адаптации состязательного домена для лучшего согласования распределения мультимодальных данных. 


\subsection*{AS-MAML \cite{AS-MAML}}

Вариант переноса с обучением, когда требуемый результат получают основываясь только на нескольких/одном тренировочной примере соответвенно. Этот тип крайне важен в реальном мире, когда невозможно для каждой задачи собрать большое количество помеченных тренировочных данных для каждого класса и в случаях, когда классы постоянно добавляются. 

\textit{Подход}: Inductive. Parameter transfer.

\textit{Задача}: Классификация графов.

Дан граф $G = {(G_1, y_1), (G_2, y_2),... , (G_n, y_n)}$, где $G_i = (V_i, E_i,X_i)$. В зависимости от метки $y_i$ он разбивается на тренировочное и тествое множество $\{(G^{train}, y^{train})\}$ и $\{(G^{test}, y^{test})\}$. При этом $y^{train}, y^{test}$ не должны иметь общих классов. На этапе обучения на каждом шаге выбирается задача $\mathcal{T}$ и каждая задача содержит поддерживающие данные $\mathcal{D}_{sup} = \{ (G_i^{train}, \textbf{y}_i^{train}) \}_{i=1}^s$ и данные запроса $\mathcal{D}_{que} = \{ (G_i^{text}, \textbf{y}_i^{test}) \}_{i=1}^q $. Имея помеченные данные в $\mathcal{D}_{sup}$, необходимо предсказать метки в $\mathcal{D}_{que}$.

\textit{Основная идея}:  Передача знаний от обученной задачи классификации графов к новым задачам. Этот метод является объединением методов GNN \cite{GNN} в качестве основы и MAML \cite{MAML} для автоматического подбора оптимальных гиперпараметров. 

Этап 1: применение GNN. Цель GNN это представить граф в виде эмбедингов. Это делается в предположении теоремы Банаха о неподвижной точке, так как каждый эмбединг зависит от эмбедингов свох соседей. Существуют различные методы GNN, в зависимсоти от того в каком виде представлена функция для построения эмбединга. В методе AS-MAML используется агрегатор среднего значения, как в методе GraphSAGE  \cite{Hamilton}:

\begin{equation}
\textbf{h}_v^l = \sigma \left( \textbf{W} \cdot mean(\{ \textbf{h}_v^{l-1}\} \cup \{ \textbf{h}_u^{l-1},   \forall u \in \mathcal{N}(v) ) \} \right),
\end{equation}
где $\textbf{h}_v^l$ - это $l$ - ый слой представления вершины $v, \sigma$ - сигмоидная фунцкия, $\textbf{W} $ параметры агрегатора, $ \mathcal{N}(v)$ содержит соседей вершины $v$. 

А затем использовать обычную сверточную нейронную сеть. 

Этап 2: применение MAML для реализации быстрой адапатации. \cite{Du} предложил основанный на обучении с подкреплением (Reinforcement Learning - RL) шаговый контроллер для проведения метаобучения для предсказания связей между вершинами. Но выбраннная функция потерь является слишком неоптимальной, чтобы рассматриваться как вознаграждение за преодоление переобучения. В связи с этим и разработан новый шаговый контроллер для ускорения обучения и преодоления переобучения. Он также управляется RL, но оптимальный шаг адаптации выбирается исходя из показателя ANI (Average Node Information) и функции потерь в качестве входных данных и точности классификации в качестве вознаграждения:

%\begin{equation}
%ANI_i^l = \frac{1}{n_i^l} \sum_{j=1} || [(\textbf{I}_i^l - (\textbf{D}_i^l)^{-1} \textbf{A}_i^l) \textbf{H}_i^l ]_{j} ||_1  
%\end{equation}

\textit{Сравнение с другими методами}:
\begin{itemize}

\item Существует несколько методов классификации вершин \cite{Satorras}, \cite{Kim}, \cite{Liu}, \cite{Yao}, основанные на GNN и ускоряющие передачу знаний и методы предсказания связей \cite{Du} между вершинами. Но они не могут быть расширены на задачи классификации графов. 

\item В \cite{Chauhan} предложена классификация графов, основанная на спектральных изерениях, но этот метод предполагат, что тестовые классы принадлжат тому же множуству классов, что и тренировочные данные.  

\end{itemize}

\textit{Будущее развитие}:  Данный метод может быть расширен на другие задачи классификации графов, как например распознавание действий по скелету и анализ подграфов для социальных сетей. 

\newpage
\begin{center}
			{\section*{Выводы}}
		\end{center}

Было рассмотрено несколько различных методов, использующих совершенно разные подходы:
и Inductive и Trasnductive обучение, перенос и параметров, и представлений признаков, и относительных знаний. Методы, решающие задачи классификации узлов и графов и задачи предсказания связей между вершинами. Подходы в данных методах отличаются значительно: одни методы достаточно просты и лишь на последнем слое нейронной сети меняют веса, другие состаят из нескольких компонент, каждая из которых решает отдельную задачу. Метод SR2LR стоит особняком, так как рассматривает графы знаний, отличающиеся представлением зависимостей между вершинами графа. 

Основное будущее развитие всех методов это применение их к различным наборам данных и другим задачам, а также улучшения методов, направленные на уменьшение используемых данных, необходимых для обучения, на разработки других способов отображения данных из исходного домена в целевой и тд. 

\newpage
\begin{center}
			{\section*{Список использованной литературы}}
		\end{center}

\begin{thebibliography}{50}

\bibitem{Survey on TL} S. Pan, Q. Yang. 2010. A survey on transfer learning. \textit{IEEE Transactions on Knowledge and Data Engineering} 22(10):1345–1359.
\bibitem{GNN} Z. Wu, S. Pan, F. Chen, G. Long,C. Zhang, S. Yu Philip. 2019. A comprehensive survey on graph neural networks. arXiv:1901.00596.

\bibitem{SR2LR} Mihalkova, Lilyana and Mooney, Raymond J. 2009. Transfer learning from minimal target data by mapping across relational domains. \textit{Twenty-First International Joint Conference on Artificial Intelligence}. 
\bibitem{Getoor and Taskar}  L. Getoor, B. Taskar, editors. Introduction to Statistical Relational Learning. MIT Press, Cambridge, MA, 2007.

\bibitem{Richardson and Domingos} M. Richardson and P. Domingos. Markov logic networks. Machine Learning, 62:107–136, 2006.
\bibitem{Mihalkova et al} L. Mihalkova, T. Huynh, and R. J. Mooney. Mapping and revising Markov logic networks for transfer learning. (AAAI-07).
\bibitem{Davis and Domingos} J. Davis, P. Domingos. Deep transfer via second-order markov logic. \textit{In Proceedings of the AAAI Workshop on Transfer Learning For Complex Tasks}. 2008
\bibitem{Forbus and Oblinger}  Kenneth D. Forbus , Dan Oblinger. Making SME greedy and pragmatic. (CogSci-90).
\bibitem{Intristic} J. Lee, H. Kim, J. Lee, and S. Yoon, “Transfer learning for deep learningon graph-structured data.” in AAAI, 2017, pp. 2154–2160 
\bibitem{Sonawane}  Sonawane, S., and Kulkarni, P. 2014. Graph based representation and analysis of text document: A survey of techniques. \textit{International Journal of Computer Applications} 96(19).
\bibitem{Henaff} Henaff, M.; Bruna, J.; and LeCun, Y. 2015. Deep convolutional networks on graph - structured data. arXiv:1506.05163.
\bibitem{AdaGCN} Quanyu Dai, Xiao Shen, Xiao-Ming Wu, Dan Wang. 2019. Network Transfer Learning via Adversarial Domain Adaptation with Graph Convolution. arXiv:1909.01541(2019)

\bibitem{Deepwalk} B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: online learning of social representations,” in KDD, 2014, pp. 701–710.
\bibitem{LINE} J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “LINE: large-scale information network embedding,” in WWW, 2015, pp. 1067–1077.
\bibitem{node2vec} A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” in KDD, 2016, pp. 855–864.

\bibitem{Salakhutdiniv} Z. Yang, W. W. Cohen, and R. Salakhutdinov, “Revisiting semi- supervised learning with graph embeddings,” in ICML, 2016, pp. 40–48.
\bibitem{Kipf} T. N. Kipf and M. Welling, “Semi-supervised classification with graph convolutional networks,” in ICLR, 2017.

\bibitem{Hamilton} W. L. Hamilton, R. Ying, and J. Leskovec, “Inductive representa- tion learning on large graphs,” in NIPS, 2017.
\bibitem{Velickovic} [12] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Li`o, and Y. Bengio, “Graph attention networks,” CoRR, 2017.
\bibitem{Ni} J. Ni, S. Chang, X. Liu, W. Cheng, H. Chen, D. Xu, and X. Zhang, “Co-regularized deep multi-network embedding,” in WWW, 2018, pp. 469–478.
\bibitem{Xu} L. Xu, X. Wei, J. Cao, and P. S. Yu, “Embedding of embedding (EOE): joint embedding for coupled heterogeneous networks,” in WSDM, 2017, pp. 741–749.
\bibitem{Yang} S. J. Pan and Q. Yang, “A survey on transfer learning,” \textit{IEEE Trans. Knowl. Data Eng}., vol. 22, no. 10, pp. 1345–1359, 2010.
\bibitem{Wang} M. Wang and W. Deng, “Deep visual domain adaptation: A survey,” Neurocomputing, vol. 312, pp. 135–153, 2018
\bibitem{AS-MAML} Ma, Ning, et al. "Few-Shot Graph Classification with Model Agnostic Meta-Learning." arXiv preprint arXiv:2003.08246 (2020).
\bibitem{MAML} Finn, Chelsea, Pieter Abbeel, and Sergey Levine. "Model-agnostic meta-learning for fast adaptation of deep networks." \textit{Proceedings of the 34th International Conference on Machine Learning-Volume 70.} JMLR. org, 2017
\bibitem{Du} Zhengxiao Du, Xiaowei Wang, Hongxia Yang, Jingren Zhou, and Jie Tang. Sequential scenario- specific meta learner for online recommendation. In KDD’19, page 2895–2904, 2019.
\bibitem{Satorras} Victor Garcia Satorras and Joan Bruna Estrach. Few-shot learning with graph neural networks. In ICLR, 2018.
\bibitem{Kim} Jongmin Kim, Taesup Kim, Sungwoong Kim, and Chang D. Yoo. Edge-labeling graph neural net- work for few-shot learning. In CVPR, June 2019.
\bibitem{Liu} Lu Liu, Tianyi Zhou, Guodong Long, Jing Jiang, and Chengqi Zhang. Learning to propagate for graph meta-learning. In NeurIPS, 2019. 
\bibitem{Yao} Huaxiu Yao, Xian Wu, Zhiqiang Tao, Yaliang Li, Bolin Ding, Ruirui Li, and Zhenhui Li. Au- tomated relational meta-learning. In ICLR, 2020.
\bibitem{Chauhan} Jatin Chauhan, Deepak Nathani, and Manohar Kaul. Few-shot learning on graphs via super- classes based on graph spectral measures. In ICLR, 2020.

\end{thebibliography}
%\bibliography{currentbase}{}
%\bibliographystyle{plain}

\end{document}