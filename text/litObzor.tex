\documentclass[11pt, titlepackage]{article}
\usepackage[left=3cm, right=3cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[T2A]{fontenc}
\usepackage{cite}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx} 
\usepackage{indentfirst}
\usepackage{multirow}
\usepackage{footnote}
\usepackage{longtable}
\usepackage{array}
\usepackage{caption,tabularx,booktabs}
\usepackage{tikz} 
\usepackage{longtable}
\usepackage{amsmath}
% переименовываем  список литературы в "список используемой литературы"
\addto\captionsrussian{\def\refname{\centering Список используемых источников}}
\title{Литературный обзор по теме <<Разработка алгоритмов обучения без учителя для графовых структур данных>>}
\author{Андреева Полина}
\date{}

\begin{document}
\maketitle
\setcounter{page}{1}
\clearpage
\begin{center}
{\tableofcontents}
\end{center}

\newpage


\begin{center}
			{\section{Введение}}
		\end{center}
		
Графовое  представление данных это естественный, понятный человеку, в связи с чем, очень распространенный способ визуализации тех данных, которые могут взаимодействовать друг с другом. Графы используются повсеместно: от генетики до банковского дела. Как следствие, может возникать множество важных задач с использованием информации из графов: рекомендации товаров/фильмов/друзей в социальных сетях, классификация протеинов в молекулах и т.д. Эти задачи можно разделить на 4 основных типа:

\begin{itemize} 

\item Предсказание связи между вершинами

\item Классификация вершин

\item Классификация ребер %(FNN - feed forward neural network)

\item Классификация графов % (FNN - feed forward neural network) Few-Shot Graph Classification with Model Agnostic Meta-Learning Ning Ma
\end{itemize}

Из-за своей распространенности, а также типов возникающих задач, графы могут играть важную роль в машинном обучении. 

Но задачи машинного и глубокого обучения оперируют как правило с матричными структурами. Поэтому основной проблемой  графов является включение информации из графовой структуры, являющейся намного более сложной, чем матричная, в модель машинного обучения. Традиционных подходы используют для этой цели какие-либо количественные параметры графа (матрица смежности, степени вершин, самая короткая длина пути между вершинами и т.д.) \cite{Statistics,linkPred} что является довольно ограниченным способом: во-первых, невозможно изменять эти значения во время обучения, так как они считаются один раз во время препроцессинга, во-вторых, данные подходы могут быть очень неэффективными по времени из-за высокой размерности графа. 

Поэтому, более эффективным способом оказалось независимое от задачи обучение представлений вершин графов в низкоразмерном пространстве. Что является уже не препроцессингом, а задачей машинного обучения само по себе \cite{EmbedingSurvey, comparUnRep}. Данные представления вершин называются эмбеддингами и получают из совмещения информации, полученной из матрицы признаков и от соседей. Эмбеддинги строятся во время обучения, как и в любой нейронной сети в ходе минимизации некоторой функции, называемой функцией потерь и означающей насколько далеким результат получился от желаемого. Варианты функций потерь для графовых нейронных сетей и их влияние на качество построенных эмбеддингов будут рассмотрены в данном обзоре. 

\newpage

\begin{center}
			{\section{Обучение нейронных сетей}}
		\end{center}

В самом общем случае, нейронная сеть состоит из трех частей: входной слой; скрытые слои, в которых происходят линейные преобразования с матрицей весов $\textbf{W}$ входных данных в выходные; и выходной слой с нелинейным преобразованием и получением результата. 
А обучение нейронной сети включает в себя два этапа, которые проведенные друг за другом называются эпохой.

Первый этап --- это прямое преобразование входных данных в выходные, называющийся прямым проходом. Матрица весов чаще всего инициализируется случайно, но по итогу обучения веса подбираются таким образом, чтобы результат выходного слоя был достаточно удовлетворительным. Для определения того, насколько хорош этот результат после каждой эпохи, необходимо ввести некоторую функцию, называемую функцией потерь
$\mathcal{L}$. Аргументами данной функции являются: полученный результат в ходе прямого прохода, а также, в некоторых случаях, результат, к которому надо стремиться. Значением этой функции является потеря, показывающая насколько далеко результат от желаемого. То есть, веса настраиваются таким образом, чтобы минимизировать функцию потерь. 

Необходимым условием экстремума для любой дифферецируемой функции является равенство нулю производной по всем переменным. В случае функции потерь нейронной сети, нулю приравнивваются производные по весам. Более того, если сеть глубокая, то есть, если скрытых слоев, идущих последовательно друг за другом, больше, чем один, то и производные по весам более ранних слоев, надо брать последовательно, выражая через производные по весам более поздних слоев (производная сложной функции). Это и есть второй этап, называемый обратным распространением ошибки. После окончания этого этапа получают производные по всем весам. Вектор, состоящий из этих производных, --- градиент функции потерь, указывающий в сторону ее возрастания в пространстве весов. Так как необходимо получить именно минимум функции, то каждый вес "исправляется" в сторону, обратную градиенту. В общем случае:
\begin{equation}
\textbf{W} = \textbf{W} - \lambda \frac{\partial \mathcal{L}}{\partial \textbf{W}},
\end{equation}
где $\lambda$ --- параметр, означающий длину шага изменения веса в сторону, обратную градиенту. Называется скоростью обучения. 

В задачах, где есть размеченные данные, функция потерь показывает насколько расходится результат с настоящими отметками. Но таких размеченных данных бывает мало, или для какой-то конкретной задачи может не быть вовсе. Для решения таких задач была придумана парадигма неконтролируемого обучения. Самой распространенными вариациями задач такого рода являются кластеризация (минимизируются расстояния между данными) и автоэнкодеры (сначала входная информация сжимается до небольшого размера, а затем восстанавливается, минимизируется ошибка реконструкции).

Еще одним важным подходом в обучении нейронных сетей является построение представления (эмбеддинга) в более низкоразмерном пространстве для каждого входного элемента данных. Этот подход является наиболее общим, потому что теперь имеется возможность перевести данные в низкоразмерные представления, и уже после обучения отдельно на эмбеддингах решать задачи классификации, предсказания и тд.
 Например, данный способ очень помогает в случаях, если стоит задача классификации, а размеченных данных слишком мало. В таком случае можно сначала обучить представления, а затем обучить классификатор на представлениях имеющихся данных. После этого, для новых вершин построить представление и в пространстве эмбеддингов найти вершины какого класса ближе к рассматриваемой. Функции потерь в случае обучения представлений также делятся на контролируемые (например кросс-энтропия, сравнивающая результат с имеющимися данными) и неконтролируемые (ошибка реконструкции). 

Именно такое построение представлений активно используется в графовых нейронных сетях. Такая популярость достигнута благодаря независимости данного подхода от конкретной задачи и удобству отображения графово-структурированных данных в матрично-структурированные.

\begin{center}
			{\section{Варианты графовых нейронных сетей}}
		\end{center}

Как видно из предыдущего пункта, на текущий момент для решения вопроса графовых нейронных сетей наиболее популярный и эффективный способ - построение эмбеддингов. Такие методы разделяются по двум критериям. Во-первых, во время построения эмбеддингов, то есть в процессе прямого прохода - по наличию или отсутствию возможности распространения задач на невидимые до тренировки данные. Во-вторых, по формату функций потерь - контролируемые или нет. 

Transductive манера построения функции потерь предполагает  изменение самого вектора - эмбеддинга в процессе минимизации функции потерь; все последующие задачи можно решать только на уже имеющихся эмбеддингах \cite{Deepwalk, node2vec, LINE}. Inductive манера позволяет в процессе решения последующих задач вводить новые вершины и строить эмбеддинги для них. Для этого настраиваются во время обучения сети не сами эмбеддинги, а параметры некоторой функции  $f: G \rightarrow \mathbb{R}^d$, отображающей граф $G(V,E)$, где $V$ -- множество вершин графа, а $E$ -- множество рёбер графа, в евклидово прострнатсво размерностью $d$. Последний способ вдохновлен сверточыми нейронными сетями \cite{GraphSAGE, GAN, GCN}.

Важную роль в обучении представлений играет функция потерь, ведь именно в процессе ее минимизации  выводится более оптимальный эмбеддинг. В задачах на графах можно также как и в обычном машинном обучении, рассматривать как контролируемые функции потерь (сравнение предсказаний классов с реальными значениями \cite{GCN}), так и неконтролируемые (ошибка реконструкции \cite{SEED, SDNE}, новые варианты функций потерь, появившиеся благодаря графовой структуре данных, позволяющей измерять некоторого рода "близость" вершин в графе\cite{VERSE,Deepwalk,APP,LINE}).

Варианты неконтролируемых, transductive задач предоставляют большие возможности. Поэтому основной причиной рассмотрения именно этих вариаций является их универсальность и гибкость для решения множества последующих задач. 

\begin{center}
{\section{Энкодер и функция потерь в графовой нейронной сети}}
\end{center}

\textbf{Энкодер}

Основная идея построения энкодера в том, чтобы на каждом слое сети каким-либо образом агрегировать информацию от соседей, затем соединить новый эмбеддинг соседства с эмбеддингом рассматриваемой вершины на предыдущем слое и воспользоваться этим эмбеддингом для построения представления следующей вершины (см. Рис. 1). 
\begin{figure} [h!]
	\begin{minipage}{\linewidth}
	 		\center {\includegraphics[width =1\linewidth]{pictures/GNN.png}} \\ \small{Рисунок 1 \cite{GCN}}
 	\end{minipage}
	 \end{figure}

Более детально: для построения эмбеддинга $\textbf{$\Phi$}_v$ вершины $v$ за $K$ шагов, необходимо инициализировать начальные эмбеддинги для каждой вершины. Как правило в качестве инициализации выбирается вектор признаков вершины, если такой имеется, если нет - то случайно.

\begin{equation}
\textbf{h}_v^0 = \textbf{x}_v  
\end{equation}

Далее,  с помощью некоторой функции - аггрегатора, собирается информация от соседей рассматриваемой вершины в один новый эмбеддинг соседства. Эта информация соединяется с эмбеддингом самой вершины на предыдущем слое нейронной сети, и на эту агрегированную ииформацию применяется слой нейронной сети. Как итог - получается эмбеддинг вершины на новом слое. 

\begin{equation}
\textbf{h}^{k}_v = \sigma(\textbf{W}_k \cdot AGG(\{ \textbf{h}^{k-1}_u: u \in \mathcal{N}(v) \cup \{v\}\}) ) 
\end{equation}
В данном случае, $\textbf{h}_v^k - $ представление вершины $v$ на $k-$ом слое. 

Наиболее простым вариантом функции $AGG$ является средняя сумма \cite{GCN}. Эмбеддинги, полученные на последнем слое $K$, являются конечным вариантом $\textbf{$\Phi$}_v$. В ходе обучения нейронной сети, которое подразумевает минимизацию функции потерь, просиходит "настраивание" весов \textbf{W}. 

\textbf{Функции потерь в графовых нейронных сетях}

В случае неконтролируемых задач графовых нейронных сетей, в качестве функции потерь может выступать ошибка реконструкции. Для этого, помимо энкодера, строится и декодер, и минимизируется разница между реальным и декодированным значением: $||X - \hat{X}||$ или $||A - \hat{A}||$ \cite{SDNE, SEED}. Но все же более интересным вариантом будет рассмотреть функцию потерь, построенную таким образом, чтоб учитывать графовую структуру данных. В таком случае, функция потерь будет стремиться к тому, чтобы ближайшие соседи в графе были также близко и в низкоразмерном пространстве. Такие функции потерь используют при построении и transductive эмбеддингов в методах \cite{Deepwalk, node2vec, LINE, HOPE} и др., но их достаточно просто распространить и на графовые сверточные нейронные сети. В общем случае, при рассмотрении последнего варианта функций потерь встает вопрос, как обозначить "близость" вершин и эмбеддингов. Про близость в низкоразмерном прострнастве говорить достаточно прсото - это, например, косинусное расстояние $\textbf{$\Phi$}_u^{\text{T}} \cdot \textbf{$\Phi$}_v$. Осталось определить близость вершин в графе. Обозначим $similarity(v,u)$ - какая-то абстрактная схожесть вершины $v$ и $u$.

Вариантов для вида $similarity$ множество:
\begin{itemize}
\item непосредственная близость по соединениям ребрами (тогда $similarity(u,v)$ - это элемент матрицы смежности) \cite{eigenmaps, factorization, LINE}

\item вероятность появления в ходе случайного блуждания ($similarity(u,v) $-- вероятность появления данной пары в ходе случайного блуждания) \cite{Deepwalk, node2vec, struc2vec,SDNE }

\item какая - то особая метрика близости, функция, которая дя каждой пары вершин выдает значение, типа Personalized PageRank. \cite{HOPE, VERSE,APP}

\item и тд.
\end{itemize}

Итого, идея функция потерь для неконтролируемой, transductive задачи на графах выражена в следующем соотношении:
 
\begin{equation}
similarity(v,u) \approx \textbf{$\Phi$}_u^\text{T}\cdot \textbf{$\Phi$}
\end{equation}

Наконец, можyj подвести итог и перейти к постановке задачи 

\textbf{Постановка задачи }

Используя граф $G(V,E)$, матрицу смежности графа $A$, матрицу признаков $X$, необходимо построить функцию $f$, которая отображает каждую вершину графа в вектор в низкоразмерном пространстве (см. Рисунок 2) $f:G \rightarrow R^n$ таким образом, чтобы данные векторы (называемые эмбеддингами) похожих вершин графа лежали ближе друг к другу $similarity(u,v) \approx \textbf{$\Phi$}_u^\text{T}\cdot \textbf{$\Phi$} $. Соответственно для этого необходимо также определить и меру похожести эмбеддингов.

В данном обзоре будут рассмотрены различные функции потерь и проведен их сравнительный анализ на нескольких вариантах сверточных нейронных сетей. 
 
\begin{figure} [h!]
	\begin{minipage}{\linewidth}
	 		\center {\includegraphics[width =0.7\linewidth]{pictures/u.png}} \\ \small{Рисунок 2}
 	\end{minipage}
	 \end{figure}

Граф представляется в виде упорядоченной пары $G=(V,E)$, где $V = \{v_i\}$ -- непустое множество вершин, а $E = \{e_{i,j}\}$ --- множество пар веришин, называемых ребрами ($e_{i,j}$ --- это реберо между вершинами $v_i, u_i \in V$). Порядок графа $|V|$ --- число вершин в графе, для краткости обозначается $n$. Аналогично, размер графа $|E|$ --- число рёбер, обозначим $m$. 
$\mathcal{N}(v)$ --  множество, состоящее только из тех вершин, которые являются соседями для вершины $v$. $\textbf{A}$ -- матрица смежности, где каждое число $a_{ij}$ означает наличие ($a_{i,j}=1$) или отсутствие ($a_{ij}=0$) ребра между вершинами $v_i$ и $v_j$ в случае невзвешенного графа и вес ребра в случае взвешенного. Диагональная матрица $\textbf{D}$ является матрицей степеней, у которой на диагонали стоят степени соответвующей вершины: $d_{ii} = \sum_{j=1}^n a_{ij}$. 
 $\textbf{W}=\{w_{ij}\}$ ---  матрица настраиваемых весов в нейронной сети. Матрица $\textbf{X}$ --- матрица признаков вершин в графе. Каждая колонка $X_i - $ это вектор признаков соответствующей вершины $v_i$. Размерность пространства признаков равна $d$.
$\mathcal{L} - $  функция потерь. $\textbf{$\Phi$}_i, \textbf{$\Theta$}_i$ --- векторы в низкоразмерном пространстве, т. н. эмбединги вершины $v_i$.  Первый вектор означает представление вершины как исходной, а второе - представление вершины как контекстной для другой вершины. 
\newpage
\begin{center}
			{\section{Наборы данных и метрики качества}}
		\end{center}

\textbf{Задачи}

Качество построенных эмбедингов проверяется на последующих задачах. Чаще всего решаются задачи классификации вершин и предсказания рёбер между вершинами, реже - кластеризации вершин. Также встречаются случаи, когда авторы метода проверят качество, визуализируя получившиеся эмбединги или реконструируя граф и считая разницу с изначальным. 

В данном разделе описаны наиболее популярные наборы данных и метрики для проверки качества методов, которые будут изложены в следующем разделе.

\textbf{Наборы данных}

Наборы данных по смыслу делятся на социальные сети, сети цитирования, сети смежных слов и тд, так что представим в данном разделе соответствующую классификацию. В таблице \ref{tab:datasets} представлена краткая справка по наиболее часто встречаемым наборам данных: размер, направленность, а описание методов представлено ниже:

\begin{enumerate}
\item  Социальные сети: SN-Twitter, Flickr, YouTube, Reddit, Epinions, BlogCatalog.

Вершины представляют собой пользователей, а ребра - дружбу между ними. Некоторые сети имеют атрибуты узлов, как например у сети авторов BlogCatalog атрибуты - темы, на которые пишет данный автор. У сети YouTube, метки вершин предствляют предпочитаемые жанры.

\item Сети цитирования: Cora, Citeseer, CoCit, DBLP, PubMed.

 Каждая вершина представляет собой "мешок слов" статьи, а ребро между двумя вершинами - существование цитирования. Метка вершины - это сфера тематики данной статьи.
  
\item Сети со-авторств: Arxiv. 

Вершины - авторы. Ребро между двумя вершинами существует, если авторы участвовали в написании одно статьи.

\item Сети слов: Wikipedia. 

Вершины - слова. Если между двумя вершинами стоит ребро, значит эти два слова появляются в окне из 5 слов не менее 5 раз. Метки означают части речи.

\item Другое: 

Zachary’s karate network - известная и часто используемая для визуализации сеть университетского клуба карате. 


PPI Protein-Protein Interaction. Каждый граф соответствует отдельной ткани человека. Вершины - протеины c наобором атрибутов. Метка - роль белка с точки зрения его клеточных функций. 

\end{enumerate}
\newpage
\begin{table}[h!]
\caption{\label{tab:datasets} Справка по наборам данных.}
\begin{center}
\begin{tabular}{|p{100pt}| p{55pt}| p{80pt}|p{150pt}|}       
\hline
Категория & Название датасета  & Характеристика & Размер \\
\hline
\multirow{6}{100pt}{Социальная сеть } & SN-Twitter & Направленный & $|V|=465K$ $|E|=834K$ \\ 
\cline{2-4}
 & Flickr &  Ненаправленный & $|V|=80K$ $|E|=5,90M $ \newline $|L|=195$ \\
\cline{2-4}
 & YouTube &Ненаправленный & $|V|=1,13M$ $|E|=2,99M$ \newline $|L|=47$ \\
\cline{2-4}
 & Reddit& Ненаправленный &$|V|=231K$ $|E|=11,6M$ \newline $|L|=41$ \\
\cline{2-4}
 & Epinion& Направленный & $|V|=75K$ $|E|=508K$ \\
\cline{2-4}
 & BlogCatalog & Ненаправленный&$|V|=10K$ $|E|=33K$ \newline $|L|=39$ \\
\hline
\multirow{5}{100pt}{Сеть цитирования} &  Cora &Ненаправленный& $|V|=23K$ $|E|=91K$ \newline $|L|=70$ \\
\cline{2-4}
 &  Citeseer&Ненаправленный & $|V| = 3K$ $|E|= 4K$ \newline $|L|=6$ \\
\cline{2-4}
&  CoCit & Направленный &$|V|=44K$ $|E|=195K$ \newline $|L|=15$ \\
\cline{2-4}
&  DBLP-Ci &Направленный&$|V|=12.5K$ $|E|=49K$ \\
\cline{2-4}
 &  PubMed&Направленный& $|V|=19K$ $|E|=44K$ \newline $|L|=3$ \\
\hline
Сеть соавторовств & Arxiv GR-QC i & Ненаправленный & $|V|=5K$ $|E|=28K$ \\
%проверить все датасеты на направленность : Arxiv, Wikipedua, PPI, karate
\hline
Сеть смежности слов & Wikipedia  &Ненаправленный & $|V|=4K$ $|E|=184K$ \newline $|L|=40$ \\
\hline
\multirow{2}{100pt}{Другое} & Zachary's karate network  &Ненаправленный&$|V|=34$ $|E|=78$ \newline $|L|=4$ \\
\cline{2-4}
 &  PPI &Ненаправленный& $|V|=3K$ $|E|=38K$ \newline $|L|=50$\\
\hline
\end{tabular}
\end{center}
\end{table} 

\newpage
Наиболее часто используемые \textbf{метрики} для проверки качества метода на конкретных задачах собраны в таблице \ref{tab:metrics}:


\begin{table}[h!]
\caption{\label{tab:metrics}Метрики качества.}

\setlength{\extrarowheight}{10pt}
\begin{tabular}{|p{100pt}|p{180pt}|l|}
\hline
Метрика & Формула, описание & Для каких задач \\
\hline
 Micro-F1 score   \footnotemark & \Large $\frac{2\cdot P\cdot R}{P+R}$  & Классификация вершин  \\
\hline
Macro-F1 score & \Large $\frac{\sum_{l\in L} F1(l)}{|L|},$  \normalsize где $ F1(l) - F1 - $ score для метки $l$& Классификация вершин \\
\hline
 AUC (Area Under Curve)& Площадь под кривой ошибок (кривая в осях $TP/(TP+FN)$ к $FP/(TN+FP)$, где точки относятся к различным значениям порога отсечения, при котором считается, что ребро существует)  & Предсказание рёбер \\
\hline
Precision & \Large $\frac{N^k \cap N(v)}{k}$, \normalsize где $N^k - k $ ближаших соседей вершины судя по построенным эмбедингам, $N(v) - $ истинные соседи вершины $v$. &  Реконструкция графа\\
\hline
NMI (Normalized Mutual Information) & \Large $\frac{2\cdot I(Y;C)}{H(Y)+H(C)}$, \normalsize где  $Y - $истинные метки классов, $C -$ метки кластеризации, $H(.,.) -$энтропия $I(.,.) -$ взаимная инфорация & Кластеризация вершин \\
\hline
\end{tabular}
\end{table} 
\footnotetext{Данная метрика считается глобально, подсчетом TP = true Positives, FP = False Postives, FN = False Negative по всем меткам $l \in L$. \newline

$P = \frac{\sum_{l\in L} TP(l)}{\sum_{l\in L} (TP(l)+FP(l))}$,   $R = \frac{\sum_{l\in L} TP(l)}{\sum_{l\in L} (TP(l)+FN(l))}$ }

\newpage 
\begin{center}
{\section{Классификация и описание методов}}
\end{center}

Во-первых, функции потерь можно в общем случае разделить на несколько соновных видов:
\begin{itemize}
\item Максимизация вероятности появления одной вершины в соседстве другой 
\item Задание некоторой матрицы, каждый элемент которой означает меру схожести соответствующих вершин
\end{itemize}

Более подробно каждый вариант из рассматриваемых функций потерь будет описан в пункте \ref{section:LossFunctions} 

Во-вторых, реализация этих функиций потерь будет рассмотрена на нескольких разных энкодерах, взятых из методов GCN (graph convolutional network), GAN (graph sttention network), GraphSAGE (Sample and Aggregate). Различие между этими энкодерами описано в пункте \ref{section:Encoders}

\begin{center}
{\subsection{Виды функции потерь}\label{section:LossFunctions}}
\end{center}

В данном разделе кратко описаны идеи всех рассматриваемых в обзоре функций потерь и обощены в таблице \ref{tab:LossFunctions}. 

В таблице \ref{tab:methods} собраны функции потерь по отдельности. В первом столбце --- название метода, где появилась данная функция потерь. Во втором --- рассматривается ли для вершины только одно представление или два (целевое, которое является основным и контекстное --- являющееся вспомогательным для построения эмбеддингов других вершин), в третьем --- сам вид функции потерь, а в последнем --- способ оптимизации. 

\begin{itemize}
\item Метод Laplacian Eigenmaps \cite{eigenmaps} стремясь сохранить два эмбединга ближе в случае, когда в графе между соответствующими вершинами больше вес (в случае взвешенного графа), минимизирует следующую функцию потерь:
\begin{equation}
\mathcal{L}(\Phi) = \frac{1}{2} \sum_{i,j} |\Phi_i - \Phi_j|^2 a_{i,j} = tr(\Phi^T\textbf{L}\Phi)
\end{equation}
где $\textbf{L}=\textbf{D} - \textbf{A}$ -- лапласиан графа, $\Phi - $ матрица эмбеддингов, в которой отдельный ряд $\Phi_v$ соответствует эмбеддингу вершины $v$.

Решение этой задачи - это собственные вектора, соответствующие $d$ наименьшим собственным числам нормализованного Лапласиана графа.  
%\item Методы  \cite{Sam, reduction} строят эмбединги, минимизируя  функцию потерь, основанную на ошибке линейной реконструкции. 

\item Graph Factorization \cite{factorization} предполагает, что степень схожести между вершинами определяет матрица смежности:
\begin{equation}
\mathcal{L}(\Phi) = \frac{1}{2} \sum_{i,j} ( a_{i,j} - \Phi_i\cdot\Phi_j )^2 = || \textbf{A} - \Phi\cdot\Theta||^2
\end{equation}

%\item GraRep \cite{GraRep} определяет вероятность перехода вершин как $T=D^{-1}A$ и сохраняет схожесть $к$-го порядка минимизируя $|| X^k - Y_s^kY_t^{kT}||$, где  $X^k$ выводится из $T^k$.    разных порядков а затем конкатенирует получившиеся представления. Недостаток это сложность масштабирования. %не посомотрела так как не нашла, 
\item В методе HOPE \cite{HOPE}, также как и в предыдущем минимизируется $||\textbf{C} - \Phi \cdot\Theta||^2$, где $\textbf{C}$ --- матрица, каждый элемент которой отражает схожесть между двумя соответвующими вершинами. Для минимизации эотй функции потерь используется SVD - разложение:
\begin{enumerate}
\item В случае Katz Index, матрица $\textbf{C}$ представляет собой взвешенную сумму всех возможных путей между двумя вершинами. 
\begin{equation}\label{eqn:HOPEKatz}
\textbf{C}^{Katz} = \sum_{l=1}^{\infty} \beta \cdot \textbf{A}^l = \beta\cdot\textbf{A}\cdot\textbf{C}^{Katz} + \beta \cdot \textbf{A},
\end{equation}
где $\beta$ -- коэффициент затухания определяет как быстро затухает вес пути с возрастанием длины этого пути. В пределе получится:
\begin{equation}
\textbf{C}^{Katz} = (\textbf{I}-\beta\cdot\textbf{A})^{-1}\cdot\beta\cdot\textbf{A}
\end{equation}
\item Rooted PageRank: элемент матрицы \textbf{C} в данном случае означает вероятность для конкретной вершины, оказаться в ходе бесконечного случайного блуждания с рестартом в другой вершине. 
\begin{equation}
\textbf{C}^{RPR}(t) = \alpha\cdot\textbf{C}^{RPR}(t-1)\cdot (\textbf{D}^{-1}\textbf{A}) + (1-\alpha)\cdot\textbf{I}
\end{equation}

где $\alpha$ -- вероятность рандомного перехода к соседу. Если для краткости $\textbf{D}^{-1}\textbf{A}$ обозначить одной матрицей $\textbf{P}$, а также рассмотреть $\textbf{C}^{RPR}$ при стремлении $t$ к бесконечности, то получится однозначно-заданный вид:
     \begin{equation}
   \begin{split}
 \textbf{С}^{PPR}(t) = \alpha^2\cdot \textbf{С}^{PPR}(t-2)\textbf{P}^2 +\alpha (1-\alpha)\textbf{P} + (1-\alpha) \textbf{I} =  \\
 = \alpha^t \textbf{С}^{PPR}(0) \textbf{P}^t + (1-\alpha) \sum_{i=0}^{t-1}\alpha^i \textbf{P}^i
 \end{split}
 \end{equation}
  \begin{equation}
   \textbf{C}^{PPR}=\lim_{t \rightarrow \infty }\textbf{C}^{PPR}(t) = (1-\alpha) (\textbf{I}-\alpha\textbf{P})^{-1}
 \end{equation}  
\item Common Neighbors отображат количество вершин, являющихся соседями одновременно для обоих рассматриваемых вершин:
\begin{equation}
\textbf{C}^{CN} = \textbf{A}^2 
\end{equation}
\item Adamic-Adar score аналогично Common Neighbours считает количество одних и тех же соседей для двух вершин, но учитывает некоторый "вес" каждый вершины, который обратен ее степени:
\begin{equation}
\textbf{C}^{AA} = \textbf{A} \cdot\textbf{D} \cdot\textbf{A}
\end{equation}
\end{enumerate}
Авторы статьи показывают, что любую матрицу $C$, отражающую схожесть вершин, можно представить в виде $C=\textbf{M}_g^{-1}\textbf{M}_l$, а в связи с разреженностью $\textbf{M}_g,\textbf{M}_l$ можно эффективно использовать разреженное обобщенное сингулярное разложение (SVD) для построения эмбедингов графа: SVD применяется к $\textbf{M}_g, \textbf{M}_l$. Оптимальные значения эмбедингов - считаются как корни произведения синуглярного числа на соответвующий сингулярный вектор.  

\item Авторы статьи \cite{NETMF} показывают, что каждая из четырех моделей: DeepWalk, LINE, PTE, Node2vec, выполняют неявную матричную факторизацию (алгоритм NetMF). Для каждой модели выведены матричные формы. Например, алгоритм DeepWalk экивалентен факторизации следующей матрицы:
\begin{equation}\label{eqn:NetMF}
\log (\frac{vol(G)}{T}(\sum_{r=1}^T \textbf{P}^r)\textbf{D}^{-1}) - \log(b)
\end{equation}
в данном случае $b$ негативных примеров для Negative Sampling, vol(G) - объем взвешенного графа (сумма степеней всех вершин), T - длина случайного блуждания, r - размер окна в DeepWalk.

Если для краткости положить $\textbf{M} = (\frac{vol(G)}{bT}(\sum_{r=1}^T \textbf{P}^r)\textbf{D}^{-1}) $. Тогда, предлагаемый алгоритм NetMF применяет SVD к матрице $\log \textbf{M}$, а оптимальные значения эмбедингов - это, также как и в предыдущем пункте, корни произведения синуглярного числа на соответвующий сингулярный вектор.   

\end{itemize}

\begin{table}[ph]
\caption{\label{tab:methods}Методы неконтролируемого обучения}
\setlength{\extrarowheight}{10pt}
\begin{tabular}{|p{60pt}|p{60pt}|p{180pt}|p{90pt}|}
\hline
Метод & Контекстный эмбединг  & Функция потерь & Подходы к оптимизации \\
\hline
 Laplacian EigenMaps & $\times$  & $ \frac{1}{2} \sum_{i,j} |\Phi_i - \Phi_j|^2 A_{i,j} \newline = tr(\Phi^TL\Phi)$ & Matrix Factorization\\
\hline
 Graph Factorization & $\times$  & $\frac{1}{2} \sum_{i,j} ( A_{i,j} - \Phi_i\cdot\Phi_j )^2\newline + \frac{\lambda}{2}\sum_i ||\Phi_i||^2$ & SGD with  asynchronous optimization\\
\hline
 HOPE &   \checkmark &$||\textbf{C} - \Phi \cdot\Theta||_F^2$, $C-$ матрица схожести вершин, см. уравнения \ref{eqn:HOPEKatz}, \ref{eqn:HOPERPR}, \ref{eqn:HOPECommonNeigbor} & Matrix Factorization\\
\hline
NetMF &      \checkmark  & $|| \log (\frac{vol(G)}{bT}(\sum_{r=1}^T \textbf{P}^r)\textbf{D}^{-1}) - \Phi \cdot \Theta||_F^2$\newline (значение параметров см. в описании к ур-ию \ref{eqn:NetMF}) &Matrix Factorization + Negative Sampling\\
\hline
 DeepWalk &   \checkmark  & $- \sum_{v_j \in V} \sum_{v_i \in N(v_j)} log \frac{\exp (\Theta_i \cdot \Phi_j)}{\sum_{v_k \in V} \exp (\Theta_k \cdot \Phi_j) } $, где $N(v)$ -- см. таблицу \ref{tab:notation}. В данном случае случайные блуждания фиксированной длины & Hierarchical softmax \\
\hline
Node2Vec &  \checkmark      & $- \sum_{v_j \in V} \sum_{v_i \in N(v_j)} log \frac{\exp (\Theta_i \cdot \Phi_j)}{\sum_{v_k \in V} \exp (\Theta_k \cdot \Phi_j) } $\newline Случаыные блуждания меют два параметра - вероятноность перехода к вершинам, отвечающим за исследования локальной  и глобальной структур& Negative Sampling \\
\hline
 Struc2Vec&   \checkmark  & $- \sum_{v_j \in V} \sum_{v_i \in N(v_j)} log \frac{\exp (\Theta_i \cdot \Phi_j)}{\sum_{v_k \in V} \exp (\Theta_k \cdot \Phi_j) } $\newline Случайные блуждания строятся по контекстному графу (см ур.-я \ref{eqn:RandomWalkForStruc2Vec},\ref{eqn:RandomWalkForStruc2Vec2}), учитываемому структурную схожесть вершин & Hierarchical Softmax \\
\hline
 Seed & $\times$ &     $ ||X - \hat{X}||_2^2$  & Deep autoencoders\\
\hline
\end{tabular}
\end{table} 

\begin{table}
\setlength{\extrarowheight}{10pt}
\begin{tabular}{|p{60pt}|p{60pt}|p{180pt}|p{90pt}|}
\hline
App &  \checkmark  &  $- \sum_{v_j \in V} \sum_{v_i \in N(v_j)} \log\ \frac{1}{1+ exp(-\Theta_i \cdot \Phi_j)}$\newline для построения случайных последовательностей испольузется метод Monte-Carlo End-Point &Skip-Gram with Negative Sampling   \\
\hline
 VERSE &  $\times$  & $-\sum_{i,j=1}^{n} sim_G(v_i,v_j) \log\frac{\exp (\Phi_i \cdot \Phi_j)}{\sum_{k=1}^n \exp (\Phi_i \cdot \Phi_k)}$ \newline $sim_G$ -- распределение схожести вершин графа. В случае PPR, $sim_G(v,\cdot)$ -- последняя вершина в одном случайном блуждании начатого с вершины v, в случаях других мер схожести, определяется уравнениями \ref{SR}, \ref{ADJ} &Noise Constractive Estimations. Negative Sampling  \\
\hline
GraphSAGE &  $\times$  & $- \sum_{v_j \in V} \sum_{v_i \in N(v_j)} \log  \frac{1}{1+ exp(-\Phi_i \cdot \Phi_j)}$   &Negative Sampling  \\
\hline
SDNE  &  $\times$ &    $|| (\hat{A} - A) \odot B||_F^2 + \alpha \sum_{v_i,v_j \in V} a_{i,j}||\Phi_i - \Phi_j||_2^2 $&  Deep autoencoders\\
\hline
 LINE-1 & $\times$  & $- \sum_{v_i,v_j\in V} a_{ij}\log  \frac{1}{1+ exp(-\Phi_i \cdot \Phi_j)}$& Negative Sampling \\
\hline
 LINE-2 & \checkmark & $ - \sum_{v_i,v_j\in V} a_{ij}\log \frac{\exp (\Phi_i \cdot \Theta_j)}{\sum_{v_k \in V} \exp (\Phi_i \cdot \Theta_k)}$& Negative Sampling\\
\hline
\end{tabular}
\end{table} 

%Ou et al. [33] utilize sparse generalized SVD to generate a graph embedding, HOPE, from a similarity matrix amenable to decomposition into two sparse proximity matrices. 
%Матрица S - является матрицей сходства. Она раскладыаается на две матрицы , одна из них это композиция матрицы смежности и степеней, а другая - единичная, есть еще вариан когда считаются через матрицы вероятностей перехода.  В общем, у таких двух матриц считаются с.з, а потом их используют для того чтоб построить эмбединги. 



%линейное снижение размерности значит что переход в линейное пространство и исопльзование линейного преобразования 
\begin{itemize}
\item Функция потерь в SDNE \cite{SDNE} состоит из двух частей, каждая из которых отвечает за близость между вершинами 1 и 2 порядка соответсвенно.

Близость первого порядка - локальная близость между двумя вершинами. Чем больше вес между вершинами, тем ближе вершины.

Близость второго порядка - схожесть между структурой соседства двух вершин. Математически: $p_u = (a_{u,1}, ... , a_{u,|V|}) - $ определяет близость первого порядка для вершины $u$ ко всем остальным. Тогда близость второго порядка между вершинами $u,v$ определяется схожестью между $p_v,p_u$

Во-первых, в данном методе строится автоэнкодер и в качестве учета близости второго порядка используют ошибку реконструкции матрицы смежности. В оригинальной статье используются б\textbf{о}льшие штрафы для не нулевых элементов, чем для нулевых добавлением умножения на некоторые смещения $\textbf{b}_i$ (т.е. ненулевые элементы должны иметь меньшую ошибку реконструкции, чем нулевые):
\begin{equation}
\mathcal{L}_{2nd} = ||(\hat{A} - A) \odot B||_F^2
\end{equation}
где $\odot$ -- произведение Адамара, $B$--матрицы смещений (biases).

Для того, чтобы учитывать локальную структуру (близости 1 порядка) строится функция потерь на основе Laplacian Eigenmaps -- похожие вершины в графе, в пространтсве эмбедингов также должны быть ближе:

\begin{equation}
\mathcal{L}_{1st} = \sum_{i,j=1}^n a_{i,j} ||\Phi_i - \Phi_j||_2^2
\end{equation}

Общая функция потерь выглядит следующим образом: 
\begin{multline}
\mathcal{L}_{mix} = \mathcal{L}_{2nd} + \alpha\mathcal{L}_{1st}  =\\
=|| (\hat{A} - A) \odot B||_F^2 + \alpha \sum_{i,j=1}^n a_{i,j}||\Phi_i - \Phi_j||_2^2 
\end{multline}
$\alpha$ - параметр, контролирующий отношение степеней важности локальной структуры к глобальной.


\item В качестве еще одного метода, использующего автоэнкодер рассмотрим метод $SEED$ \cite{SEED}, ориентированного на построения эмбедингов графов, а не вершин.  Обучает автоэнкодер на случайных блужданиях типа WEAVE (каждый подграф представляется в виде матрицы $X$, где каждый столбец $p$ представляет одну вершину в виде конкатенации атрибуата этой вершины и наиболее раннего визита в ходе $p$-го блуждания). Обучение автоэнкодера происходит минимизируя ошибку реконструкции $ \hat{X}$:
\begin{equation}
\mathcal{L} = ||X - \hat{X}||_2^2
\end{equation}

На последнем шаге усредненяются представления подграфов, или, если обобщать, то отображаются полученные представления $\Phi$ в некоторое новое пространство и усредняют полученные представления:
\begin{equation}
\hat{\mu}_G = \frac{1}{s} \sum_{i=1}^s \phi(\Phi_i)
\end{equation}

\item Теперь можно приступить к рассмотрению методов, основанных на случайных блужданиях. Эти методы считают в качестве меры схожести двух вершин - вероятность появления их во время одного случайного блуждания. 

Итак, сначала запускается $\gamma$ случайных блужданий, каждое длинной $t$. Выбирается размер окна $w$. Для каждой вершины соседями считаются другие вершины, попавшие в окно размера $w$. Таким образом строится множество соседей для каждой вершины.  И затем минимизируется:

\begin{equation}
\mathcal{L} = \sum_{v_j \in V} \sum_{v_i \in N(v_j)} - log(P(v_i|\Phi_j))
\end{equation}

Самый основной и первый метод подонбого рода в графовых нейронных сетях, DeepWalk, \cite{Deepwalk} использует случайные блуждания фиксированной длины.

\item Node2vec \cite{node2vec} в отличие от DeepWalk исследует и локальную и глобальную структуру графа и с помощью двух гиперапараметров может учитывать влияние каждой из сторон: один гиперпараметр отвечает за вероятность агента во время случайных блужданий вернуться на шаг назад, т.е. за учет локальной структуры графа, а другой гиперпараметр, отвечает за вероятность агента сделать шаг в сторону от вершины, с которой началось блуждание, таким образом, учитывая глобальную структуру графа. Таким образом, в Node2Vec $N(v)$ отличается от DeepWalk из-за самого характера случайного блуждания.
\item Struc2vec \cite{struc2vec} рассчитывает расстояние между вершинами по структурной схожести, а не топологической и является немного более сложным чем DeepWalk и Node2Vec за счет построения нового графа, называемого контекстным. 
\begin{enumerate}
\item Мера структурной близости между двумя вершинами, учитывающими сосдество размера $k$ (соседи, расстояние до который менее или равно $k$ рёбер) -- это разница в степени вершины и степеней ее соседей:
\begin{equation}
f_k(u,v) = f_{k-1}(u,v) + g(s(R_k(u),s(R_k(v)))
\end{equation}
где $R_k(u)$ -- множество соседей вершины $u$ строго на расстоянии $k$, $s(R)$ -- отсортированная последовательность степеней каждой вершины из $R$.  $g(R_1, R_2)$ -- расстояние между двумя отсортированными последовательностями чисел.

\item На данном этапе строится новый, взвешенный, многослойный, так называемый контекстный, граф. Каждый слой $k$ отвечает за соседство вершин в радиусе $k$. В каждом слое $k$ находятся все вершины, а ребра между вершинами внутри слоя имеют вес, обратно пропорциональный структурной схожести между этими вершинами:
\begin{equation}
w_k(u,v) = \exp^{-f_k(u,v)}
\end{equation}
Между слоями ребра соединяют лишь одни и те же вершины. Вес между ребром от $u_k$ к $u_{k+1}$ равен логарифму суммы ребер, инцидентных $u_k$ и имеющих вес больший, чем средний по данному слою.

\item Далее по этому контекстному графу строится несколько случайных блужданий фиксированной длины, начиная с нулевого слоя. Вероятность перехода к вершине внутри слоя равна:
\begin{equation}\label{eqn:RandomWalkForStruc2Vec}
p_k(u,v) = \frac{\exp^{-f_k(u,v)}}{\sum_{l \neq u, l \in V} \exp^{-f_k(u,l)}}
\end{equation} 
А вероятность перейти на следующий слой:

\begin{equation}\label{eqn:RandomWalkForStruc2Vec2}
\begin{aligned}
	p_k(u_k,u_{k+1}) &= \frac{w(u_k,u_{k+1})}{w(u_k,u_{k+1}) + w(u_k,u_{k-1})},\\
	p_k(u_k,u_{k-1}) & =1 - p_k(u_k,u_{k+1})
\end{aligned}
\end{equation}
\item Наконец, как и в DeepWalk, Node2vec, для построения представлений каждой вершины используется Skip-Gram, который максимизирует вероятность появления контекста вершины (контекст вершины задается окном размера w, центрированного на вершине в последовталеьности случайного блуждания, построенного на предыдущем шаге)
 
\end{enumerate}

\item VERSE \cite{VERSE} вводит распределения схожести между вершинами и минимизируют расхождение Кульбака - Лейблера (KL) между  распределением схожести вершин в графе $sim_G$ и распределением схожести эмбедингов $sim_E$:
\begin{equation}
\sum_{v \in V} KL(sim_G(v,\cdot) || sim_E(v,\cdot))
\end{equation}

В качестве распределения схожести вершин в пространстве эмбеддингов используется нормализованное с помощью softmax скалярное произведение эмбеддингов:
 \begin{equation}
sim_E(v,\cdot) = \frac{\exp{\Phi_v \Phi^{T}}}{\sum_{i=1}^n \exp(\Phi_v \Phi_i)}
\end{equation}


Также как и HOPE использует различные меры для распределения схожести в графе (PPR, SimRank, Adjacency similarity):

\begin{enumerate}
\item Personalized PageRank. 
По оеределению: 
\begin{equation}
 ppr_v(t+1) = \alpha\cdot ppr_v(t)\textbf{P} + (1-\alpha) s,
 \end{equation}
где $s$ --- начальное распределение. В случае рассмотрения вершины $v$, $s$ --- это вектор длины $n$, со всеми элементами равными нулю кроме одного, равного 1 на месте, соответвующем вершине $v$. 
Тогда, в пределе $\lim_{t \rightarrow \infty} ppr_v(t) = sim_G(v,\cdot)$, и как уже было посчитано в пункте про метод HOPE, один экзамепляр $sim_G(v,\cdot)$ это последняя вершина в одном бесконечном случайном блуждании с вероятностью рестарта $1-\alpha$, начатом из вершины $v$.
\item SimRank: мера структурной взаимосвязи двух вершин, основана на предположении, что схожие вершины связаны с дргуими схожими вершинами. Определяется рекурсивно:
\begin{equation}\label{SR}
sim_G^{SR} = \frac{C}{|I(u)| |I(v)|} \sum_{i=1}^{|I(u)|} \sum_{j=1}^{|I(v)|} sim_G^{SR}(I_i(u),I_j(v))
\end{equation}
$I(v)$ -- множество соседей вершины $v$, с ребрами, входящими в $v$, $C$ -- число между 0 и 1, геометрически обесценивает важность дальних узлов. 
\item Adjacency similarity: если $Out(u)$ -- степень выходящих ребер из вершины u, то 
\begin{equation}\label{ADJ}
sim_G^{ADJ}(u,v) = \begin{cases}
   1/Out(u) & \text{если} (u,v) \in E\\
   0 & \text{иначе}
 \end{cases}
\end{equation}

\end{enumerate}

Рассмотрим подробнее функцию потерь:

По определению расстояния Кульбака-Лейбнера для дискретных распределений:
\begin{equation}
KL(P||Q) = \sum_{i=1}^n p_i \log\frac{p_i}{q_i}
\end{equation}

В случае с распределениями схожести в графе и прстранстве эмбеддингов, получаем:
\begin{multline}
KL(sim_G(v,\cdot)||sim_E(v,\cdot))= \sum_{i=1}^n sim_G(v,\cdot)\log\frac{sim_G(v,i)}{sim_E(v,i)} = \\
= \sum_{i=1}^n sim_G(v,i)\log sim_G(v,i) - \sum_{i=1}^n sim_G(v,i)\log sim_E(v,i)
\end{multline}
Но первое слагаемое, включает в себя только распределение схожести в графе, что не влияет на минимизацию, так как задается изначально. Поэтому,  в качестве функции потерь остается только:

\begin{equation}\label{eqn:LossVERSE}
\mathcal{L} = - \sum_{v \in V} sim_G(v,\cdot) \log \frac{\exp{\Phi_v \Phi^{T}}}{\sum_{i=1}^n \exp(\Phi_v \Phi_i)}
\end{equation}
Для оптимизации используется Noise Contrastive Estimation (NCE): алгоритм строит классификатор (логистическую регрессию), который разделяет вершины из настоящего распределения $sim_G(v,\cdot)$ и распределения шума $Q$. Производная от NCE с увеличением негативных примеров, сходится  градиенту функции (\ref{eqn:LossVERSE}).

В отличие от HOPE требует на вход не обязательно весь граф из-за чего может быть использован на больших графах.  

\item LINE \cite{LINE} Явно задает две функции, отвечающие за близость 1 и 2 порядка, а затем минимизирует функцию - комбинацию из двух. 

Так же, как и в VERSE, задаются два распределения совместных вероятностей двух вершин и эмбедингов: одно, $\hat{p}$ - используя матрицу смежности, другое, $p$ - для эмбедингов. Например для случая близости 1 порядка: 
\begin{equation}
p_1(v_i,v_j) = \frac{1}{1+ exp(-\Phi_i \cdot \Phi_j)}
\end{equation}
\begin{equation}
\hat{p_1}(v_i,v_j) = \frac{a_{ij}}{\sum_{i,j \in V} a_{ij}}
\end{equation}

А затем минимизируют расхождение Кульбака - Лейблера (KL) между  распределениями:

\begin{equation}
\mathcal{L} = KL(\hat{p_1},p_1) =  \sum_{i,j} \hat{p_1}_{ij} \log \frac{\hat{p_1}_{ij}}{p_{1_{ij}}} = \sum_{i,j} \hat{p_1}_{ij} \log \hat{p_1}_{ij} - \sum_{i,j} \hat{p_1}_{ij} \log p_{1_{ij}} 
\end{equation}
так как на результат минимизации функции не влияют константы, то окончательно минимизируется следующая функция: 
\begin{equation}
\mathcal{L} = - \sum_{i,j\in V} a_{ij}\log  \frac{1}{1+ exp(-\Phi_i \cdot \Phi_j)}
\end{equation}

В случае сохранения близости второго порядка:

\begin{equation}
p_2(v_j|v_i) = \frac{\exp (\Phi_i \cdot \Theta_j)}{\sum_{k \in V} \exp (\Phi_i \cdot \Theta_k)}
\end{equation}
\begin{equation}
\hat{p}_2(v_j|v_i) = \frac{a_{ij}}{d_i}, 
\end{equation}
где $d_i = \sum_{k \in N(i)} a_{ik} - $ степень вершины

Минимизируется следующая функция:

\begin{equation}
\sum_{i \in V} \lambda_i KL(\hat{p}_2 (\cdot | v_i), p_2(\cdot|v_i))
\end{equation}
$\lambda $ вводится для того чтоб показать важность каждой вершины в графе, которая может быть определена как степень вершины или по алгоритмам PageRank. Для простоты в оригинальной статье $\lambda_i$ был положен как $d_i$. Тогда, как и в случае LINE-1, не учитывая константы, фукнция минимизации выглядит следующим образом:
\begin{equation}
\mathcal{L} = - \sum_{i,j\in V} a_{ij}\log \frac{\exp (\Phi_i \cdot \Theta_j)}{\sum_{k \in V} \exp (\Phi_i \cdot \Theta_k)}
\end{equation}

\item APP  \cite{APP} в качестве меры схожести тоже считает вероятность оказаться в одной вершине, в ходе случайного блуждания бесконечной длины с вероятностью рестарта $\alpha$ , начатого из другой. Но используется некоторая аппроксимация данного варианта с помощью метода сэмплировния Монте-Карло End-Point. Рандомно выбирются пути, начинающиеся в одной вершине с вероятностью остановки $\alpha$ и заканчивающийся в другой. По аналогии с DeepWalk, Node2Vec, каждый путь рассматривается как направленная последовательность, но в которой пары вершин рассматриваются только по прямому направлению. Таким образом и учитывается несимметричность. Затем оптимизируется следующая функция потерь, учитывающая Negative Sampling:
 
\begin{equation}
\mathcal{L} = \sum_j \sum_i \#Sampled_j(i) \cdot(\log\sigma(\Phi_j\cdot\Theta_i) + b \cdot E_{t_n \sim P_D}[\log\sigma(-\Phi_j\cdot\Theta_n)])
\end{equation}
где  $\#Sampled_j(i) - $ количество путей $(j,i)$, $b$ -- количество негативных примеров, берущихся из распределения $P_D$, $\sigma$  в анном слуачае сигмоида.
\end{itemize}

Все вышеперечисленые функции потерь можно разделить на некоторые основные виды, эти обощения представлены в таблице \ref{tab:LossFunctions}
\begin{table}[ph]
\caption{\label{tab:LossFunctions}Общй вид функций потерь}
\setlength{\extrarowheight}{10pt}
\begin{tabular}{|p{60pt}|p{190pt}|p{110pt}|}
\hline
id & Общий вид & Методы  \\
\hline
 1 & $-\sum_{i=1}^{n} \sum_{j=1}^{n}c_{i,j} \log\frac{\exp (\Phi_i \cdot \Theta_j)}{\sum_{k=1}^n \exp (\Phi_i \cdot \Theta_k)}$ & VERSE,LINE\\
 \hline
 2 & $-\sum_{i=1}^{n} \sum_{j: v_j \in N(v_i)}c_{i,j} \log\frac{\exp (\Phi_i \cdot \Theta_j)}{\sum_{k=1}^n \exp (\Phi_i \cdot \Theta_k)}$ & DeepWalk,Node2Vec,\newline Struc2Vec,APP,\newline GraphSage\\
 \hline
 3 & $||C-\Phi \cdot \Theta||^2$ & HOPE,NetMF, Graph Factorization\\
 \hline
 4 & $tr(\Phi^T L \Phi) = \frac{1}{2} \sum_{i=1}^{n}\sum_{j=1}^{n}|\Phi_i - \Phi_j|^2 a_{ij}$ & LaplacianEigenMaps, SDNE-1\\
 \hline
 5 & Ошибки реконструкции (О.Р.)\newline $|| \hat{M} -M ||$& Seed (О.Р. матрицы фичей M= X), SDNE-2 (О.Р. матрицы смежности M= A)\\
 \hline
\end{tabular}
\end{table}

 \begin{center}
{\subsection{Обобщения}\label{section:generalization}}
\end{center}

В таблице \ref{tab:LossFunctions} представлен общий вид различных функций потерь. Можно заметить несколько схожестей: 
\begin{itemize}
\item В методе NetMF рассказано как можно перевести функции потерь вида 2) в вид 3)
\item Кроме этого, с помощью матрицы перехода, уже встречавшейся в обзоре, $\textbf{P} = \textbf{D}^{-1}\textbf{A}$ можно представить функцию потерь вида 2) в виде 1)

\end{itemize}

Рассмотрим эти два утверждения подробнее:

\begin{itemize}
\item Во-первых, описание метода NeMF общими словами:
\begin{enumerate}
\item В изначальной функции потерь делаются несколько замен обозначений  
\item Так как функция потерь минимизируется по $\Phi^{T}\Theta$, то берется производная от $\mathcal{L}$ по $\Phi^{T} \Theta$ и приравнивается к нулю
\item Ищутся корни. Итого находят выражение для $\Phi^{T} \Theta$. Обозначают для краткости $\Phi^{T} \Theta = \log (\textbf{M})$ 
\item Но теперь стоит задача найти сами эмбеддинги, если известно их произведение
\item В явном виде сложно найти, тогда минимизируется $||\log (\textbf{M}) - \Phi^{T}\Theta||$
\item А решение данной минимизации будет SVD на $\log (\textbf{M})$.
\item  $\log (\textbf{M})= U_d \Sigma_d V_d^{T}$. 
\item Оптимальные значения эмбедингов $\Phi = U_d \sqrt{\Sigma_d}$
\end{enumerate} 
Например алгоритм LINE эквивалентен факторизации матрицы:
\begin{equation}\label{eqn:NetMF_LINE}
\log (vol(G)D^{-1}AD) - \log(b)
\end{equation}
А алгоритм DeepWalk экивалентен факторизации следующей матрицы:
\begin{equation}\label{eqn:NetMF}
\log (\frac{vol(G)}{T}(\sum_{r=1}^T \textbf{P}^r)\textbf{D}^{-1}) - \log(b)
\end{equation}
в данном случае $b$ негативных примеров для Negative Sampling, vol(G) - объем взвешенного графа (сумма степеней всех вершин), T - длина случайного блуждания, r - размер окна в DeepWalk.

\item Во-вторых, в методе DeepWalk важно количество пар $(i,j)$ ($i$--целевая, $j$--контекст) внутри окон всех запущенных случайных блужданий. Рассмотрим упрощенный вариант, когда длина случайного блуждания равна длине окна $2 w+1$. Если окно имеет длину w, то вероятность появления пары (i,j) считается как объединение событий: $A_{1} \cup$ ... $\cup A_{w} \cup A_{-1}$... $\cup A_{-w}$, где $A_{l}$ --- событие того, что случайным блужданием из $i$ в $j$ попали за l шагов. Тогда, вероятность внутри одного блуждания появления пары $(i,j)$ равна 
\begin{equation}
Pr(i,j) = Pr(A_{-w}\cup ...\cup A_{-1}\cup A_{1} ...\cup A_{w}) = \sum_{r=1}^w (\textbf{P}^r (i,j) + \textbf{P}^r (j,i) )
\end{equation}
То есть, если от вершины $i$ было запущено $\gamma$ случайных блужданий, то пар (i,j) будет $\gamma \sum_{r=1}^w (\textbf{P}^r (i,j) + \textbf{P}^r (j,i) )$ штук, или в матричном виде:
$\gamma \sum_{r=1}^w (\textbf{P}^r  + \textbf{P$^\text{T}$}^r )$ 
А т.к. каждый элемент матрицы \textbf{P} больше нуля и меньше единицы, то применим формулу убывающей геометрической прогрессии:

\begin{equation}
\textbf{C} = \gamma \sum_{r=1}^w (\textbf{P}^r  + \textbf{P$^\text{T}$}^r ) = \gamma ( \textbf{P}(\textbf{I}-\textbf{P}^w)(\textbf{I}-\textbf{P})^{-1} + \textbf{P$^\text{T}$}(\textbf{I}-\textbf{P$^\text{T}$}^w)(\textbf{I}-\textbf{P$^\text{T}$})^{-1})
\end{equation}

За $\textbf{C}$ мы обозначили так называемую котекстную матрицу. Теперь, 
\begin{equation}
\sum_{i,j} c_{ij} \log(\frac{\exp(\Phi_i \cdot \Theta_j)}{\sum_{k\in V} (\exp(\Phi_i \cdot \Theta_k)) })
\end{equation}
Эквивалентно функции потерь DeepWalk для случая, когда длина окна равна длине случайных блужданий.
\end{itemize}

 \begin{center}
{\subsection{Варианты энкодеров}\label{section:Encoders}}
\end{center}

Довольно распространены и популярны среди методов глубокого обучения на графах методы GCN (Graph Convolutional Network) \cite{GCN} и GAN (Graph Attention Network) \cite{GAN}. Однако в оригинальном виде они используют в качестве функции потерь cross-entropy, которая считается для режима обучения с учителем (необходимы размеченные данные).  Тем не менее можно рассмотреть только части, касающиеся неконтролируемого этапа - построения отоюражающей функции. 

\begin{itemize}
\item GCN \cite{GCN}: в качестве агрегирующей функции выбиратся взвешенная сумма по соседям, и эмбеддинг самой вершины на прошлом слое тоже прибавляет:
\begin{equation}
\begin{aligned}
\textbf{h}_v^0 &= \textbf{x}_v   \\
 \textbf{h}_v^k & = \sigma (\textbf{W}_k \sum_{u \in N(v)} \frac{\textbf{h}_u^{k-1}}{|N(v)|} + \textbf{B}_k \textbf{h}_v^{k-1}), \forall k \in \{1, ... , K \} \\
\Phi_v & = \textbf{h}_v^K
\end{aligned}
\end{equation}

\item GAN \cite{GAN} - Идея в том, что соседи оказывают разное влияние на данную вершину и необходимо это учесть: 
\begin{equation}
 \textbf{h}_v^k = \sigma (\sum_{u \in N(v)} \alpha_{vu} \textbf{W}_k  \textbf{h}_u^{k-1})
\end{equation}

\item CraphSAGE \cite{GraphSAGE} В отличие от предыдущего метода, может использовать разные варианты аггрегирования информации от соседей кроме среднего значения, например, выбор максимального значения или применение LSTM. Кроме того, еще одно отличие от базового варианта это конкатеринирование представления самой вершины с прошлого слоя к агрегированной информации от соседних вершин, вместо суммирования с ней.  %МАСШТАБИРУЕМЫЙ 
Преимущество над предыдущим методом - это обобщение:
\begin{equation}
 \textbf{h}_v^k = \sigma ([\textbf{W}_k \cdot AGG (\{ \textbf{h}_u^{k-1}, \forall u \in N(v) \}), \textbf{B}_k\textbf{h}_v^{k-1} ])
\end{equation}

%SDNE 
\end{itemize}

\textbf{Выводы}: 
\begin{itemize}
\item Традиционные методы построения эмбедингов (снованные на факторизации) используют плотные матрицы, которые характеризуют граф, из-за чего эффективны лишь на небольших графах, а так же учитывают схожесть только по расстоянию между вершинами, а не атрибутам.

\item Методы, основанные на случайных блужданиях учитывают не только ближайших соседей, но и глобальную структуру графа. Работают быстрей, так как используют для вычислений не все пары вешин. 

\item Сверточные методы учитывают только локальную структуру, но зато более вычислительно эффективны и учитывают атрибуты вершин. 

\item Существует ряд методов (VERSE, LINE, HOPE), которые учитывают понятие мер схожести.  
\end{itemize}
%\addcontentsline{toc}{section}{Сравнительный анализ}
\newpage


\newpage
\begin{center}
			{\section{Рекомендации по использованию методов}}
		\end{center}
\textbf{(предполагается, что эти рекомендации мы получим сами как итог - при каких энкодерах какие функции потерь более эффективны для каких задач, а пока тут информация из других статей)}
В работах \cite{comparUnRep} и \cite{EmbedingSurvey} был проведен подробный сравнительный анализ рассматриваемых в данном обзоре методов. 
%прочитать работу \cite{EmbedingSurvey} и выписать еще больше рекомендаций 
 
Основные результаты следующие: 
\begin{enumerate}
\item Методы, учитывающие роль вершины как источника и контекста при изучении представлений, рекомендуются для прогнозирования связей в ориентированных графах. 
%\item Некоторые структурные свойства, такие как коэффициент кластеризации, транзитивность, взаимность и т. д., рекомендуется учитывать при выборе конкретного метода.
\item Простой классификатор, основанный на непосредственном соседстве, предлагает лучшую или сопоставимую производительность для ряда наборов данных.
\item Для задач предсказания связей на неориентированных графах методы на основе PPR ( APP и VERSE) - являются наиболее эффективными методами во всех наборах данных.
\item В задачах предсказания связей, метод LINE, который непосредственно использует матрицу смежности в качестве матрицы близости, превосходит методы случайного блуждания для неориентированных графов. 
\item В задачах предсказания связей, для ориентированных графов с низкой взаимностью повторное представление контекста узла играет большую роль, и для задач предсказания направленных связей следует использовать методы кодирования и использования двух пространств внедрения для исходной и целевой контекстных представлений узлов.
\item В задачах предсказания связей более глубокие модели не имеют значительного преимущества перед поверхностыми. 
%\item Влияние структурных характеристик: для задачи предсказания связей, основнное на PPR построение показывает лучшие результаты и для ненаправленных и для направленных графов. Также, для направленных графов с низкой взаимностью, контекст основанный на Katz similarity показывает лучшие результаты. Смещенные блуждания с другой стороны имеют преимущества в предсказании связей для графов с высоким коэффициентом кластеризации, высокой транзитивностью и высокой взаимностью. Для низких коэффициентов кластеризации и транзитивности LINE-1 показывает намного более хороший результат для предсказания связей. Методы основанные на случайных блужданиях более надежные для задач классицикации вершин, а методы основанные на агрегаторах соседеней показывают лучшие результаты если есть большая схожесть между метками соседей.
%\item Роль контекстного представления: контекстное представление следует явно использовать в предсказаниях направленных связей. Чем меньше взаимность (т.е. чем больше ассимитричность в роли вершин как источников и контекста) в направленных графах, тем более важно контекстное представление. 

%\end{enumerate}
%\newpage
%Сводная таблица 2 показывает как лучше использовать каждый отдельный метод. 
%\begin{figure} [h!]
%	\begin{minipage}{\linewidth}
%	 		\center {\includegraphics[width =1\linewidth]{results.png}} \\ \small{Таблица 2 \cite{comparUnRep}}
 %	\end{minipage}
	% \end{figure}

\item Для направленных графов, для задачи реконструкции графов HOPE предпочитается APP
\item Для задач классификации узлов, степень гомофилии графа должна быть подсчитана прежде чем выбирать подход. Подходы глубокого обучения, основанные на аггрегации подходят для высокой степени гомофилии, а для низкой - DeepWalk.
\item Node2Vec более предпочтителен для класссификации вершин, а для предсказания рёбер - методы, учитывающие разные степени схожести (HOPE,SDNE)
\end{enumerate}

\newpage
%\addcontentsline{toc}{section}{Заключение}
\begin{center}
			{\section{Заключение}}
		\end{center}

Было рассмотрено множество различных методов, которые разделены на общие группы по подходу к обучению представления графов. Методы различаются также и учетом различной информации графов (локальная или глобальная структуры, схожесть по атрибутам вершин). Основным наиболее эффективным подходом остается построение эмбедингов на основе нейронных сверточных сетей, хотя и другие методы показывают преимущества на конкретных задачах. При решении задач и выборе метода, необходимо учитывать структурные свойства графов, размер графа, тип задач. 

Также был сделан акцент на экспериментальную часть всех методов и основная информация по наиболее часто использованным датасетам и метрикам собрана в таблицах \ref{tab:datasets}, \ref{tab:metrics}
 
Основное будущее развитие всех методов это масштабирование на большие графы, а также улучшения методов, направленные на улучшение качества и скорости работы алгоритмов. 

\newpage
\addcontentsline{toc}{section}{Список использованных источников}
\def \thebibliographyname{Список использованных источников}
\begin{thebibliography}{50}

\bibitem{Statistics} S. Bhagat, G. Cormode, and S. Muthukrishnan. Node classification in social networks. In Social Network Data Analytics, pages 115–148. 2011.
\bibitem{linkPred}  M. E. J. Newman. The structure of scientific collaboration networks. Proceedings of the
National Academy of Sciences, 98:404–409, 2001.

\bibitem{GNN} Z. Wu, S. Pan, F. Chen, G. Long,C. Zhang, S. Yu Philip. 2019. A comprehensive survey on graph neural networks. arXiv:1901.00596.
\bibitem{EmbedingSurvey} P. Goyal and E. Ferrara, “Graph embedding techniques, applications, and performance: A survey,” Knowledge-Based Syst., vol. 151, 2018, doi: 10.1016/j.knosys.2018.03.022.
\bibitem{eigenmaps} Mikhail Belkin and Partha Niyogi. 2001. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS. 585–591

\bibitem{Sam}Sam T Roweis and Lawrence K Saul. 2000. Nonlinear dimensionality reduction by locally linear embedding. science 290, 5500 (2000), 2323–2326. 

\bibitem{reduction} Joshua B Tenenbaum, Vin De Silva, and John C Langford. 2000. A global geometric framework for nonlinear dimensionality reduction. science 290, 5500 (2000), 2319– 2323
\bibitem{reduction2} Shuicheng Yan, Dong Xu, Benyu Zhang, Hong-Jiang Zhang, Qiang Yang, and Stephen Lin. 2007. Graph embedding and extensions: A general framework for dimensionality reduction. TPAMI 29, 1 (2007)

\bibitem{factorization} Amr Ahmed, Nino Shervashidze, Shravan Narayanamurthy, Vanja Josifovski, and Alexander J Smola. 2013. Distributed large-scale natural graph factorization. In WWW. ACM, 37–48.
\bibitem{HOPE} Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, and Wenwu Zhu. 2016. Asymmetric transitivity preserving graph embedding. In KDD. 1105–1114
\bibitem{VERSE} A. Tsitsulin, D. Mottin, P. Karras, and E. Müller, “VERSE: Versatile graph embeddings from similarity measures,” Web Conf. 2018 - Proc. World Wide Web Conf. WWW 2018, pp. 539–548, 2018, doi: 10.1145/3178876.3186120.
\bibitem{ARCTE}  G. Rizos, S. Papadopoulos, and Y. Kompatsiaris, Multilabel user classification using the community structure of online networks, vol. 12, no. 3. 2017.

 \bibitem{Deepwalk} B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: online learning of social representations,” in KDD, 2014, pp. 701–710.
\bibitem{skipgram} Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Distributed Representations ofWords and Phrases and their Composition- ality. In NIPS. 3111–3119
\bibitem{GraRep} Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2015. GraRep: Learning Graph Representations with Global Structural Information. In CIKM. 891–900
\bibitem{struc2vec} Leonardo FR Ribeiro, Pedro HP Saverese, and Daniel R Figueiredo. 2017. struc2vec: Learning node representations from structural identity. In KDD. ACM, 385–394
\bibitem{LINE} J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “LINE: large-scale information network embedding,” in WWW, 2015, pp. 1067–1077.
\bibitem{node2vec} A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” in KDD, 2016, pp. 855–864.
\bibitem{SEED} L. Wang et al., “INDUCTIVE AND UNSUPERVISED REPRESENTATION LEARNING ON GRAPH STRUCTURED OBJECTS,” 2016.
\bibitem{TransE} A. Bordes, N. Usunier, A. Garcia-dur, J. Weston, and O. Yakhnenko, “Translating Embeddings for Modeling Multi-relational Data,” pp. 1–9.
\bibitem{GraphSAGE}  W. L. Hamilton, R. Ying, and J. Leskovec, “Inductive representation learning on large graphs,” Adv. Neural Inf. Process. Syst., vol. 2017-Decem, no. Nips, pp. 1025–1035, 2017.
\bibitem{GAN} P. Velickovic, A. Casanova, P. Liò, G. Cucurull, A. Romero, and Y. Bengio, “Graph attention networks,” 6th Int. Conf. Learn. Represent. ICLR 2018 - Conf. Track Proc., pp. 1–12, 2018.
\bibitem{SDNE} D. Wang, P. Cui, W. Zhu, Structural deep network embedding, in:  Pro-ceedings of the 22nd International Conference on Knowledge Discoveryand Data Mining, ACM, 2016, pp. 1225–1234.
\bibitem{comparUnRep} M. Khosla, V. Setty, and A. Anand, “A Comparative Study for Unsupervised Network Representation Learning,” IEEE Trans. Knowl. Data Eng., pp. 1–1, 2019, doi: 10.1109/tkde.2019.2951398.
\bibitem{APP} C. Zhou, Y. Liu, X. Liu, Z. Liu, and J. Gao. Scalable graph embedding for asymmetric proximity. In AAAI, pages 2942–2948, 2017.
\bibitem{SDNE} D.Wang, P. Cui, andW. Zhu. Structural deep network embedding. In SIGKDD, pages 1225–1234. ACM, 2016.
\bibitem{NETMF} J. Qiu, Y. Dong, H. Ma, J. Li, K. Wang, and J. Tang. Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec. In WSDM, pages 459–467, 2018.
%\bibitem{GMN} Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli. Graph matching net- works for learning the similarity of graph structured objects. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings ofInternational Conference on Machine Learning, volume 97, pp. 3835–3845, 09–15 Jun 2019
%\bibitem{GIN} Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In Proceedings ofInternational Conference on Learning Representations, 2019
%\bibitem{RWModMax} Devooght R, Mantrach A, Kivima¨ki I, Bersini H, Jaimes A, Saerens M. Random walks based modularity: application to semi-supervised learning. In: Proceedings of the 23rd international conference on World wide web. Seoul, Republic of Korea: International World Wide Web Conferences Steering Committee; 2014. p. 213–224.
%\bibitem{RepEig} Smith LM, Lerman K, Garcia-Cardona C, Percus AG, Ghosh R. Spectral clustering with epidemic diffu- sion. Physical Review E. 2013; 88(4):042813.
%\bibitem{Louvain} Blondel VD, Guillaume JL, Lambiotte R, Lefebvre E. Fast unfolding of communitie 
%\bibitem{EdgeCluster} Tang L, Liu H. Scalable learning of collective behavior based on sparse social dimensions. In: Proceed- ings of the 18th ACM conference on Information and knowledge management. Hong Kong, China: ACM; 2009. p. 1107–1116.
21.
%\bibitem{MROC}  Wang X, Tang L, Liu H, Wang L. Learning with multi-resolution overlapping communities. Knowledge and information systems. 2013; 36(2):517–535
%\bibitem{BigClam} Yang J, Leskovec J. Overlapping community detection at scale: a nonnegative matrix factorization approach. In: Proceedings of the sixth ACM international conference on Web search and data mining. Rome, Italy: ACM; 2013. p. 587–596.
56.
%\bibitem{OSLOM} Lancichinetti A, Radicchi F, Ramasco JJ, Fortunato S. Finding statistically significant communities in networks. PloS one. 2011; 6(4):e18961.

%\bibitem{Chenb} Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: Fast learning with graph convolutional networks via importance sampling. In International Conference on Learning Representations (ICLR), 2018b
\bibitem{GCN} Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. CoRR, abs/1609.02907, 2016. URL http://arxiv.org/abs/1609.02907.
%\bibitem{Yinga}  Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Proceedings ofthe 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 18, 2018a. ISBN %34
%\bibitem{Chena} Jianfei Chen, Jun Zhu, and Le Song. Stochastic training of graph convolutional networks with variance reduction. In ICML, pp. 941–949, 2018a
%\bibitem{Gao} Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional networks. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’18, pp. 1416–1424, New York, NY, USA, 2018. ACM. ISBN 978-1-4503-5552-0.
%\bibitem{Huang} Wenbing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. Adaptive sampling towards fast graph representation learning. In Advances in Neural Information Processing Systems, pp. 4558–4567, 2018.

%\bibitem{Zeng} Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor K. Prasanna. Accurate, efficient and scalable graph embedding. CoRR, abs/1810.11899, 2018. URL http: //arxiv.org/abs/1810.11899
%\bibitem{Chiang} Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An ef- ficient algorithm for training deep and large graph convolutional networks. CoRR, abs/1905.07953, 2019. URL http://arxiv.org/abs/1905.07953

%\bibitem{Zhang} Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan: Gated attention networks for learning on large and spatiotemporal graphs. arXiv preprint arXiv:1803.07294, 2018
%\bibitem{Lu}  Haonan Lu, Seth H. Huang, Tian Ye, and Xiuyan Guo. Graph star net for generalized multi-task learning. CoRR, abs/1906.12330, 2019. URL http://arxiv.org/abs/1906.12330
%\bibitem{Klicpera} Johannes Klicpera, Aleksandar Bojchevski, and Stephan Gunnemann. Personalized embedding propa- gation: Combining neural networks on graphs with personalized pagerank. CoRR, abs/1810.05997, 2018. URL http://arxiv.org/abs/1810.05997

%\bibitem{He} Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385
%\bibitem{Xu2018} Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. arXiv preprint arXiv:1806.03536, 2018
%\bibitem{GraphSAINT} H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V. Prasanna, “GraphSAINT: Graph Sampling Based Inductive Learning Method,” no. 2018, 2019, [Online]. Available: http://arxiv.org/abs/1907.04931..

%\bibitem{Survey on TL} S. Pan, Q. Yang. 2010. A survey on transfer learning. \textit{IEEE Transactions on Knowledge and Data Engineering} 22(10):1345–1359.
%\bibitem{SR2LR} Mihalkova, Lilyana and Mooney, Raymond J. 2009. Transfer learning from minimal target data by mapping across relational domains. \textit{Twenty-First International Joint Conference on Artificial Intelligence}. 
%\bibitem{Getoor and Taskar}  L. Getoor, B. Taskar, editors. Introduction to Statistical Relational Learning. MIT Press, Cambridge, MA, 2007.

%\bibitem{Richardson and Domingos} M. Richardson and P. Domingos. Markov logic networks. Machine Learning, 62:107–136, 2006.
%\bibitem{Mihalkova et al} L. Mihalkova, T. Huynh, and R. J. Mooney. Mapping and revising Markov logic networks for transfer learning. (AAAI-07).
%\bibitem{Davis and Domingos} J. Davis, P. Domingos. Deep transfer via second-order markov logic. \textit{In Proceedings of the AAAI Workshop on Transfer Learning For Complex Tasks}. 2008
%\bibitem{Forbus and Oblinger}  Kenneth D. Forbus , Dan Oblinger. Making SME greedy and pragmatic. (CogSci-90).
%\bibitem{Intristic} J. Lee, H. Kim, J. Lee, and S. Yoon, “Transfer learning for deep learningon graph-structured data.” in AAAI, 2017, pp. 2154–2160 
%\bibitem{Sonawane}  Sonawane, S., and Kulkarni, P. 2014. Graph based representation and analysis of text document: A survey of techniques. \textit{International Journal of Computer Applications} 96(19).
%\bibitem{Henaff} Henaff, M.; Bruna, J.; and LeCun, Y. 2015. Deep convolutional networks on graph - structured data. arXiv:1506.05163.
%\bibitem{AdaGCN} Quanyu Dai, Xiao Shen, Xiao-Ming Wu, Dan Wang. 2019. Network Transfer Learning via Adversarial Domain Adaptation with Graph Convolution. arXiv:1909.01541(2019)



%\bibitem{Salakhutdiniv} Z. Yang, W. W. Cohen, and R. Salakhutdinov, “Revisiting semi- supervised learning with graph embeddings,” in ICML, 2016, pp. 40–48.
%\bibitem{Kipf} T. N. Kipf and M. Welling, “Semi-supervised classification with graph convolutional networks,” in ICLR, 2017.


%\bibitem{Ni} J. Ni, S. Chang, X. Liu, W. Cheng, H. Chen, D. Xu, and X. Zhang, “Co-regularized deep multi-network embedding,” in WWW, 2018, pp. 469–478.
%\bibitem{Xu} L. Xu, X. Wei, J. Cao, and P. S. Yu, “Embedding of embedding (EOE): joint embedding for coupled heterogeneous networks,” in WSDM, 2017, pp. 741–749.
%\bibitem{Yang} S. J. Pan and Q. Yang, “A survey on transfer learning,” \textit{IEEE Trans. Knowl. Data Eng}., vol. 22, no. 10, pp. 1345–1359, 2010.
%\bibitem{Wang} M. Wang and W. Deng, “Deep visual domain adaptation: A survey,” Neurocomputing, vol. 312, pp. 135–153, 2018
%\bibitem{AS-MAML} Ma, Ning, et al. "Few-Shot Graph Classification with Model Agnostic Meta-Learning." arXiv preprint arXiv:2003.08246 (2020).
%\bibitem{MAML} Finn, Chelsea, Pieter Abbeel, and Sergey Levine. "Model-agnostic meta-learning for fast adaptation of deep networks." \textit{Proceedings of the 34th International Conference on Machine Learning-Volume 70.} JMLR. org, 2017
%\bibitem{Du} Zhengxiao Du, Xiaowei Wang, Hongxia Yang, Jingren Zhou, and Jie Tang. Sequential scenario- specific meta learner for online recommendation. In KDD’19, page 2895–2904, 2019.
%\bibitem{Satorras} Victor Garcia Satorras and Joan Bruna Estrach. Few-shot learning with graph neural networks. In ICLR, 2018.
%\bibitem{Kim} Jongmin Kim, Taesup Kim, Sungwoong Kim, and Chang D. Yoo. Edge-labeling graph neural net- work for few-shot learning. In CVPR, June 2019.
%\bibitem{Liu} Lu Liu, Tianyi Zhou, Guodong Long, Jing Jiang, and Chengqi Zhang. Learning to propagate for graph meta-learning. In NeurIPS, 2019. 
%\bibitem{Yao} Huaxiu Yao, Xian Wu, Zhiqiang Tao, Yaliang Li, Bolin Ding, Ruirui Li, and Zhenhui Li. Au- tomated relational meta-learning. In ICLR, 2020.
%\bibitem{Chauhan} Jatin Chauhan, Deepak Nathani, and Manohar Kaul. Few-shot learning on graphs via super- classes based on graph spectral measures. In ICLR, 2020.

\end{thebibliography}
%\bibliography{currentbase}{}
%\bibliographystyle{plain}

\end{document}