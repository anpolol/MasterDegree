\documentclass[11pt, titlepackage]{article}
\usepackage[left=3cm, right=3cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[T2A]{fontenc}
\usepackage{cite}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx} 
\usepackage{indentfirst}
\usepackage{multirow}
\usepackage{footnote}
\usepackage{longtable}
\usepackage{array}
\usepackage{caption,tabularx,booktabs}
\usepackage{tikz} 
\usepackage{longtable}
\usepackage{amsmath}
% переименовываем  список литературы в "список используемой литературы"
\addto\captionsrussian{\def\refname{\centering Список используемых источников}}
\title{Литературный обзор по теме <<Разработка алгоритмов обучения без учителя для графовых структур данных>>}
\author{Андреева Полина}
\date{}

\begin{document}
\maketitle
\setcounter{page}{1}
\clearpage
\begin{center}
{\tableofcontents}
\end{center}

\newpage


\begin{center}
			{\section{Введение}}
		\end{center}
		
Графовое  представление данных это естественный, понятный человеку с одной стороны, и удобный, подходящий для машинной обработки с другой, способ визуализации данных из различных областей: от генетики до банковского дела. И хотя большинство самых распространенных архитектур нейронных сетей (например CNN, RNN) ориентировано на векторные структуры, в последнее время появились новые подходы глубокого обучения для представления и моделирования графово-структурированных данных. 

Большинство успехов в глубоком убучении достигнуто с применением одной из двух парадигм - обучения с учителем (контролируемое обучение) или обучение с подкреплением. В первом случае  модель делает предположение, а результат сравнивается с истинным значением, после чего производится обратное распространение ошибки. Во втором подходе, агент взаимодействует со средой. Он делает действие, а среда подает сигналы подкрепления, поэтому такое обучение является частным случаем обучения с учителем, но учителем является среда или её модель.  Но в обоих случаях пределы обучения определяются людьми, внешними надзирателями. 
Для истинного интеллекта необходимы более независимые стретегии обучения. В связи с чем возникает еще одна парадигма - неконтролируемое обучение, когда агент обучается с целью обучиться (например, младенцы изучают мир из любопытсва, через наблюдение, без подсказок). Более того, углубляясь в понимание работы человеческого интеллекта, можно заметить, что во взрослой жизни, люди не обучаются постоянно с нуля, а приспосабливаются к текущей ситуации, используя имеющиеся навыки и знания. По аналогии с чем в глубоком обучении придумана следующая парадигма, развивающая работу искусственного интеллекта - обучение с переносом. С точки зрения затраченных ресурсов, такой способ решения задач намного эффективней, так как не требует большого объема новых данных.  

В данном обзоре сначала будет рассмотрен основной подход алгоритмов на графах. Затем - кратко расшифрованы основные обозначения, необходимые при описании методов, сформулирована задача, представлены наиболее часто используемые наборы данных и метрики проверки качества в экспериментах.  В следующем параграфе описаны сами методы, разделенные на классы. В конце даны рекомендации по использованию конкретных методов на конкретных задачах. 

\newpage

\begin{center}
			{\section{Задачи на графах}}
		\end{center}


Графы -- структуры, состоящие из уникальных сущностей (узлов или вершин графа) и связей между ними (ребер графа), являются основной  формой  представления  и  хранения  знаний и удобным иснтрументом для работы с большинством видов данных. Среди основных преимуществ графового представления данных это логическая строгость и масштабируемость  на  большие  объемы  информации. 
Последнее время было сделано много попыток расширить сверточные нейронные сети на графы. Проблема графовых структур данных в самом не-векторном представлении. В связи с чем, основным подходом в задачах на графах стало представление элементов графа в некотором высокоразмерном пространстве, в виде векторов, так называемых эмбедингов \cite{GNN}. А далее такое представление в виде таблицы подается на вход классификатору. В таком виде задача решается как для любой другой обычной нейронной сети.  Методы, использующие эмбединги, включают в себя Graph Neural Networks (GNNs) \cite{GNN} и были применены к различных задачам относящимся  к графам.  

Задачи на графах можно разделить на следующие наиболее общие группы:

\begin{itemize} 

\item Предсказание связи между вершинами

\item Классификация вершин

\item Классификация ребер %(FNN - feed forward neural network)

\item Классификация графов % (FNN - feed forward neural network) Few-Shot Graph Classification with Model Agnostic Meta-Learning Ning Ma
\end{itemize}
%??????? упоминать ли методы представления графов в виде эмбедингов (node2vec, random walk)

Очень много существующих методов работают в режиме обучения без учителя -- то есть, без привязки к решаемой задаче, получают векторное представление узла в графе. А качество таких эмбедингов проверяют на классических задачах, чаще всего - классификация вершин.  
\newpage

\begin{center}
			{\section{Необходимые обозначения и формулировка проблемы}}
		\end{center}

\textbf{Обозначения}

Граф представляется в виде упорядоченной пары $G=(V,E)$, где $V =\{ v_i\} - $ непустое множество вершин, а $E = \{ e_{ij}\} - $ множество пар веришин, называемых ребрами ($e_{ij}-$ это реберо между вершиными $v_i$ и $v_j$ ). Порядок графа $|V| - $ число вершин в графе обозначается для краткости $n$. Аналогично, размер графа $|E| -$ число рёбер обозначим $m$. $A - $ матрица смежности, где каждое число $(a_{i,j})$ означает наличие ($a_{i,j}=1$) или отсутвие ($a_{i,j}=0$) ребра между вершинами $v_i$ и $v_j$ в случае не взвешенного графа и вес ребра в случае взвешенного. $W=\{w_{ij}\} - $  матрица весов в нейронной сети. Матрица $X - $ матрица признаков вершин в графе. Каждая колонка $X_i - $ это вектор признаков соответствующей вершины $v_i$. Размерность пространства признаков равна $d$.
$\mathcal{L} - $  функция потерь. $\Phi_i, \Theta_i - $ векторы в низкоразмерном пространстве, т.н. эмбединги вершины $v_i$.  Первый вектор означает представление вершины как исходной, а второе - представление вершины как контекстной для другой вершины. 
Вышеперечисленные обозначения собрана в таблице \ref{tab:notation}

\begin{table}[h]
\caption{\label{tab:notation}Необходимые обозначения}
\begin{center}
\begin{tabular}{|l|p{150pt}|}
\hline
Обозначение & Расшифровка \\
\hline
$|V| = n$ & Количество вершин в графе \\
$|E| = m$ & Количество рёбер в графе \\
$\textbf{A}=\{ a_{ij}\}$ & Матрица смежности \\
\textbf{I} & Единичная матрица \\
\textbf{D} & Матрица степеней вершин графа \\
\textbf{P} = $\textbf{D}^{-1}\textbf{A}$ & Матрица переходов \\
 $\textbf{X} = \{x_{ij} \}, i \in\{1,..,n\}, j \in\{1,..,r\} $   & Матрица признаков вершин \\
$r$ & Размерность признакового \newline пространства\\
$\textbf{W}$ & Матрица весов \\
$\mathcal{L} $ & Функция потерь \\
$\Phi_i, \Theta_i$ & Исходный и контекстный \newline эмбединги вершины $v_i$ \\
$d$ & Размерность эмбедингов\\
$N(v)$ &  Соседи вершины $v$ внутри окна определенного размера последовательности случайного блуждания \\
\hline
\end{tabular}
\end{center}
\end{table} 
\textbf{Формулировка проблемы}

Контролируемое машинное обучение требует  конструкирования признаков для каждой задачи заново, что является трудоемкой проблемой. В связи с чем, важной стала задача построения эффективного и независимого от задач обучения признаков графа. Задачи неконтрлируемого обучения представлений графа подразумевает построение некоторой функции $f$, которая отображает каждую вершину графа $v_i$ в вектор $\Phi_i$ в низкоразмерном пространстве (см. Рисунок 1) таким образом, чтобы эмбединги похожих вершин графа лежали близко: $Similarity(v_i,v_j) \approx \Phi_i^T\Phi_j$. Перечисленные в данном обзоре методы различаются а) выбором меры схожести двух вершин - структурной, по признакам, или одновременно б) способом отображения / отображающей функцией f.  


\begin{figure} [h!]
	\begin{minipage}{\linewidth}
	 		\center {\includegraphics[width =0.7\linewidth]{pictures/u.png}} \\ \small{Рисунок 1}
 	\end{minipage}
	 \end{figure}

\newpage
\begin{center}
			{\section{Наборы данных и метрики качества}}
		\end{center}

\textbf{Задачи}

Качество построенных эмбедингов проверяется на последующих задачах. Чаще всего решаются задачи классификации вершин и предсказания рёбер между вершинами, реже - кластеризации вершин. Также встречаются случаи, когда авторы метода проверят качество, визуализируя получившиеся эмбединги или реконструируя граф и считая разницу с изначальным. 

В данном разделе описаны наиболее популярные наборы данных и метрики для проверки качества методов, которые будут изложены в следующем разделе.

\textbf{Наборы данных}

Наборы данных по смыслу делятся на социальные сети, сети цитирования, сети смежных слов и тд, так что представим в данном разделе соответствующую классификацию. В таблице \ref{tab:datasets} представлена краткая справка по наиболее часто встречаемым наборам данных: размер, направленность, а описание методов представлено ниже:

\begin{enumerate}
\item  Социальные сети: SN-Twitter, Flickr, YouTube, Reddit, Epinions, BlogCatalog.

Вершины представляют собой пользователей, а ребра - дружбу между ними. Некоторые сети имеют атрибуты узлов, как например у сети авторов BlogCatalog атрибуты - темы, на которые пишет данный автор. У сети YouTube, метки вершин предствляют предпочитаемые жанры.

\item Сети цитирования: Cora, Citeseer, CoCit, DBLP, PubMed.

 Каждая вершина представляет собой "мешок слов" статьи, а ребро между двумя вершинами - существование цитирования. Метка вершины - это сфера тематики данной статьи.
  
\item Сети со-авторств: Arxiv. 

Вершины - авторы. Ребро между двумя вершинами существует, если авторы участвовали в написании одно статьи.

\item Сети слов: Wikipedia. 

Вершины - слова. Если между двумя вершинами стоит ребро, значит эти два слова появляются в окне из 5 слов не менее 5 раз. Метки означают части речи.

\item Другое: 

Zachary’s karate network - известная и часто используемая для визуализации сеть университетского клуба карате. 


PPI Protein-Protein Interaction. Каждый граф соответствует отдельной ткани человека. Вершины - протеины c наобором атрибутов. Метка - роль белка с точки зрения его клеточных функций. 

\end{enumerate}
\newpage
\begin{table}[h!]
\caption{\label{tab:datasets} Справка по наборам данных.}
\begin{center}
\begin{tabular}{|p{100pt}| p{55pt}| p{80pt}|p{150pt}|}       
\hline
Категория & Название датасета  & Характеристика & Размер \\
\hline
\multirow{6}{100pt}{Социальная сеть } & SN-Twitter & Направленный & $|V|=465K$ $|E|=834K$ \\ 
\cline{2-4}
 & Flickr &  Ненаправленный & $|V|=80K$ $|E|=5,90M $ \newline $|L|=195$ \\
\cline{2-4}
 & YouTube &Ненаправленный & $|V|=1,13M$ $|E|=2,99M$ \newline $|L|=47$ \\
\cline{2-4}
 & Reddit& Ненаправленный &$|V|=231K$ $|E|=11,6M$ \newline $|L|=41$ \\
\cline{2-4}
 & Epinion& Направленный & $|V|=75K$ $|E|=508K$ \\
\cline{2-4}
 & BlogCatalog & Ненаправленный&$|V|=10K$ $|E|=33K$ \newline $|L|=39$ \\
\hline
\multirow{5}{100pt}{Сеть цитирования} &  Cora &Ненаправленный& $|V|=23K$ $|E|=91K$ \newline $|L|=70$ \\
\cline{2-4}
 &  Citeseer&Ненаправленный & $|V| = 3K$ $|E|= 4K$ \newline $|L|=6$ \\
\cline{2-4}
&  CoCit & Направленный &$|V|=44K$ $|E|=195K$ \newline $|L|=15$ \\
\cline{2-4}
&  DBLP-Ci &Направленный&$|V|=12.5K$ $|E|=49K$ \\
\cline{2-4}
 &  PubMed&Направленный& $|V|=19K$ $|E|=44K$ \newline $|L|=3$ \\
\hline
Сеть соавторовств & Arxiv GR-QC i & Ненаправленный & $|V|=5K$ $|E|=28K$ \\
%проверить все датасеты на направленность : Arxiv, Wikipedua, PPI, karate
\hline
Сеть смежности слов & Wikipedia  &Ненаправленный & $|V|=4K$ $|E|=184K$ \newline $|L|=40$ \\
\hline
\multirow{2}{100pt}{Другое} & Zachary's karate network  &Ненаправленный&$|V|=34$ $|E|=78$ \newline $|L|=4$ \\
\cline{2-4}
 &  PPI &Ненаправленный& $|V|=3K$ $|E|=38K$ \newline $|L|=50$\\
\hline
\end{tabular}
\end{center}
\end{table} 

\newpage
Наиболее часто используемые \textbf{метрики} для проверки качества метода на конкретных задачах собраны в таблице \ref{tab:metrics}:


\begin{table}[h!]
\caption{\label{tab:metrics}Метрики качества.}

\setlength{\extrarowheight}{10pt}
\begin{tabular}{|p{100pt}|p{180pt}|l|}
\hline
Метрика & Формула, описание & Для каких задач \\
\hline
 Micro-F1 score   \footnotemark & \Large $\frac{2\cdot P\cdot R}{P+R}$  & Классификация вершин  \\
\hline
Macro-F1 score & \Large $\frac{\sum_{l\in L} F1(l)}{|L|},$  \normalsize где $ F1(l) - F1 - $ score для метки $l$& Классификация вершин \\
\hline
 AUC (Area Under Curve)& Площадь под кривой ошибок (кривая в осях $TP/(TP+FN)$ к $FP/(TN+FP)$, где точки относятся к различным значениям порога отсечения, при котором считается, что ребро существует)  & Предсказание рёбер \\
\hline
Precision & \Large $\frac{N^k \cap N(v)}{k}$, \normalsize где $N^k - k $ ближаших соседей вершины судя по построенным эмбедингам, $N(v) - $ истинные соседи вершины $v$. &  Реконструкция графа\\
\hline
NMI (Normalized Mutual Information) & \Large $\frac{2\cdot I(Y;C)}{H(Y)+H(C)}$, \normalsize где  $Y - $истинные метки классов, $C -$ метки кластеризации, $H(.,.) -$энтропия $I(.,.) -$ взаимная инфорация & Кластеризация вершин \\
\hline
\end{tabular}
\end{table} 
\footnotetext{Данная метрика считается глобально, подсчетом TP = true Positives, FP = False Postives, FN = False Negative по всем меткам $l \in L$. \newline

$P = \frac{\sum_{l\in L} TP(l)}{\sum_{l\in L} (TP(l)+FP(l))}$,   $R = \frac{\sum_{l\in L} TP(l)}{\sum_{l\in L} (TP(l)+FN(l))}$ }

\newpage 
\begin{center}
{\section{Классификация и описание методов}}
\end{center}

Разделение на классы, использованное в данном обзоре взято из \cite{EmbedingSurvey} и добавлены дополнительные методы. 
Данная классификация разделяет методы по подходу отображения вершин графа в эмбединги: методы, основанные на а) факторизации, б) случайных блужданиях, в) глубоком обучении и г) другие. 

В данном разделе кратко описаны идеи всех рассматриваемых методов, а в таблице \ref{tab:methods} собрана короткая справка по методам: на чем основаны методы, функции потерь, выбранные авторами оригинальных статей подходы к оптимизации и наличие/отсутствие разделения эбедингов на контекстный и целевой.

\begin{center}
{\subsection{Методы, основанные на факторизации} }
\end{center}

Методы, основанные на факторизации рассматривают матрицы, отражающие соединения между вершинами графа: матрица смежности, Лапласиан графа, матрицу вероятностей перехода между вершинами. Затем эта матрицы факторизуется для получения эмбедингов. 

\begin{itemize}
\item Метод Laplacian Eigenmaps \cite{eigenmaps} стремясь сохранить два эмбединга ближе в случае, когда в графе между соответствующими вершинами больше вес (в случае взвешенного графа), минимизирует следующую функцию потерь:
\begin{equation}
\mathcal{L}(\Phi) = \frac{1}{2} \sum_{i,j} |\Phi_i - \Phi_j|^2 a_{i,j} = tr(\Phi^T\textbf{L}\Phi)
\end{equation}
где $\textbf{L}=\textbf{D} - \textbf{A}$ -- лапласиан графа.

Решение этой задачи - это собственные вектора, соответствующие $d$ наименьшим собственным числам нормализованного Лапласиана графа.  
%\item Методы  \cite{Sam, reduction} строят эмбединги, минимизируя  функцию потерь, основанную на ошибке линейной реконструкции. 

\item Graph Factorization \cite{factorization} для быстрого разложения матрицы смежности на собственные векторы, использует стохастический градиентный спуск для минимизации следующей функции потерь:
\begin{equation}
\mathcal{L}(\Phi,\lambda) = \frac{1}{2} \sum_{i,j} ( A_{i,j} - \Phi_i\cdot\Phi_j )^2+ \frac{\lambda}{2}\sum_i ||\Phi_i||^2
\end{equation}

Преимущество - скорость. 

%\item GraRep \cite{GraRep} определяет вероятность перехода вершин как $T=D^{-1}A$ и сохраняет схожесть $к$-го порядка минимизируя $|| X^k - Y_s^kY_t^{kT}||$, где  $X^k$ выводится из $T^k$.    разных порядков а затем конкатенирует получившиеся представления. Недостаток это сложность масштабирования. %не посомотрела так как не нашла, 
\item В методе HOPE \cite{HOPE} минимизируется $||\textbf{C} - \Phi \cdot\Theta||_F^2$. В данном случае элементы матрицы $c_{ij}$ отражают близость вершин $v_i,v_j$. $C$ можно представить в виде $C=\textbf{M}_g^{-1}\textbf{M}_l$, а матрицы $\textbf{M}_g,\textbf{M}_l$ отличаются для каждой из мер схожести:
\begin{enumerate}
\item Katz Index: 
\begin{equation}\label{eqn:HOPEKatz}
\begin{aligned}
\textbf{M}_g = \textbf{I}-\beta \cdot\textbf{A},\\
 \textbf{M}_l = \beta \cdot \textbf{A}, 
\end{aligned}
\end{equation}
где $\beta$ -- коэффициент затухания определяет как быстро затухает вес пути с возрастанием длины этого пути
\item Rooted PageRank
\begin{equation}
\begin{aligned}
\textbf{M}_g = \textbf{I}-\alpha \cdot \textbf{P},\\
 \textbf{M}_l = (1-\alpha) \cdot \textbf{I}, 
\end{aligned}
\end{equation}
где $\alpha$ -- вероятность рандомного перехода к соседу. 
\item Common Neighbors
\begin{equation}\label{eqn:HOPERPR}
\begin{aligned}
\textbf{M}_g = \textbf{I}, \\
 \textbf{M}_l = \textbf{A}^2, 
\end{aligned}
\end{equation}
\item Adamic-Adar score
\begin{equation}\label{eqn:HOPECommonNeigbor}
\begin{aligned}
\textbf{M}_g = \textbf{I}, \\
 \textbf{M}_l = \textbf{A}\cdot \textbf{D} \cdot \textbf{A}, 
\end{aligned}
\end{equation}
\end{enumerate}

В связи с разреженностью $\textbf{M}_g,\textbf{M}_l$ можно эффективно использовать разреженное обобщенное сингулярное разложение (SVD) для построения эмбедингов графа: SVD применяется к $\textbf{M}_g, \textbf{M}_l$. Оптимальные значения эмбедингов - считаются как корни произведения синуглярного числа на соответвующий сингулярный вектор.  

%NETMF более подронбно, но коротко, соновные формулы вставить 
\item Авторы статьи \cite{NETMF} показывают, что каждая из четырех моделей: DeepWalk, LINE, PTE, Node2vec, выполняют неявную матричную факторизацию (алгоритм NetMF). Для каждой модели выведены матричные формы. Например, алгоритм DeepWalk экивалентен факторизации следующей матрицы:
\begin{equation}\label{eqn:NetMF}
\log (\frac{vol(G)}{T}(\sum_{r=1}^T \textbf{P}^r)\textbf{D}^{-1}) - \log(b)
\end{equation}
в данном случае $b$ негативных примеров для Negative Sampling, vol(G) - объем взвешенного графа (сумма степеней всех вершин), T - длина случайного блуждания, r - размер окна в DeepWalk.

Если для краткости положить $\textbf{M} = (\frac{vol(G)}{bT}(\sum_{r=1}^T \textbf{P}^r)\textbf{D}^{-1}) $. Тогда, предлагаемый алгоритм NetMF применяет SVD к матрице $\log \textbf{M}$, а оптимальные значения эмбедингов - это, также как и в предыдущем пункте, корни произведения синуглярного числа на соответвующий сингулярный вектор.   

\end{itemize}

\begin{table}[ph]
\caption{\label{tab:methods}Методы неконтролируемого обучения}
\setlength{\extrarowheight}{10pt}
\begin{tabular}{|p{68pt}|p{60pt}|p{60pt}|p{180pt}|p{90pt}|}
\hline
Основан на & Метод & Контекстный эмбединг  & Функция потерь & Подходы к оптимизации \\
\hline
 факторизации & Laplacian EigenMaps & $\times$  & $ \frac{1}{2} \sum_{i,j} |\Phi_i - \Phi_j|^2 A_{i,j} \newline = tr(\Phi^TL\Phi)$ & Matrix Factorization\\
\hline
факторизации & Graph Factorization & $\times$  & $\frac{1}{2} \sum_{i,j} ( A_{i,j} - \Phi_i\cdot\Phi_j )^2\newline + \frac{\lambda}{2}\sum_i ||\Phi_i||^2$ & SGD with  asynchronous optimization\\
\hline
факторизации & HOPE &   \checkmark &$||\textbf{C} - \Phi \cdot\Theta||_F^2$, $C-$ матрица схожести вершин, см. уравнения \ref{eqn:HOPEKatz}, \ref{eqn:HOPERPR}, \ref{eqn:HOPECommonNeigbor} & Matrix Factorization\\
\hline
факторизации & NetMF &      \checkmark  & $|| \log (\frac{vol(G)}{bT}(\sum_{r=1}^T \textbf{P}^r)\textbf{D}^{-1}) - \Phi \cdot \Theta||_F^2$\newline (значение параметров см. в описании к ур-ию \ref{eqn:NetMF}) &Matrix Factorization + Negative Sampling\\
\hline
случайных блужданиях & DeepWalk &   \checkmark  & $- \sum_{v_j \in V} \sum_{v_i \in N(v_j)} log \frac{\exp (\Theta_i \cdot \Phi_j)}{\sum_{v_k \in V} \exp (\Theta_k \cdot \Phi_j) } $, где $N(v)$ -- см. таблицу \ref{tab:notation}. В данном случае случайные блуждания фиксированной длины & Hierarchical softmax \\
\hline
случайных блужданиях  & Node2Vec &  \checkmark      & $- \sum_{v_j \in V} \sum_{v_i \in N(v_j)} log \frac{\exp (\Theta_i \cdot \Phi_j)}{\sum_{v_k \in V} \exp (\Theta_k \cdot \Phi_j) } $\newline Случаыные блуждания меют два параметра - вероятноность перехода к вершинам, отвечающим за исследования локальной  и глобальной структур& Negative Sampling \\
\hline
случайных блужданиях  & Struc2Vec&   \checkmark  & $- \sum_{v_j \in V} \sum_{v_i \in N(v_j)} log \frac{\exp (\Theta_i \cdot \Phi_j)}{\sum_{v_k \in V} \exp (\Theta_k \cdot \Phi_j) } $\newline СлучАйные блуждания строятся по контекстному графу (см ур.-я \ref{eqn:RandomWalkForStruc2Vec},\ref{eqn:RandomWalkForStruc2Vec2}), учитываемому структурную схожесть вершин & Hierarchical Softmax \\
\hline
случайных блужданиях  & Seed & $\times$ &     $ ||X - \hat{X}||_2^2$  & Deep autoencoders\\
\hline
\end{tabular}
\end{table} 

\begin{table}
\setlength{\extrarowheight}{10pt}
\begin{tabular}{|p{68pt}|p{60pt}|p{60pt}|p{180pt}|p{90pt}|}
\hline
случайных блужданиях  & App &  \checkmark  &  $- \sum_{v_j \in V} \sum_{v_i \in N(v_j)} \log\ \frac{1}{1+ exp(-\Theta_i \cdot \Phi_j)}$\newline для построения случайных последовательностей испольузется метод Monte-Carlo End-Point &Skip-Gram with Negative Sampling   \\
\hline
случайных блужданиях & VERSE &  $\times$  & $-\sum_{i,j=1}^{n} sim_G(v_i,v_j) \log\frac{\exp (\Phi_i \cdot \Phi_j)}{\sum_{k=1}^n \exp (\Phi_i \cdot \Phi_k)}$ \newline $sim_G$ -- распределение схожести вершин графа. В случае PPR, $sim_G(v,\cdot)$ -- последняя вершина в одном случайном блуждании начатого с вершины v, в случаях других мер схожести, определяется уравнениями \ref{SR}, \ref{ADJ} &Noise Constractive Estimations. Negative Sampling  \\
\hline
глубоком обучении & GraphSAGE &  $\times$  & $- \sum_{v_j \in V} \sum_{v_i \in N(v_j)} \log  \frac{1}{1+ exp(-\Phi_i \cdot \Phi_j)}$   &Negative Sampling  \\
\hline
глубоком обучении  & SDNE  &  $\times$ &    $|| (\hat{A} - A) \odot B||_F^2 + \alpha \sum_{v_i,v_j \in V} a_{i,j}||\Phi_i - \Phi_j||_2^2 $&  Deep autoencoders\\
\hline
моделировании соседства & LINE-1 & $\times$  & $- \sum_{v_i,v_j\in V} a_{ij}\log  \frac{1}{1+ exp(-\Phi_i \cdot \Phi_j)}$& Negative Sampling \\
\hline
моделирование соседства & LINE-2 & \checkmark & $ - \sum_{v_i,v_j\in V} a_{ij}\log \frac{\exp (\Phi_i \cdot \Theta_j)}{\sum_{v_k \in V} \exp (\Phi_i \cdot \Theta_k)}$& Negative Sampling\\
\hline
\end{tabular}
\end{table} 

%Ou et al. [33] utilize sparse generalized SVD to generate a graph embedding, HOPE, from a similarity matrix amenable to decomposition into two sparse proximity matrices. 
%Матрица S - является матрицей сходства. Она раскладыаается на две матрицы , одна из них это композиция матрицы смежности и степеней, а другая - единичная, есть еще вариан когда считаются через матрицы вероятностей перехода.  В общем, у таких двух матриц считаются с.з, а потом их используют для того чтоб построить эмбединги. 



%линейное снижение размерности значит что переход в линейное пространство и исопльзование линейного преобразования 

\begin{center}
{\subsection{Методы, основанные на случайных блужданиях} }
\end{center}

Коротко, все последующие методы можно описать тремя шагами: a) использовать некоторую функцию $f$, для представления вершин графа в низко-размерном пространстве в виде векторов. Например, $\Phi_i, \Phi_j$ для вершин $v_i, v_j$,  б) Определить функцию хожести двух вершин $Similarity(v_i,v_j)$ в) Оптимизировать параметры функции $f$ из первого шага, таким образом, чтобы $Similarity(v_i,v_j) \approx \Phi_i^T\Phi_j$.

Собственно, методы основанные на RandomWalk считают в качестве данной меры схожести - вероятность появления этих двух вершин во время случайного блуждания. 

Итак, сначала запускаются случайные блуждания, строится множество соседей для каждой вершины и затем минимизируется:

\begin{equation}
\mathcal{L} = \sum_{v_j \in V} \sum_{v_i \in N(v_j)} - log(P(v_i|\Phi_j))
\end{equation}

\begin{itemize}
\item DeepWalk \cite{Deepwalk} использует случайные блуждания фиксированной длины.
\item Node2vec \cite{node2vec} в отличие от DeepWalk исследует и локальную и глобальную структуру графа и с помощью двух гиперапараметров может учитывать влияние каждой из сторон: один гиперпараметр отвечает за вероятность агента во время случайных блужданий вернуться на шаг назад, т.е. за учет локальной структуры графа, а другой гиперпараметр, отвечает за вероятность агента сделать шаг в сторону от вершины, с которой началось блуждание, таким образом, учитывая глобальную структуру графа. Таким образом, в Node2Vec $N(v)$ отличается от DeepWalk из-за самого характера случайного блуждания.
\item Struc2vec \cite{struc2vec} рассчитывает расстояние между вершинами по структурной схожести, а не топологической. 
\begin{enumerate}
\item Мера структурной близости между двумя вершинами, учитывающими сосдество размера $k$ (соседи, расстояние до который менее или равно $k$) -- это разница в степени вершины и степеней ее соседей:
\begin{equation}
f_k(u,v) = f_{k-1}(u,v) + g(s(R_k(u),s(R_k(v)))
\end{equation}
где $R_k(u)$ -- множество соседей вершины $u$ строго на расстоянии $k$. $s(R)$ -- отсортированная последовательность степеней каждой вершины из $R$.  $g(R_1, R_2)$ -- расстояние между двумя отсортированными последовательностями чисел. В оригинальной статье, описывающей данный метод, используется Dynamic Time Wraping (DTW), которое минимизирует расстояние между каждой парой элементов двух последоватльностей. Последнее считается следующим образом:
\begin{equation}
d(a,b) =\frac{max(a,b)}{min(a,b)} - 1
\end{equation}
Хотя в принципе в данном методе можно использовать и другие способы нахождения расстояния между последовательностями степеней вершин.

\item После чего строится новый, взвешенный, многослойный, так называемый контекстный, граф. Каждый слой $k$ отвечает за соседство вершин в радиусе $k$. В каждом слое $k$ находятся все вершины, а ребра между вершинами внутри слоя имеют вес, обратно пропорциональный структурной схожести между этими вершинами:
\begin{equation}
w_k(u,v) = \exp^{-f_k(u,v)}
\end{equation}
Между слоями ребра соединяют лишь соотевтвующие вершины. Вес между ребром от $u_k$ к $u_{k+1}$ равен логарифму суммы ребер, инцидентных $u_k$ и имеющих вес больший, чем средний по данному слою.

\item Далее по этому контекстному графу строится несколько случайных блужданий фиксированной длины, начиная с нулевого слоя. Вероятность перехода к вершине внутри слоя равна:
\begin{equation}\label{eqn:RandomWalkForStruc2Vec}
p_k(u,v) = \frac{\exp^{-f_k(u,v)}}{\sum_{l \neq u, l \in V} \exp^{-f_k(u,l)}}
\end{equation} 
А вероятность перейти на следующий слой:

\begin{equation}\label{eqn:RandomWalkForStruc2Vec2}
\begin{aligned}
	p_k(u_k,u_{k+1}) &= \frac{w(u_k,u_{k+1})}{w(u_k,u_{k+1}) + w(u_k,u_{k-1})},\\
	p_k(u_k,u_{k-1}) & =1 - p_k(u_k,u_{k+1})
\end{aligned}
\end{equation}
\item Далее, как и в DeepWalk, Node2vec, для построения представлений каждой вершины используется Skip-Gram, который максимизирует вероятность появления контекста вершины (контекст вершины задается окном размера w, центрированного на вершине в последовталеьности случайного блуждания, построенного на предыдущем шаге)
 
\end{enumerate}
\item SEED \cite{SEED} ориентирован на построения эмбедингов графов, а не вершин.  Обучает автоэнкодер на случайных блужданиях типа WEAVE (каждый подграф представляется в виде матрицы $X$, где каждый столбец $p$ представляет одну вершину в виде конкатенации атрибуата этой вершины и наиболее раннего визита в ходе $p$-го блуждания). Обучение автоэнкодера происходит минимизируя ошибку реконструкции $ \hat{X}$:
\begin{equation}
\mathcal{L} = ||X - \hat{X}||_2^2
\end{equation}

На последнем шаге усредненяются представления подграфов, или, если обобщать, то отображаются полученные представления $\Phi$ в некоторое новое пространство и усредняют полученные представления:
\begin{equation}
\hat{\mu}_G = \frac{1}{s} \sum_{i=1}^s \phi(\Phi_i)
\end{equation}


\item APP  \cite{APP} - сохраняет несимметричность, таким образом, что приписывает каждой вершине две роли - исходную и целевую $(\Phi_i, \Theta_i)$. 
Используя метод сэмплировния Монте-Карло End-Point,  рандомно выбирются пути, начинающиеся в одной вершине с вероятностью остановки $\alpha$ и заканчивающийся в другой. По аналогии с DeepWalk, Node2Vec, каждый путь рассматривается как направленная последовательность, но в которой пары вершин рассматриваются только по прямому направлению. Таким образом и учитывается несимметричность. Затем оптимизируется следующая функция потерь, учитывающая Negative Sampling:
 
\begin{equation}
\mathcal{L} = \sum_j \sum_i \#Sampled_j(i) \cdot(\log\sigma(\Phi_j\cdot\Theta_i) + b \cdot E_{t_n \sim P_D}[\log\sigma(-\Phi_j\cdot\Theta_n)])
\end{equation}
где  $\#Sampled_j(i) - $ количество путей $(j,i)$, $b$ -- количество негативных примеров, берущихся из распределения $P_D$, $\sigma$  в анном слуачае сигмоида.

\item TransE \cite{TransE} приспособлен для графов знаний, у которых ребра также могут быть разных типов.  Частая задача на таких графах - предсказание пропущенных связей. В TransE отношения между сущностями представлены в виде триплета:
\begin{equation}
h(\text{head entity}),l(\text{relation}),t(\text{tail entity}) \rightarrow (h,l,t) 
\end{equation}
Затем, сущности переводятся одним из вышеперечисленных методов в простнаство эмбедингов.

Далее отношения предсталяются как переносы (translations):
\begin{equation}
 \genfrac{}{}{0pt}{}{(h+l) \approx t,  \text{данный факт - правда}}{(h+l) \neq t,  \text{иначе} }  
\end{equation} 

И в конце происходит оптимизация на основе Negative Sampling. 
\item VERSE \cite{VERSE} так же использует понятия схожести между вершинами и минимизируют расхождение Кульбака - Лейблера (KL) между  распределением схожести вершин в графе $sim_G$ и распределением схожести эмбедингов $sim_E$:
\begin{equation}
\sum_{v \in V} KL(sim_G(v,\cdot) || sim_E(v,\cdot))
\end{equation}

В качестве распределения схожести вершин в пространстве эмбеддингов используется нормализованное с помощью softmax скалярное произведение эмбеддингов:
 \begin{equation}
sim_E(v,\cdot) = \frac{\exp{\Phi_v \Phi^{T}}}{\sum_{i=1}^n \exp(\Phi_v \Phi_i)}
\end{equation}
В данном выше уравнении $\Phi - $ матрица эмбеддингов, в которой отдельный ряд $\Phi_v$ соответствует эмбеддингу вершины $v$.

Также как и HOPE использует различные меры для распределения схожести в графе (PPR, SimRank, Adjacency similarity):

\begin{enumerate}
\item Personalized PageRank: один экзамепляр $sim_G(v,\cdot)$ это последняя вершина в одом случайном блуждании, начатом с вершины $v$.
\item SimRank: мера структорной взаимосвязи двух вершин, основана на предположении, что схожие вершины связаны с дргуими схожими вершинами. Определяется рекурсивно:
\begin{equation}\label{SR}
sim_G^{SR} = \frac{C}{|I(u)| |I(v)|} \sum_{i=1}^{|I(u)|} \sum_{j=1}^{|I(v)|} sim_G^{SR}(I_i(u),I_j(v))
\end{equation}
$I(v)$ -- множество соседей вершины $v$, с ребрами, входящими в $v$, $C$ -- число между 0 и 1, геометрически обесценивает важность дальних узлов. 
\item Adjacency similarity: если $Out(u)$ -- степень выходящих ребер из вершины u, то 
\begin{equation}\label{ADJ}
sim_G^{ADJ}(u,v) = \begin{cases}
   1/Out(u) & \text{если} (u,v) \in E\\
   0 & \text{иначе}
 \end{cases}
\end{equation}

\end{enumerate}

Рассмотрим подробнее функцию потерь:

По определению расстояния Кульбака-Лейбнера для дискретных распределений:
\begin{equation}
KL(P||Q) = \sum_{i=1}^n p_i \log\frac{p_i}{q_i}
\end{equation}

В случае с распределениями схожести в графе и прстранстве эмбеддингов, получаем:
\begin{multline}
KL(sim_G(v,\cdot)||sim_E(v,\cdot))= \sum_{i=1}^n sim_G(v,\cdot)\log\frac{sim_G(v,i)}{sim_E(v,i)} = \\
= \sum_{i=1}^n sim_G(v,i)\log sim_G(v,i) - \sum_{i=1}^n sim_G(v,i)\log sim_E(v,i)
\end{multline}
Но первое слагаемое, включает в себ ятолько распределение схожест в графе, что не влияет на минимизацию, так как задается изначально. Поэтому,  в качестве функции потерь остается только:

\begin{equation}\label{eqn:LossVERSE}
\mathcal{L} = - \sum_{v \in V} sim_G(v,\cdot) \log \frac{\exp{\Phi_v \Phi^{T}}}{\sum_{i=1}^n \exp(\Phi_v \Phi_i)}
\end{equation}
Для оптимизации используется Noise Contrastive Estimation (NCE): алгоритм строит классификатор (логистическую регрессию), который разделяет вершины из настоящего распределения $sim_G(v,\cdot)$ и распределения шума $Q$. Производная от NCE с увеличением негативных примеров, сходится  градиенту функции (\ref{eqn:LossVERSE}).

В отличие от HOPE использует нелинейные преобразовнаия и требует на вход не обязательно весь граф из-за чего может быть использован на больших графах.  
\end{itemize}

Преимуществом данных методов являются: а) вычислительная эффективность, так как они не требуют рассматрения абсолютно всех пар вершин, а только те, что появляются в одном случайном блуждании; б) возможность учитывают как локальную так и глобальную структуру графа.


 \begin{center}
{\subsection{Методы, осованные на глубоком обучении}}
\end{center}

Общая идея данных методов - агрегировать информацию от соседних вершин и передавать на следующий слой, выстраивая таким образом полноценную нейронную сеть. (см. Рисунок 2). Довольно распространены и популярны среди методов глубокого обучения на графах методы GCN (Graph Convolutional Network) \cite{GCN} и GAT (Graph Attention Network) \cite{GAN}. Однако в оригинальном виде они используют в качестве функции потерь cross-entropy, которая считается для режима обучения с учителем (необходимы размеченные данные).  Тем не менее их можно модифицировать, беря только части, касающиеся неконтролируемого этапа:  
\begin{figure} [h!]
	\begin{minipage}{\linewidth}
	 		\center {\includegraphics[width =1\linewidth]{pictures/GNN.png}} \\ \small{Рисунок 2}
 	\end{minipage}
	 \end{figure}
\begin{itemize}
\newpage
\item GCN \cite{GCN} - на каждом новом слое строится представление вершины путем агрегирования информации от соседей на прошлом слое и суммируется с представлением самой вершины на прошлом слое. Веса тренируются с помощью SGD:
\begin{equation}
\begin{aligned}
\textbf{h}_v^0 &= \textbf{x}_v   \\
 \textbf{h}_v^k & = \sigma (\textbf{W}_k \sum_{u \in N(v)} \frac{\textbf{h}_u^{k-1}}{|N(v)|} + \textbf{B}_k \textbf{h}_v^{k-1}), \forall k \in \{1, ... , K \} \\
\Phi_v & = \textbf{h}_v^K
\end{aligned}
\end{equation}



Где $\textbf{h}_v^k - $ представление вершины $v$ на $k-$ом слое. 
\item GAT \cite{GAN} - Идея в том, что соседи оказывают разное влияние на данную вершину и необходимо это учесть: 
\begin{equation}
 \textbf{h}_v^k = \sigma (\sum_{u \in N(v)} \alpha_{vu} \textbf{W}_k  \textbf{h}_u^{k-1})
\end{equation}

Существует эффективная модификация GCN, метод GraphSAGE, который работает полностью в неконтролируемом режиме и подходит для больших графов. 
\item CraphSAGE \cite{GraphSAGE} В отличие от предыдущего метода, может использовать разные варианты аггрегирования информации от соседей кроме среднего значения, например, выбор максимального значения или применение LSTM. Кроме того, еще одно отличие от базового варианта это конкатеринирование представления самой вершины с прошлого слоя к агрегированной информации от соседних вершин, вместо суммирования с ней.  %МАСШТАБИРУЕМЫЙ 
Преимущество над предыдущим методом - это обобщение:
\begin{equation}
 \textbf{h}_v^k = \sigma ([\textbf{W}_k \cdot AGG (\{ \textbf{h}_u^{k-1}, \forall u \in N(v) \}), \textbf{B}_k\textbf{h}_v^{k-1} ])
\end{equation}

Для настройки весов минимизируется функция потерь, которая может иметь как и контролируемый вид (cross-entropy), так и неконтролирумый, аналогичный DeepWalk:
\begin{equation}
- \sum_{v_j \in V} \sum_{v_i \in N(v_j)} \log  \sigma (\Phi_i \cdot \Phi_j)
\end{equation}

%SDNE 
\item SDNE \cite{SDNE} состоит из двух шагов: 1) Неконтролируемая часть, данный автоэнкодер отвечает за построение эмбедингов, учитывая близость 2 порядка. 2) Контролируемая часть, строится такая функция потерь, чтоб накладывать штрафы, когда похожие вершины в пространстве эмбедингов находятся далеко друг от друга, т.е. учитывать близость вершин 1 порядка  с помощью матрицы смежности.

Близость первого порядка - локальная близость между двумя вершинами. Чем больше вес между вершинами, тем ближе вершины.

Близость второго порядка - схожесть между структурой соседства двух вершин. Математически: $p_u = (a_{u,1}, ... , a_{u,|V|}) - $ определяет близость первого порядка для вершины $u$ ко всем остальным. Тогда близость второго порядка между вершинами $u,v$ определяется сходестью между $p_v,p_u$

Глубокий автоэнкодер коротко можно описать следующим образом. Это неконтролируемая модель, состоящая из двух частей - энкодер и декодер $g$. Энкодер состоит из нескольких нелинейных функций, которые отображают входные данные (вершины) в скрытые представления (в конечном счете - в эмбединги). Декодер тоже состоит из нескольких нелинейных функций, и наоборот принимая на вход эмбединги, реконструирует граф. 

Имея в качестве входа $\textbf{a}_i$ (колонки матрицы смежности) скрытые представления каждого слоя записываются следующим образом:
\begin{equation}
\begin{aligned}
	\textbf{y}_i^{(1)} & =  \sigma (W^{(1)}\textbf{a}_i + \textbf{b}^{(1)}),\\
	\textbf{y}_i^{(k)} & = \sigma (W^{(k)}\textbf{y}_i^{(k-1)} + \textbf{b}^{(k)}), k = 2,...,K
\end{aligned}
\end{equation}

$y_i^{(K)}= \Phi_i$ -- и есть эмбединг вершины $v_i$. После получения всех $\Phi_i$ , можно получить реконструкцию $\hat{\textbf{a}}_i$ обращая процесс энкодера. 

В качестве ошибки реконструкции в оригинальной статье используются б\textbf{о}льшие штрафы для не нулевых элементов, чем для нулевых добавлением умножения на смещения $\textbf{b}_i$ (т.е. ненулевые элементы должны иметь меньшую ошибку реконструкции, чем нулевые):
\begin{equation}
\mathcal{L}_{2nd} = ||(\hat{A} - A) \odot B||_F^2
\end{equation}
где $\odot$ -- произвдение Адамара, $B$--матрицы смещений (biases). $g $-- декодер. 

Для того, чтобы учитывать локальную структуру (близости 1 порядка) строится функция потерь на основе Laplacian Eigenmaps -- похожие вершины в пространтсве эмбедингов также должны быть ближе:

\begin{equation}
\mathcal{L}_{1st} = \sum_{i,j=1}^n a_{i,j} ||\Phi_i - \Phi_j||_2^2
\end{equation}

Общая функция потерь выглядит следующим образом: 
\begin{multline}
\mathcal{L}_{mix} = \mathcal{L}_{2nd} + \alpha\mathcal{L}_{1st}  =\\
=|| (\hat{A} - A) \odot B||_F^2 + \alpha \sum_{i,j=1}^n a_{i,j}||\Phi_i - \Phi_j||_2^2 
\end{multline}
$\alpha$ - параметр, контролирующий отношение степеней важности локальной структуры к глобальной.
\end{itemize}

\begin{center}
{\subsection{Другие методы}}
\end{center}

\begin{itemize}
\item LINE \cite{LINE} Явно задает две функции, отвечающие за близость 1 и 2 порядка, а затем минимизирует функцию - комбинацию из двух. 

Так же, как и в VERSE, задаются два распределения совместных вероятностей двух вершин/эмбедингов, одно - используя матрицу смежности, другое - для эмбедингов. Например для случая близости 1 порядка: 
\begin{equation}
p_1(v_i,v_j) = \frac{1}{1+ exp(-\Phi_i \cdot \Phi_j)}
\end{equation}
\begin{equation}
\hat{p_1}(v_i,v_j) = \frac{a_{ij}}{\sum_{i,j \in V} a_{ij}}
\end{equation}

А затем минимизируют расхождение Кульбака - Лейблера (KL) между  распределениями:

\begin{equation}
\mathcal{L} = KL(\hat{p_1},p_1) =  \sum_{i,j} \hat{p_1}_{ij} \log \frac{\hat{p_1}_{ij}}{p_{1_{ij}}} = \sum_{i,j} \hat{p_1}_{ij} \log \hat{p_1}_{ij} - \sum_{i,j} \hat{p_1}_{ij} \log p_{1_{ij}} 
\end{equation}
так как на результат минимизации функции не влияют константы, то окончательно минимизируется следующая функция: 
\begin{equation}
\mathcal{L} = - \sum_{i,j\in V} a_{ij}\log  \frac{1}{1+ exp(-\Phi_i \cdot \Phi_j)}
\end{equation}

В случае сохранения близости второго порядка:

\begin{equation}
p_2(v_j|v_i) = \frac{\exp (\Phi_i \cdot \Theta_j)}{\sum_{k \in V} \exp (\Phi_i \cdot \Theta_k)}
\end{equation}
\begin{equation}
\hat{p}_2(v_j|v_i) = \frac{a_{ij}}{d_i}, 
\end{equation}
где $d_i = \sum_{k \in N(i)} a_{ik} - $ степень вершины

Минимизируется следующая функция:

\begin{equation}
\sum_{i \in V} \lambda_i KL(\hat{p}_2 (\cdot | v_i), p_2(\cdot|v_i))
\end{equation}
$\lambda $ вводится для того чтоб показать важность каждой вершины в графе, которая может быть определена как степень вершины или по алгоритмам PageRank. Для простоты в оригинальной статье $\lambda_i$ был положен как $d_i$. Тогда, как и в случае LINE-1, не учитывая константы, фукнция минимизации выглядит следующим образом:
\begin{equation}
\mathcal{L} = - \sum_{i,j\in V} a_{ij}\log \frac{\exp (\Phi_i \cdot \Theta_j)}{\sum_{k \in V} \exp (\Phi_i \cdot \Theta_k)}
\end{equation}
\end{itemize}


\textbf{Выводы}: 
\begin{itemize}
\item Традиционные методы построения эмбедингов (снованные на факторизации) используют плотные матрицы, которые характеризуют граф, из-за чего эффективны лишь на небольших графах, а так же учитывают схожесть только по расстоянию между вершинами, а не атрибутам.

\item Методы, основанные на случайных блужданиях учитывают не только ближайших соседей, но и глобальную структуру графа. Работают быстрей, так как используют для вычислений не все пары вешин. 

\item Сверточные методы учитывают только локальную структуру, но зато более вычислительно эффективны и учитывают атрибуты вершин. 

\item Существует ряд методов (VERSE, LINE, HOPE), которые учитывают понятие мер схожести.  
\end{itemize}
%\addcontentsline{toc}{section}{Сравнительный анализ}
\newpage


\newpage
\begin{center}
			{\section{Рекомендации по использованию методов}}
		\end{center}

В работах \cite{comparUnRep} и \cite{EmbedingSurvey} был проведен подробный сравнительный анализ рассматриваемых в данном обзоре методов. 
%прочитать работу \cite{EmbedingSurvey} и выписать еще больше рекомендаций 
 
Основные результаты следующие: 
\begin{enumerate}
\item Методы, учитывающие роль вершины как источника и контекста при изучении представлений, рекомендуются для прогнозирования связей в ориентированных графах. 
%\item Некоторые структурные свойства, такие как коэффициент кластеризации, транзитивность, взаимность и т. д., рекомендуется учитывать при выборе конкретного метода.
\item Простой классификатор, основанный на непосредственном соседстве, предлагает лучшую или сопоставимую производительность для ряда наборов данных.
\item Для задач предсказания связей на неориентированных графах методы на основе PPR ( APP и VERSE) - являются наиболее эффективными методами во всех наборах данных.
\item В задачах предсказания связей, метод LINE, который непосредственно использует матрицу смежности в качестве матрицы близости, превосходит методы случайного блуждания для неориентированных графов. 
\item В задачах предсказания связей, для ориентированных графов с низкой взаимностью повторное представление контекста узла играет большую роль, и для задач предсказания направленных связей следует использовать методы кодирования и использования двух пространств внедрения для исходной и целевой контекстных представлений узлов.
\item В задачах предсказания связей более глубокие модели не имеют значительного преимущества перед поверхностыми. 
%\item Влияние структурных характеристик: для задачи предсказания связей, основнное на PPR построение показывает лучшие результаты и для ненаправленных и для направленных графов. Также, для направленных графов с низкой взаимностью, контекст основанный на Katz similarity показывает лучшие результаты. Смещенные блуждания с другой стороны имеют преимущества в предсказании связей для графов с высоким коэффициентом кластеризации, высокой транзитивностью и высокой взаимностью. Для низких коэффициентов кластеризации и транзитивности LINE-1 показывает намного более хороший результат для предсказания связей. Методы основанные на случайных блужданиях более надежные для задач классицикации вершин, а методы основанные на агрегаторах соседеней показывают лучшие результаты если есть большая схожесть между метками соседей.
%\item Роль контекстного представления: контекстное представление следует явно использовать в предсказаниях направленных связей. Чем меньше взаимность (т.е. чем больше ассимитричность в роли вершин как источников и контекста) в направленных графах, тем более важно контекстное представление. 

%\end{enumerate}
%\newpage
%Сводная таблица 2 показывает как лучше использовать каждый отдельный метод. 
%\begin{figure} [h!]
%	\begin{minipage}{\linewidth}
%	 		\center {\includegraphics[width =1\linewidth]{results.png}} \\ \small{Таблица 2 \cite{comparUnRep}}
 %	\end{minipage}
	% \end{figure}

\item Для направленных графов, для задачи реконструкции графов HOPE предпочитается APP
\item Для задач классификации узлов, степень гомофилии графа должна быть подсчитана прежде чем выбирать подход. Подходы глубокого обучения, основанные на аггрегации подходят для высокой степени гомофилии, а для низкой - DeepWalk.
\item Node2Vec более предпочтителен для класссификации вершин, а для предсказания рёбер - методы, учитывающие разные степени схожести (HOPE,SDNE)
\end{enumerate}

\newpage
%\addcontentsline{toc}{section}{Заключение}
\begin{center}
			{\section{Заключение}}
		\end{center}

Было рассмотрено множество различных методов, которые разделены на общие группы по подходу к обучению представления графов. Методы различаются также и учетом различной информации графов (локальная или глобальная структуры, схожесть по атрибутам вершин). Основным наиболее эффективным подходом остается построение эмбедингов на основе нейронных сверточных сетей, хотя и другие методы показывают преимущества на конкретных задачах. При решении задач и выборе метода, необходимо учитывать структурные свойства графов, размер графа, тип задач. 

Также был сделан акцент на экспериментальную часть всех методов и основная информация по наиболее часто использованным датасетам и метрикам собрана в таблицах \ref{tab:datasets}, \ref{tab:metrics}
 
Основное будущее развитие всех методов это масштабирование на большие графы, а также улучшения методов, направленные на улучшение качества и скорости работы алгоритмов. 

\newpage
\addcontentsline{toc}{section}{Список использованных источников}
\def \thebibliographyname{Список использованных источников}
\begin{thebibliography}{50}


\bibitem{GNN} Z. Wu, S. Pan, F. Chen, G. Long,C. Zhang, S. Yu Philip. 2019. A comprehensive survey on graph neural networks. arXiv:1901.00596.
\bibitem{EmbedingSurvey} P. Goyal and E. Ferrara, “Graph embedding techniques, applications, and performance: A survey,” Knowledge-Based Syst., vol. 151, 2018, doi: 10.1016/j.knosys.2018.03.022.
\bibitem{eigenmaps} Mikhail Belkin and Partha Niyogi. 2001. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS. 585–591

\bibitem{Sam}Sam T Roweis and Lawrence K Saul. 2000. Nonlinear dimensionality reduction by locally linear embedding. science 290, 5500 (2000), 2323–2326. 

\bibitem{reduction} Joshua B Tenenbaum, Vin De Silva, and John C Langford. 2000. A global geometric framework for nonlinear dimensionality reduction. science 290, 5500 (2000), 2319– 2323
\bibitem{reduction2} Shuicheng Yan, Dong Xu, Benyu Zhang, Hong-Jiang Zhang, Qiang Yang, and Stephen Lin. 2007. Graph embedding and extensions: A general framework for dimensionality reduction. TPAMI 29, 1 (2007)

\bibitem{factorization} Amr Ahmed, Nino Shervashidze, Shravan Narayanamurthy, Vanja Josifovski, and Alexander J Smola. 2013. Distributed large-scale natural graph factorization. In WWW. ACM, 37–48.
\bibitem{HOPE} Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, and Wenwu Zhu. 2016. Asymmetric transitivity preserving graph embedding. In KDD. 1105–1114
\bibitem{VERSE} A. Tsitsulin, D. Mottin, P. Karras, and E. Müller, “VERSE: Versatile graph embeddings from similarity measures,” Web Conf. 2018 - Proc. World Wide Web Conf. WWW 2018, pp. 539–548, 2018, doi: 10.1145/3178876.3186120.
\bibitem{ARCTE}  G. Rizos, S. Papadopoulos, and Y. Kompatsiaris, Multilabel user classification using the community structure of online networks, vol. 12, no. 3. 2017.

 \bibitem{Deepwalk} B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: online learning of social representations,” in KDD, 2014, pp. 701–710.
\bibitem{skipgram} Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Distributed Representations ofWords and Phrases and their Composition- ality. In NIPS. 3111–3119
\bibitem{GraRep} Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2015. GraRep: Learning Graph Representations with Global Structural Information. In CIKM. 891–900
\bibitem{struc2vec} Leonardo FR Ribeiro, Pedro HP Saverese, and Daniel R Figueiredo. 2017. struc2vec: Learning node representations from structural identity. In KDD. ACM, 385–394
\bibitem{LINE} J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “LINE: large-scale information network embedding,” in WWW, 2015, pp. 1067–1077.
\bibitem{node2vec} A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” in KDD, 2016, pp. 855–864.
\bibitem{SEED} L. Wang et al., “INDUCTIVE AND UNSUPERVISED REPRESENTATION LEARNING ON GRAPH STRUCTURED OBJECTS,” 2016.
\bibitem{TransE} A. Bordes, N. Usunier, A. Garcia-dur, J. Weston, and O. Yakhnenko, “Translating Embeddings for Modeling Multi-relational Data,” pp. 1–9.
\bibitem{GraphSAGE}  W. L. Hamilton, R. Ying, and J. Leskovec, “Inductive representation learning on large graphs,” Adv. Neural Inf. Process. Syst., vol. 2017-Decem, no. Nips, pp. 1025–1035, 2017.
\bibitem{GAN} P. Velickovic, A. Casanova, P. Liò, G. Cucurull, A. Romero, and Y. Bengio, “Graph attention networks,” 6th Int. Conf. Learn. Represent. ICLR 2018 - Conf. Track Proc., pp. 1–12, 2018.
\bibitem{SDNE} D. Wang, P. Cui, W. Zhu, Structural deep network embedding, in:  Pro-ceedings of the 22nd International Conference on Knowledge Discoveryand Data Mining, ACM, 2016, pp. 1225–1234.
\bibitem{comparUnRep} M. Khosla, V. Setty, and A. Anand, “A Comparative Study for Unsupervised Network Representation Learning,” IEEE Trans. Knowl. Data Eng., pp. 1–1, 2019, doi: 10.1109/tkde.2019.2951398.
\bibitem{APP} C. Zhou, Y. Liu, X. Liu, Z. Liu, and J. Gao. Scalable graph embedding for asymmetric proximity. In AAAI, pages 2942–2948, 2017.
\bibitem{SDNE} D.Wang, P. Cui, andW. Zhu. Structural deep network embedding. In SIGKDD, pages 1225–1234. ACM, 2016.
\bibitem{NETMF} J. Qiu, Y. Dong, H. Ma, J. Li, K. Wang, and J. Tang. Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec. In WSDM, pages 459–467, 2018.
%\bibitem{GMN} Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli. Graph matching net- works for learning the similarity of graph structured objects. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings ofInternational Conference on Machine Learning, volume 97, pp. 3835–3845, 09–15 Jun 2019
%\bibitem{GIN} Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In Proceedings ofInternational Conference on Learning Representations, 2019
%\bibitem{RWModMax} Devooght R, Mantrach A, Kivima¨ki I, Bersini H, Jaimes A, Saerens M. Random walks based modularity: application to semi-supervised learning. In: Proceedings of the 23rd international conference on World wide web. Seoul, Republic of Korea: International World Wide Web Conferences Steering Committee; 2014. p. 213–224.
%\bibitem{RepEig} Smith LM, Lerman K, Garcia-Cardona C, Percus AG, Ghosh R. Spectral clustering with epidemic diffu- sion. Physical Review E. 2013; 88(4):042813.
%\bibitem{Louvain} Blondel VD, Guillaume JL, Lambiotte R, Lefebvre E. Fast unfolding of communitie 
%\bibitem{EdgeCluster} Tang L, Liu H. Scalable learning of collective behavior based on sparse social dimensions. In: Proceed- ings of the 18th ACM conference on Information and knowledge management. Hong Kong, China: ACM; 2009. p. 1107–1116.
21.
%\bibitem{MROC}  Wang X, Tang L, Liu H, Wang L. Learning with multi-resolution overlapping communities. Knowledge and information systems. 2013; 36(2):517–535
%\bibitem{BigClam} Yang J, Leskovec J. Overlapping community detection at scale: a nonnegative matrix factorization approach. In: Proceedings of the sixth ACM international conference on Web search and data mining. Rome, Italy: ACM; 2013. p. 587–596.
56.
%\bibitem{OSLOM} Lancichinetti A, Radicchi F, Ramasco JJ, Fortunato S. Finding statistically significant communities in networks. PloS one. 2011; 6(4):e18961.

%\bibitem{Chenb} Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: Fast learning with graph convolutional networks via importance sampling. In International Conference on Learning Representations (ICLR), 2018b
\bibitem{GCN} Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. CoRR, abs/1609.02907, 2016. URL http://arxiv.org/abs/1609.02907.
%\bibitem{Yinga}  Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Proceedings ofthe 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 18, 2018a. ISBN %34
%\bibitem{Chena} Jianfei Chen, Jun Zhu, and Le Song. Stochastic training of graph convolutional networks with variance reduction. In ICML, pp. 941–949, 2018a
%\bibitem{Gao} Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional networks. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’18, pp. 1416–1424, New York, NY, USA, 2018. ACM. ISBN 978-1-4503-5552-0.
%\bibitem{Huang} Wenbing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. Adaptive sampling towards fast graph representation learning. In Advances in Neural Information Processing Systems, pp. 4558–4567, 2018.

%\bibitem{Zeng} Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor K. Prasanna. Accurate, efficient and scalable graph embedding. CoRR, abs/1810.11899, 2018. URL http: //arxiv.org/abs/1810.11899
%\bibitem{Chiang} Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An ef- ficient algorithm for training deep and large graph convolutional networks. CoRR, abs/1905.07953, 2019. URL http://arxiv.org/abs/1905.07953

%\bibitem{Zhang} Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan: Gated attention networks for learning on large and spatiotemporal graphs. arXiv preprint arXiv:1803.07294, 2018
%\bibitem{Lu}  Haonan Lu, Seth H. Huang, Tian Ye, and Xiuyan Guo. Graph star net for generalized multi-task learning. CoRR, abs/1906.12330, 2019. URL http://arxiv.org/abs/1906.12330
%\bibitem{Klicpera} Johannes Klicpera, Aleksandar Bojchevski, and Stephan Gunnemann. Personalized embedding propa- gation: Combining neural networks on graphs with personalized pagerank. CoRR, abs/1810.05997, 2018. URL http://arxiv.org/abs/1810.05997

%\bibitem{He} Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385
%\bibitem{Xu2018} Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. arXiv preprint arXiv:1806.03536, 2018
%\bibitem{GraphSAINT} H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V. Prasanna, “GraphSAINT: Graph Sampling Based Inductive Learning Method,” no. 2018, 2019, [Online]. Available: http://arxiv.org/abs/1907.04931..

%\bibitem{Survey on TL} S. Pan, Q. Yang. 2010. A survey on transfer learning. \textit{IEEE Transactions on Knowledge and Data Engineering} 22(10):1345–1359.
%\bibitem{SR2LR} Mihalkova, Lilyana and Mooney, Raymond J. 2009. Transfer learning from minimal target data by mapping across relational domains. \textit{Twenty-First International Joint Conference on Artificial Intelligence}. 
%\bibitem{Getoor and Taskar}  L. Getoor, B. Taskar, editors. Introduction to Statistical Relational Learning. MIT Press, Cambridge, MA, 2007.

%\bibitem{Richardson and Domingos} M. Richardson and P. Domingos. Markov logic networks. Machine Learning, 62:107–136, 2006.
%\bibitem{Mihalkova et al} L. Mihalkova, T. Huynh, and R. J. Mooney. Mapping and revising Markov logic networks for transfer learning. (AAAI-07).
%\bibitem{Davis and Domingos} J. Davis, P. Domingos. Deep transfer via second-order markov logic. \textit{In Proceedings of the AAAI Workshop on Transfer Learning For Complex Tasks}. 2008
%\bibitem{Forbus and Oblinger}  Kenneth D. Forbus , Dan Oblinger. Making SME greedy and pragmatic. (CogSci-90).
%\bibitem{Intristic} J. Lee, H. Kim, J. Lee, and S. Yoon, “Transfer learning for deep learningon graph-structured data.” in AAAI, 2017, pp. 2154–2160 
%\bibitem{Sonawane}  Sonawane, S., and Kulkarni, P. 2014. Graph based representation and analysis of text document: A survey of techniques. \textit{International Journal of Computer Applications} 96(19).
%\bibitem{Henaff} Henaff, M.; Bruna, J.; and LeCun, Y. 2015. Deep convolutional networks on graph - structured data. arXiv:1506.05163.
%\bibitem{AdaGCN} Quanyu Dai, Xiao Shen, Xiao-Ming Wu, Dan Wang. 2019. Network Transfer Learning via Adversarial Domain Adaptation with Graph Convolution. arXiv:1909.01541(2019)



%\bibitem{Salakhutdiniv} Z. Yang, W. W. Cohen, and R. Salakhutdinov, “Revisiting semi- supervised learning with graph embeddings,” in ICML, 2016, pp. 40–48.
%\bibitem{Kipf} T. N. Kipf and M. Welling, “Semi-supervised classification with graph convolutional networks,” in ICLR, 2017.


%\bibitem{Ni} J. Ni, S. Chang, X. Liu, W. Cheng, H. Chen, D. Xu, and X. Zhang, “Co-regularized deep multi-network embedding,” in WWW, 2018, pp. 469–478.
%\bibitem{Xu} L. Xu, X. Wei, J. Cao, and P. S. Yu, “Embedding of embedding (EOE): joint embedding for coupled heterogeneous networks,” in WSDM, 2017, pp. 741–749.
%\bibitem{Yang} S. J. Pan and Q. Yang, “A survey on transfer learning,” \textit{IEEE Trans. Knowl. Data Eng}., vol. 22, no. 10, pp. 1345–1359, 2010.
%\bibitem{Wang} M. Wang and W. Deng, “Deep visual domain adaptation: A survey,” Neurocomputing, vol. 312, pp. 135–153, 2018
%\bibitem{AS-MAML} Ma, Ning, et al. "Few-Shot Graph Classification with Model Agnostic Meta-Learning." arXiv preprint arXiv:2003.08246 (2020).
%\bibitem{MAML} Finn, Chelsea, Pieter Abbeel, and Sergey Levine. "Model-agnostic meta-learning for fast adaptation of deep networks." \textit{Proceedings of the 34th International Conference on Machine Learning-Volume 70.} JMLR. org, 2017
%\bibitem{Du} Zhengxiao Du, Xiaowei Wang, Hongxia Yang, Jingren Zhou, and Jie Tang. Sequential scenario- specific meta learner for online recommendation. In KDD’19, page 2895–2904, 2019.
%\bibitem{Satorras} Victor Garcia Satorras and Joan Bruna Estrach. Few-shot learning with graph neural networks. In ICLR, 2018.
%\bibitem{Kim} Jongmin Kim, Taesup Kim, Sungwoong Kim, and Chang D. Yoo. Edge-labeling graph neural net- work for few-shot learning. In CVPR, June 2019.
%\bibitem{Liu} Lu Liu, Tianyi Zhou, Guodong Long, Jing Jiang, and Chengqi Zhang. Learning to propagate for graph meta-learning. In NeurIPS, 2019. 
%\bibitem{Yao} Huaxiu Yao, Xian Wu, Zhiqiang Tao, Yaliang Li, Bolin Ding, Ruirui Li, and Zhenhui Li. Au- tomated relational meta-learning. In ICLR, 2020.
%\bibitem{Chauhan} Jatin Chauhan, Deepak Nathani, and Manohar Kaul. Few-shot learning on graphs via super- classes based on graph spectral measures. In ICLR, 2020.

\end{thebibliography}
%\bibliography{currentbase}{}
%\bibliographystyle{plain}

\end{document}